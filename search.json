[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Building Reproducible Analytical Pipelines",
    "section": "",
    "text": "Introduction\nThis is the 2025 edition of the course. If you’re looking for the 2024 edition, you can click here\nWhat’s new:\nThis course is based on my book titled Building Reproducible Analytical Pipelines with R. This course focuses only on certain aspects that are discussed in greater detail in the book.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Building Reproducible Analytical Pipelines",
    "section": "Schedule",
    "text": "Schedule\n\n2025/09/15 - 4 hours, Intro (1 hour) and Nix (3 hours)\n2025/09/22 - 4 hours, Git (4 hours)\n2025/10/06 - 4 hours, Functional programming (1 hour), Unit testing (3 hours)\n2025/10/13 - 4 hours, Pipelines using Nix and rixpress\n2025/10/15 - 4 hours, Packaging\n2025/10/20 - 4 hours, Docker\n2025/10/27 - 4 hours, GitHub Actions",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#reproducible-analytical-pipelines",
    "href": "index.html#reproducible-analytical-pipelines",
    "title": "Building Reproducible Analytical Pipelines",
    "section": "Reproducible analytical pipelines?",
    "text": "Reproducible analytical pipelines?\nThis course is my take on setting up code that results in some data product. This code has to be reproducible, documented and production ready. Not my original idea, but introduced by the UK’s Analysis Function.\nThe basic idea of a reproducible analytical pipeline (RAP) is to have code that always produces the same result when run, whatever this result might be. This is obviously crucial in research and science, but this is also the case in businesses that deal with data science/data-driven decision making etc.\nA well documented RAP avoids a lot of headache and is usually re-usable for other projects as well.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#data-products",
    "href": "index.html#data-products",
    "title": "Building Reproducible Analytical Pipelines",
    "section": "Data products?",
    "text": "Data products?\nIn this course each of you will develop a data product. A data product is anything that requires data as an input. This can be a very simple report in PDF or Word format or a complex web app. This website is actually also a data product, which I made using the R programming language and Quarto. Dependencies are managed by the Nix package manager and the build runs on GitHub Actions, and the website you’re seeing his hosted on GitHub Pages. By the end of the course, you’ll have all the basic knowledge to achieve something similar.\nThe focus on the course will not be about the end product itself, which you will have to choose for your project, but instead we will focus on how to set up a pipeline that results in these data products in a reproducible way.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#machine-learning",
    "href": "index.html#machine-learning",
    "title": "Building Reproducible Analytical Pipelines",
    "section": "Machine learning?",
    "text": "Machine learning?\nNo, being a master in machine learning is not enough to become a data scientist. Actually, the older I get, the more I think that machine learning is almost optional. What is not optional is knowing how:\n\nto write, test, and properly document code;\nto acquire (reading in data can be tricky!) and clean data;\nto work inside the Linux terminal/command line interface;\nto use Git, Docker for Dev(Git)Ops;\nthe Internet works (what’s a firewall? what’s a reverse proxy? what’s a domain name? etc, etc…);\n\nBut what about machine learning? Well, depending what you’ll end up doing, you might indeed focus a lot on machine learning and/or statistical modeling. That being said, in practice, it is very often much more efficient to let some automl algorithm figure out the best hyperparameters of a XGBoost model and simply use that, at least as a starting point (but good luck improving upon automl…). What matters, is that the data you’re feeding to your model is clean, that your analysis is sensible, and most importantly, that it could be understood by someone taking over (imagine you get sick) and rerun with minimal effort in the future. The model here should simply be a piece that could be replaced by another model without much impact. The model is rarely central… but of course there are exceptions to this, especially in research, but every other point I’ve made still stands. It’s just that not only do you have to care about your model a lot, you also have to care about everything else.\nSo in this course we’re going to learn a bit of all of this. We’re going to learn how to write reusable code, learn some basics of the Linux command line, Nix, Git and Docker.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#what-actually-is-reproducibility",
    "href": "index.html#what-actually-is-reproducibility",
    "title": "Building Reproducible Analytical Pipelines",
    "section": "What actually is reproducibility?",
    "text": "What actually is reproducibility?\nA reproducible project means that this project can be rerun by anyone at 0 (or very minimal) cost. But there are different levels of reproducibility, and I will discuss this in the next section. Let’s first discuss some requirements that a project must have to be considered a RAP.\n\nThe requirements of a RAP\nFor something to be truly reproducible, it has to respect the following bullet points:\n\nSource code must obviously be available and thoroughly tested and documented (which is why we will be using Git and GitHub);\nAll the dependencies must be easy to find and install (we are going to deal with this using Nix);\nTo be written with an open source programming language (nocode tools like Excel are by default non-reproducible because they can’t be used non-interactively);\nThe project needs to be run on an open source operating system (thankfully, we can deal with this without having to install and learn to use a new operating system, thanks to Docker);\nData and the paper/report need obviously to be accessible as well, if not publicly as is the case for research, then within your company.\n\nAlso, reproducibility is on a continuum, and depending on the constraints you face your project can be “not very reproducible” to “totally reproducible”. Let’s consider the following list of anything that can influence how reproducible your project truly is:\n\nVersion of the programming language used;\nVersions of the packages/libraries of said programming language used;\nOperating System, and its version;\nVersions of the underlying system libraries (which often go hand in hand with OS version, but not necessarily).\nAnd even the hardware architecture that you run all that software stack on.\n\nSo by “reproducibility is on a continuum”, what I mean is that you could set up your project in a way that none, one, two, three, four or all of the preceeding items are taken into consideration when making your project reproducible.\nThis is not a novel, or new idea. Peng (2011) already discussed this concept but named it the reproducibility spectrum.\n\n\n\nThe reproducibility spectrum from Peng’s 2011 paper.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#large-language-models",
    "href": "index.html#large-language-models",
    "title": "Building Reproducible Analytical Pipelines",
    "section": "Large Language Models",
    "text": "Large Language Models\nLLMs have rapidly become an essential powertool in the data scientist’s toolbox. But as with any powertool, beginners risk cutting their fingers if they’re not careful. So it is important to learn how to use them. This course will give you some pointers on how to integrate LLMs into your workflow.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#why-r-why-not-insert-your-favourite-programming-language",
    "href": "index.html#why-r-why-not-insert-your-favourite-programming-language",
    "title": "Building Reproducible Analytical Pipelines",
    "section": "Why R? Why not [insert your favourite programming language]",
    "text": "Why R? Why not [insert your favourite programming language]\nR is a domain-specific language whose domain is statistics, data analysis/science and machine learning, and as such has many built-in facilities to make handling data very efficient.\nIf you learn R you have access to almost 25’000 packages (as of June 2025, including both CRAN and Bioconductor packages) to:\n\nclean data (see: {dplyr}, {tidyr}, {data.table}…);\nwork with medium and big data (see: {arrow}, {sparklyr}…);\nvisualize data (see: {ggplot2}, {plotly}, {echarts4r}…);\ndo literate programming (using Rmarkdown or Quarto, you can write books, documents even create a website);\ndo functional programming (see: {purrr}…);\ncall other languages from R (see: {reticulate} to call Python from R);\ndo machine learning and AI (see: {tidymodels}, {tensorflow}, {keras}…)\ncreate webapps (see: {shiny}…)\ndomain specific statistics/machine learning (see CRAN Task Views for an exhaustive list);\nand more\n\nIt’s not just about what the packages provide: installing R and its packages and dependencies is rarely frustrating, which is not the case with Python (Python 2 vs Python 3, pip vs conda, pyenv vs venv vs uv, …, dependency hell is a real place full of snakes)\n\n\n\n\n\n\n\n\n\n\n\nThe reason this is the case is that anyone can push anything on to Pypi, and no package gets checked against its dependencies or reverse dependencies. That is not the case for R, where published packages need to declare their dependencies and can’t break any of their reverse dependencies (when this happens, authors of reverse dependencies get two weeks to fix their packages or they get removed from CRAN).\nFurthermore, and this is surprising to many people, R offers a much better package developing experience than Python.\nThat doesn’t mean that R does not have any issues. Quite the contrary, R sometimes behaves in seemingly truly bizarre ways (as an example, try running nchar(\"1000000000\") and then nchar(1000000000) and try to make sense of it). To know more about such bizarre behaviour, I recommend you read The R Inferno (linked at the end of this chapter). So, yes, R is far from perfect, but it sucks less than the alternatives (again, in my absolutely objective opinion).\n\n  🟡 Loading\n    webR...\n  \n    \n    \n      \n    \n  \n  \n  \n\n\nThat being said, Python remains extremely popular, and it is likely that you will continue writing Python. In my opinion, the future of data science is going to be more and more polyglot. Data products are evermore complex, and require being built using many languages; so ideally we would like to find a way to use whatever tool is best fit for the job at hand. Sometimes it can be R, sometimes Python, sometimes shell scripts, or any other language. This is where Nix will help us.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#nix",
    "href": "index.html#nix",
    "title": "Building Reproducible Analytical Pipelines",
    "section": "Nix",
    "text": "Nix\nNix is a package manager for Linux distributions, macOS and it even works on Windows if you enable WSL2. What’s a package manager? If you’re not a Linux user, you may not be aware. Let me explain it this way: in R, if you want to install a package to provide some functionality not included with a vanilla installation of R, you’d run this:\ninstall.packages(\"dplyr\")\nIt turns out that Linux distributions, like Ubuntu for example, work in a similar way, but for software that you’d usually install using an installer (at least on Windows). For example you could install Firefox on Ubuntu using:\nsudo apt-get install firefox\n(there’s also graphical interfaces that make this process “more user-friendly”). In Linux jargon, packages are simply what we call software (or I guess it’s all “apps” these days). These packages get downloaded from so-called repositories (think of CRAN, the repository of R packages, or Pypi, in the case of Python) but for any type of software that you might need to make your computer work: web browsers, office suites, multimedia software and so on.\nSo Nix is just another package manager that you can use to install software.\nBut what interests us is not using Nix to install Firefox, but instead to install R, Python and the R and Python packages that we require for our analysis. But why use Nix instead of the usual ways to install software on our operating systems?\nThe first thing that you should know is that Nix’s repository, nixpkgs, is huge. Humongously huge. As I’m writing these lines, there’s more than 120’000 pieces of software available, and the entirety of CRAN and Bioconductor is also available through nixpkgs. So instead of installing R as you usually do and then use install.packages() to install packages, you could use Nix to handle everything. But still, why use Nix at all?\nNix has an interesting feature: using Nix, it is possible to install software in (relatively) isolated environments. So using Nix, you can install as many versions of R and R packages that you need. Suppose that you start working on a new project. As you start the project, with Nix, you would install a project-specific version of R and R packages that you would only use for that particular project. If you switch projects, you’d switch versions of R and R packages.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#pre-requisites",
    "href": "index.html#pre-requisites",
    "title": "Building Reproducible Analytical Pipelines",
    "section": "Pre-requisites",
    "text": "Pre-requisites\nI will assume basic programming knowledge, and not much more. Ideally you’ll be following this course from a Linux machine, but if you’re macOS, that’s fine as well. On Windows, you will have to set up WSL2 to follow along.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#grading",
    "href": "index.html#grading",
    "title": "Building Reproducible Analytical Pipelines",
    "section": "Grading",
    "text": "Grading\nThe way grading works in this course is as follows: during lecture hours you will follow along. At home, you’ll be working on setting up your own pipeline. For this, choose a dataset that ideally would need some cleaning and/or tweaking to be usable. If time allows, I’ll leave some time during lecture hours for you to work on it and ask me and your colleagues for help. At the end of the semester, I will need to download your code and get it running. The less effort this takes me, the better your score. Here is a tentative breakdown:\n\nCode is on github.com and the repository is documented with a Readme.md file: 5 points;\nData and functions to run pipeline are documented and tested: 5 points;\nEvery software dependency is easily installed: 5 points;\nPipeline can be executed in one command: 5 points.\n\nThe way to fail this class is to write an undocumented script that only runs on your machine and expect me to debug it to get it to run.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#jargon",
    "href": "index.html#jargon",
    "title": "Building Reproducible Analytical Pipelines",
    "section": "Jargon",
    "text": "Jargon\nThere’s some jargon that is helpful to know when working with R. Here’s a non-exhaustive list to get you started:\n\nCRAN: the Comprehensive R Archive Network. This is a curated online repository of packages and R installers. When you type install.packages(\"package_name\") in an R console, the package gets downloaded from there;\nLibrary: the collection of R packages installed on your machine;\nR console: the program where the R interpreter runs;\nPosit/RStudio: Posit (named RStudio in the past) are the makers of the RStudio IDE and of the tidyverse collection of packages;\ntidyverse: a collection of packages created by Posit that offer a common language and syntax to perform any task required for data science — from reading in data, to cleaning data, up to machine learning and visualisation;\nbase R: refers to a vanilla installation (and vanilla capabilities) of R. Often used to contrast a tidyverse specific approach to a problem (for example, using base R’s lapply() in constrast to the tidyverse purrr::map()).\npackage::function(): Functions can be accessed in several ways in R, either by loading an entire package at the start of a script with library(dplyr) or by using dplyr::select().\nFunction factory (sometimes adverb): a function that returns a function.\nVariable: the variable of a function (as in x in f(x)) or the variable from statistical modeling (synonym of feature)\n&lt;- vs =: in practice, you can use &lt;- and = interchangeably. I prefer &lt;-, but feel free to use = if you wish.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#further-reading",
    "href": "index.html#further-reading",
    "title": "Building Reproducible Analytical Pipelines",
    "section": "Further reading",
    "text": "Further reading\n\nAn Introduction to R (from the R team themselves)\nWhat is CRAN?\nThe R Inferno\nBuilding Reproducible Analytical Pipelines with R\nReproducible Analytical Pipelines (RAP)",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Building Reproducible Analytical Pipelines",
    "section": "License",
    "text": "License\nThis course is licensed under the WTFPL.\n \n\n\n\n\nPeng, Roger D. 2011. “Reproducible Research in Computational Science.” Science 334 (6060): 1226–27.",
    "crumbs": [
      "Introduction"
    ]
  }
]