[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Building Reproducible Analytical Pipelines",
    "section": "",
    "text": "Introduction\nThis is the 2025 edition of the course. If you’re looking for the 2024 edition, you can click here\nWhat’s new:\nThis course is based on my book titled Building Reproducible Analytical Pipelines with R. This course focuses only on certain aspects that are discussed in greater detail in the book.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Building Reproducible Analytical Pipelines",
    "section": "Schedule",
    "text": "Schedule\n\n2025/09/15 - 4 hours, Intro (1 hour) and Nix (3 hours)\n2025/09/22 - 4 hours, Git (4 hours)\n2025/10/06 - 4 hours, Functional programming (1 hour), Unit testing (3 hours)\n2025/10/13 - 4 hours, Packaging\n2025/10/15 - 4 hours, Pipelines using Nix and rixpress\n2025/10/20 - 4 hours, Docker\n2025/10/21 - 4 hours, GitHub Actions",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#reproducible-analytical-pipelines",
    "href": "index.html#reproducible-analytical-pipelines",
    "title": "Building Reproducible Analytical Pipelines",
    "section": "Reproducible analytical pipelines?",
    "text": "Reproducible analytical pipelines?\nThis course is my take on setting up code that results in some data product. This code has to be reproducible, documented and production ready. Not my original idea, but introduced by the UK’s Analysis Function.\nThe basic idea of a reproducible analytical pipeline (RAP) is to have code that always produces the same result when run, whatever this result might be. This is obviously crucial in research and science, but this is also the case in businesses that deal with data science/data-driven decision making etc.\nA well documented RAP avoids a lot of headache and is usually re-usable for other projects as well.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#data-products",
    "href": "index.html#data-products",
    "title": "Building Reproducible Analytical Pipelines",
    "section": "Data products?",
    "text": "Data products?\nIn this course each of you will develop a data product. A data product is anything that requires data as an input. This can be a very simple report in PDF or Word format or a complex web app. This website is actually also a data product, which I made using the R programming language and Quarto. Dependencies are managed by the Nix package manager and the build runs on GitHub Actions, and the website you’re seeing his hosted on GitHub Pages. By the end of the course, you’ll have all the basic knowledge to achieve something similar.\nThe focus on the course will not be about the end product itself, which you will have to choose for your project, but instead we will focus on how to set up a pipeline that results in these data products in a reproducible way.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#machine-learning",
    "href": "index.html#machine-learning",
    "title": "Building Reproducible Analytical Pipelines",
    "section": "Machine learning?",
    "text": "Machine learning?\nNo, being a master in machine learning is not enough to become a data scientist. Actually, the older I get, the more I think that machine learning is almost optional. What is not optional is knowing how:\n\nto write, test, and properly document code;\nto acquire (reading in data can be tricky!) and clean data;\nto work inside the Linux terminal/command line interface;\nto use Git, Docker for Dev(Git)Ops;\nthe Internet works (what’s a firewall? what’s a reverse proxy? what’s a domain name? etc, etc…);\n\nBut what about machine learning? Well, depending what you’ll end up doing, you might indeed focus a lot on machine learning and/or statistical modeling. That being said, in practice, it is very often much more efficient to let some automl algorithm figure out the best hyperparameters of a XGBoost model and simply use that, at least as a starting point (but good luck improving upon automl…). What matters, is that the data you’re feeding to your model is clean, that your analysis is sensible, and most importantly, that it could be understood by someone taking over (imagine you get sick) and rerun with minimal effort in the future. The model here should simply be a piece that could be replaced by another model without much impact. The model is rarely central… but of course there are exceptions to this, especially in research, but every other point I’ve made still stands. It’s just that not only do you have to care about your model a lot, you also have to care about everything else.\nSo in this course we’re going to learn a bit of all of this. We’re going to learn how to write reusable code, learn some basics of the Linux command line, Nix, Git and Docker.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#what-actually-is-reproducibility",
    "href": "index.html#what-actually-is-reproducibility",
    "title": "Building Reproducible Analytical Pipelines",
    "section": "What actually is reproducibility?",
    "text": "What actually is reproducibility?\nA reproducible project means that this project can be rerun by anyone at 0 (or very minimal) cost. But there are different levels of reproducibility, and I will discuss this in the next section. Let’s first discuss some requirements that a project must have to be considered a RAP.\n\nThe requirements of a RAP\nFor something to be truly reproducible, it has to respect the following bullet points:\n\nSource code must obviously be available and thoroughly tested and documented (which is why we will be using Git and GitHub);\nAll the dependencies must be easy to find and install (we are going to deal with this using Nix);\nTo be written with an open source programming language (nocode tools like Excel are by default non-reproducible because they can’t be used non-interactively);\nThe project needs to be run on an open source operating system (thankfully, we can deal with this without having to install and learn to use a new operating system, thanks to Docker);\nData and the paper/report need obviously to be accessible as well, if not publicly as is the case for research, then within your company.\n\nAlso, reproducibility is on a continuum, and depending on the constraints you face your project can be “not very reproducible” to “totally reproducible”. Let’s consider the following list of anything that can influence how reproducible your project truly is:\n\nVersion of the programming language used;\nVersions of the packages/libraries of said programming language used;\nOperating System, and its version;\nVersions of the underlying system libraries (which often go hand in hand with OS version, but not necessarily).\nAnd even the hardware architecture that you run all that software stack on.\n\nSo by “reproducibility is on a continuum”, what I mean is that you could set up your project in a way that none, one, two, three, four or all of the preceeding items are taken into consideration when making your project reproducible.\nThis is not a novel, or new idea. Peng (2011) already discussed this concept but named it the reproducibility spectrum.\n\n\n\nThe reproducibility spectrum from Peng’s 2011 paper.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#large-language-models",
    "href": "index.html#large-language-models",
    "title": "Building Reproducible Analytical Pipelines",
    "section": "Large Language Models",
    "text": "Large Language Models\nLLMs have rapidly become an essential powertool in the data scientist’s toolbox. But as with any powertool, beginners risk cutting their fingers if they’re not careful. So it is important to learn how to use them. This course will give you some pointers on how to integrate LLMs into your workflow. All of the exercises can and should be tackled using LLMs.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#why-r-why-not-insert-your-favourite-programming-language",
    "href": "index.html#why-r-why-not-insert-your-favourite-programming-language",
    "title": "Building Reproducible Analytical Pipelines",
    "section": "Why R? Why not [insert your favourite programming language]",
    "text": "Why R? Why not [insert your favourite programming language]\nR is a domain-specific language whose domain is statistics, data analysis/science and machine learning, and as such has many built-in facilities to make handling data very efficient.\nIf you learn R you have access to almost 25’000 packages (as of June 2025, including both CRAN and Bioconductor packages) to:\n\nclean data (see: {dplyr}, {tidyr}, {data.table}…);\nwork with medium and big data (see: {arrow}, {sparklyr}…);\nvisualize data (see: {ggplot2}, {plotly}, {echarts4r}…);\ndo literate programming (using Rmarkdown or Quarto, you can write books, documents even create a website);\ndo functional programming (see: {purrr}…);\ncall other languages from R (see: {reticulate} to call Python from R);\ndo machine learning and AI (see: {tidymodels}, {tensorflow}, {keras}…)\ncreate webapps (see: {shiny}…)\ndomain specific statistics/machine learning (see CRAN Task Views for an exhaustive list);\nand more\n\nIt’s not just about what the packages provide: installing R and its packages and dependencies is rarely frustrating, which is not the case with Python (Python 2 vs Python 3, pip vs conda, pyenv vs venv vs uv, …, dependency hell is a real place full of snakes)\n\n\n\n\n\n\n\n\n\n\n\nThe reason this is the case is that anyone can push anything on to Pypi, and no package gets checked against its dependencies or reverse dependencies. That is not the case for R, where published packages need to declare their dependencies and can’t break any of their reverse dependencies (when this happens, authors of reverse dependencies get two weeks to fix their packages or they get removed from CRAN).\nFurthermore, and this is surprising to many people, R offers a much better package developing experience than Python.\nThat doesn’t mean that R does not have any issues. Quite the contrary, R sometimes behaves in seemingly truly bizarre ways (as an example, try running nchar(\"1000000000\") and then nchar(1000000000) and try to make sense of it). To know more about such bizarre behaviour, I recommend you read The R Inferno (linked at the end of this chapter). So, yes, R is far from perfect, but it sucks less than the alternatives (again, in my absolutely objective opinion).\n\n  🟡 Loading\n    webR...\n  \n    \n    \n      \n    \n  \n  \n  \n\n\nThat being said, Python remains extremely popular, and it is likely that you will continue writing Python. In my opinion, the future of data science is going to be more and more polyglot. Data products are evermore complex, and require being built using many languages; so ideally we would like to find a way to use whatever tool is best fit for the job at hand. Sometimes it can be R, sometimes Python, sometimes shell scripts, or any other language. This is where Nix will help us.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#nix",
    "href": "index.html#nix",
    "title": "Building Reproducible Analytical Pipelines",
    "section": "Nix",
    "text": "Nix\nNix is a package manager for Linux distributions, macOS and it even works on Windows if you enable WSL2. What’s a package manager? If you’re not a Linux user, you may not be aware. Let me explain it this way: in R, if you want to install a package to provide some functionality not included with a vanilla installation of R, you’d run this:\ninstall.packages(\"dplyr\")\nIt turns out that Linux distributions, like Ubuntu for example, work in a similar way, but for software that you’d usually install using an installer (at least on Windows). For example you could install Firefox on Ubuntu using:\nsudo apt-get install firefox\n(there’s also graphical interfaces that make this process “more user-friendly”). In Linux jargon, packages are simply what we call software (or I guess it’s all “apps” these days). These packages get downloaded from so-called repositories (think of CRAN, the repository of R packages, or Pypi, in the case of Python) but for any type of software that you might need to make your computer work: web browsers, office suites, multimedia software and so on.\nSo Nix is just another package manager that you can use to install software.\nBut what interests us is not using Nix to install Firefox, but instead to install R, Python and the R and Python packages that we require for our analysis. But why use Nix instead of the usual ways to install software on our operating systems?\nThe first thing that you should know is that Nix’s repository, nixpkgs, is huge. Humongously huge. As I’m writing these lines, there’s more than 120’000 pieces of software available, and the entirety of CRAN and Bioconductor is also available through nixpkgs. So instead of installing R as you usually do and then use install.packages() to install packages, you could use Nix to handle everything. But still, why use Nix at all?\nNix has an interesting feature: using Nix, it is possible to install software in (relatively) isolated environments. So using Nix, you can install as many versions of R and R packages that you need. Suppose that you start working on a new project. As you start the project, with Nix, you would install a project-specific version of R and R packages that you would only use for that particular project. If you switch projects, you’d switch versions of R and R packages.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#pre-requisites",
    "href": "index.html#pre-requisites",
    "title": "Building Reproducible Analytical Pipelines",
    "section": "Pre-requisites",
    "text": "Pre-requisites\nI will assume basic programming knowledge, and not much more. Ideally you’ll be following this course from a Linux machine, but if you’re macOS, that’s fine as well. On Windows, you will have to set up WSL2 to follow along.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#grading",
    "href": "index.html#grading",
    "title": "Building Reproducible Analytical Pipelines",
    "section": "Grading",
    "text": "Grading\nThe way grading works in this course is as follows: during lecture hours you will follow along. At home, you’ll be working on setting up your own pipeline. For this, choose a dataset that ideally would need some cleaning and/or tweaking to be usable. If time allows, I’ll leave some time during lecture hours for you to work on it and ask me and your colleagues for help. At the end of the semester, I will need to download your code and get it running. The less effort this takes me, the better your score. Here is a tentative breakdown:\n\nCode is on github.com and the repository is documented with a Readme.md file: 5 points;\nData and functions to run pipeline are documented and tested: 5 points;\nEvery software dependency is easily installed: 5 points;\nPipeline can be executed in one command: 5 points.\n\nThe way to fail this class is to write an undocumented script that only runs on your machine and expect me to debug it to get it to run, there is no excuse for this, especially in the age of LLMs.\nAt the end of each chapter, there are exercises that should help you solidify your knowledge. These are not graded, and there purely for you to learn more.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#jargon",
    "href": "index.html#jargon",
    "title": "Building Reproducible Analytical Pipelines",
    "section": "Jargon",
    "text": "Jargon\nThere’s some jargon that is helpful to know when working with R, Python, or Nix.\nHere’s a non-exhaustive list to get you started:\n\nR\n\nCRAN: the Comprehensive R Archive Network. This is a curated online repository of packages and R installers. When you type install.packages(\"package_name\") in an R console, the package gets downloaded from there;\n\nLibrary: the collection of R packages installed on your machine;\n\nR console: the program where the R interpreter runs;\n\nPosit/RStudio: Posit (named RStudio in the past) are the makers of the RStudio IDE and of the tidyverse collection of packages;\n\ntidyverse: a collection of packages created by Posit that offer a common language and syntax to perform any task required for data science — from reading in data, to cleaning data, up to machine learning and visualisation;\n\nbase R: refers to a vanilla installation (and vanilla capabilities) of R. Often used to contrast a tidyverse specific approach to a problem (for example, using base R’s lapply() in contrast to the tidyverse purrr::map());\n\npackage::function(): functions can be accessed in several ways in R, either by loading an entire package at the start of a script with library(dplyr) or by using dplyr::select();\n\nFunction factory (sometimes adverb): a function that returns a function;\n\nVariable: the variable of a function (as in x in f(x)) or the variable from statistical modeling (synonym of feature);\n\n&lt;- vs =: in practice, you can use &lt;- and = interchangeably. I prefer &lt;-, but feel free to use = if you wish.\n\n\n\nPython\n\nPyPI: the Python Package Index. Similar to CRAN for R, it is the default online repository of Python packages, used when you run pip install package_name;\n\nVirtual environment (venv): an isolated environment containing its own Python interpreter and packages, useful for keeping dependencies separate between projects;\n\nConda: an alternative package and environment manager to pip/venv, popular in data science;\n\nJupyter notebook: an interactive environment for running Python code in cells, mixing code, plots, and prose;\n\nPEP: Python Enhancement Proposal. Design documents that describe new features or conventions (for example, PEP 8 is the style guide for Python code);\n\nDecorator: a function that takes another function and extends or modifies its behavior (for example, @staticmethod);\n\nList comprehension: a compact syntax for creating lists, such as [x * x for x in range(10)];\n\n__init__.py: a special file that marks a directory as a Python package;\n\n__main__: the entry point of a Python program (if __name__ == \"__main__\":).\n\n\n\nNix\n\nNixpkgs: the main collection of Nix packages. Most software you install via Nix comes from here (CRAN and Bioconductor are mirrored, but for Python, only individual packages get packaged for Nix);\n\nDerivation: the low-level build instruction in Nix. Every package in Nix is ultimately a derivation;\n\nStore: the /nix/store/ directory, where all Nix-built packages and their dependencies live, each in its own hash-prefixed path;\n\nExpression: Nix code describing how to build something (usually written in .nix files);\n\nFlake: a standardized way to package and distribute Nix code with reproducible inputs and outputs. We will not study flakes in this course, as they are still marked as experimental;\n\nShell: a reproducible development environment created by Nix;\n\nGC (garbage collection): Nix can remove unused packages from the store;\n\nOverlay: a mechanism to extend or override packages in Nixpkgs. This out of the scope of this course;\n\nHydra: the continuous integration service often used with Nix to build packages at scale;\n\nPinning: fixing your Nixpkgs (or other inputs) to a specific commit to ensure reproducibility.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#further-reading",
    "href": "index.html#further-reading",
    "title": "Building Reproducible Analytical Pipelines",
    "section": "Further reading",
    "text": "Further reading\n\nAn Introduction to R (from the R team themselves)\nWhat is CRAN?\nThe R Inferno\nBuilding Reproducible Analytical Pipelines with R\nReproducible Analytical Pipelines (RAP)\nHow Nix Works",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Building Reproducible Analytical Pipelines",
    "section": "License",
    "text": "License\nThis course is licensed under the WTFPL.\n \n\n\n\n\nPeng, Roger D. 2011. “Reproducible Research in Computational Science.” Science 334 (6060): 1226–27.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "02-intro-nix.html",
    "href": "02-intro-nix.html",
    "title": "1  Reproducibility with Nix",
    "section": "",
    "text": "1.1 Learning Outcomes\nBy the end of this chapter, you will:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Reproducibility with Nix</span>"
    ]
  },
  {
    "objectID": "02-intro-nix.html#learning-outcomes",
    "href": "02-intro-nix.html#learning-outcomes",
    "title": "1  Reproducibility with Nix",
    "section": "",
    "text": "Understand the need for environment reproducibility in modern workflows\nUse {rix} to generate default.nix files\nBuild cross-language environments for data work or software development",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Reproducibility with Nix</span>"
    ]
  },
  {
    "objectID": "02-intro-nix.html#why-reproducibility-why-nix-1h30",
    "href": "02-intro-nix.html#why-reproducibility-why-nix-1h30",
    "title": "1  Reproducibility with Nix",
    "section": "1.2 Why Reproducibility? Why Nix? (1h30)",
    "text": "1.2 Why Reproducibility? Why Nix? (1h30)\n\n1.2.1 Motivation: Reproducibility in Scientific and Data Workflows\nTo ensure that a project is reproducible you need to deal with at least four things:\n\nMake sure that the required/correct version of R (or any other language) is installed;\nMake sure that the required versions of packages are installed;\nMake sure that system dependencies are installed (for example, you’d need a working Java installation to install the {rJava} R package on Linux);\nMake sure that you can install all of this for the hardware you have on hand.\n\nBut in practice, one or most of these bullet points are missing from projects. The goal of this course is to learn how to fullfill all the requirements to build reproducible projects.\n\n\n1.2.2 Problems with Ad-Hoc Tools\nTools like Python’s venv or R’s renv only deal with some pieces of the reproducibility puzzle. Often, they assume an underlying OS, do not capture system-level dependencies (like libxml2, pandoc, or curl), and require users to “rebuild” their environments from partial metadata. Docker helps but introduces overhead, security challenges, and complexity, and just adding it to your project doesn’t make it reproducible if you don’t explicitely take some precautionary steps.\nTraditional approaches fail to capture the entire dependency graph of a project in a deterministic way. This leads to “it works on my machine” syndromes, onboarding delays, and subtle bugs.\n\n\n1.2.3 Nix, a declarative package manager\nNix is a tool for reproducible builds and development environments, often introduced as a package manager. It captures complete dependency trees, from your programming language interpreter to every system-level library you rely on. With Nix, environments are not recreated from documentation, but rebuilt precisely from code.\nNix can be installed on Linux distributions, macOS and it even works on Windows if you enable WSL2. In this course, we will use Nix mostly as a package manager (but towards also as a build automation tool).\nHowever Nix has quite a steep learning curve, so this is why for the purposes of this course we are going to use an R package called {rix} to set up reproducible environments.\n\n\n1.2.4 The rix package\n{rix} is an R package (I’m the author) and its goal is to make writing Nix expressions easy. With {rix} you can declare the environment you need using the provided rix() function, which is the package’s main function. Calling it generates a file called default.nix which is then used by the Nix package manager to build that environment. Ideally, you would set up such an environment for each of your projects. You can then use this environment to either work interactively, or run R or Python scripts. It is possible to have as many environments as projects, and software that is common to environments will simply be re-used and not get re-installed to save space. Environments are isolated from each other, but can still interact with your system’s files, unlike with Docker where a volume must be mounted. While this is useful, it can sometimes lead to issues. For example, if you already have R installed, and a user library of R packages, more caution is required to properly use environments managed by Nix.\nYou don’t need to have R installed or be an R user to use {rix}. If you have Nix installed on your system, it is possible to “drop” into a temporary environment with R and {rix} available and generate the required Nix expression from there.\nBut first, let’s install Nix and try to use temporary shells.\n\n\n1.2.5 Installing Nix\n\n1.2.5.1 For Windows users only: some prerequisites\nIf you are on Windows, you need the Windows Subsystem for Linux 2 (WSL2) to run Nix. If you are on a recent version of Windows 10 or 11, you can simply run this as an administrator in PowerShell:\nwsl --install\nYou can find further installation notes at this official MS documentation.\nI recommend to activate systemd in Ubuntu WSL2, mainly because this supports other users than root running Nix. To set this up, please do as outlined this official Ubuntu blog entry:\n# in WSL2 Ubuntu shell\n\nsudo -i\nnano /etc/wsl.conf\nThis will open the /etc/wsl.conf in a nano, a command line text editor. Add the following line:\n[boot]\nsystemd=true\nSave the file with CTRL-O and then quit nano with CTRL-X. Then, type the following line in powershell:\nwsl --shutdown\nand then relaunch WSL (Ubuntu) from the start menu. For those of you running Windows, we will be working exclusively from WSL2 now. If that is not an option, then I highly recommend you set up a virtual machine with Ubuntu using VirtualBox for example, or dual-boot Ubuntu.\nInstalling (and uninstalling) Nix is quite simple, thanks to the installer from Determinate Systems, a company that provides services and tools built on Nix, and works the same way on Linux (native or WSL2) and macOS.\n\n\n1.2.5.2 Actually installing Nix\nDo not use your operating system’s package manager to install Nix. Instead, simply open a terminal and run the following line (on Windows, run this inside WSL):\n\ncurl --proto '=https' --tlsv1.2 -sSf \\\n  -L https://install.determinate.systems/nix | \\\n  sh -s -- install\n\nThen, install the cachix client and configure the rstats-on-nix cache: this will install binary versions of many R packages which will speed up the building process of environments:\nnix-env -iA cachix -f https://cachix.org/api/v1/install\nthen use the cache:\ncachix use rstats-on-nix\nYou only need to do this once per machine you want to use {rix} on. Many thanks to Cachix for sponsoring the rstats-on-nix cache!\nIf you get this warning when trying to install software with Nix:\nwarning: ignoring the client-specified setting 'trusted-public-keys', because it is a restricted setting and you are not a trusted user\nwarning: ignoring untrusted substituter 'https://rstats-on-nix.cachix.org', you are not a trusted user.\nRun `man nix.conf` for more information on the `substituters` configuration option.\nwarning: ignoring the client-specified setting 'trusted-public-keys', because it is a restricted setting and you are not a trusted user\nThen this means that configuration was not successful. You need to add your user to /etc/nix/nix.custom.conf:\nsudo vim /etc/nix/nix.custom.conf\nthen simply add this line in the file:\ntrusted-users = root YOURUSERNAME\nwhere YOURUSERNAME is your current login user name.\n\n\n\n1.2.6 Temporary shells\nYou now have Nix installed; before continuing, it let’s see if everything works (close all your terminals and reopen them) by droping into a temporary shell with a tool you likely have not installed on your machine.\nOpen a terminal and run:\nwhich sl\nyou will likely see something like this:\nwhich: no sl in ....\nnow run this:\nnix-shell -p sl\nand then again:\nwhich sl\nthis time you should see something like:\n/nix/store/cndqpx74312xkrrgp842ifinkd4cg89g-sl-5.05/bin/sl\nThis is the path to the sl binary installed through Nix. The path starts with /nix/store: the Nix store is where all the software installed through Nix is stored. Now type sl and see what happens!\nYou can find the list of available packages here.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Reproducibility with Nix</span>"
    ]
  },
  {
    "objectID": "02-intro-nix.html#session-1.2-dev-environments-with-nix-1h30",
    "href": "02-intro-nix.html#session-1.2-dev-environments-with-nix-1h30",
    "title": "1  Reproducibility with Nix",
    "section": "1.3 Session 1.2 – Dev Environments with Nix (1h30)",
    "text": "1.3 Session 1.2 – Dev Environments with Nix (1h30)\n\n1.3.1 Some Nix concepts\nWhile temporary shells are useful for quick testing, this is not how Nix is typically used in practice. Nix is a declarative package manager: users specify what they want to build, and Nix takes care of the rest.\nTo do so, users write files called default.nix that contain the a so-called Nix expression. This expression will contain the definition of a (or several) derivations.\nIn Nix terminology, a derivation is a specification for running an executable on precisely defined input files to repeatably produce output files at uniquely determined file system paths. (source)\nIn simpler terms, a derivation is a recipe with precisely defined inputs, steps, and a fixed output. This means that given identical inputs and build steps, the exact same output will always be produced. To achieve this level of reproducibility, several important measures must be taken:\n\nAll inputs to a derivation must be explicitly declared.\nInputs include not just data files, but also software dependencies, configuration flags, and environment variables, essentially anything necessary for the build process.\nThe build process takes place in a hermetic sandbox to ensure the exact same output is always produced.\n\nThe next sections of this document explain these three points in more detail.\n\n\n1.3.2 Derivations\nHere is an example of a simple Nix expression:\nlet\n\n pkgs = import (fetchTarball \"https://github.com/rstats-on-nix/nixpkgs/archive/2025-04-11.tar.gz\") {};\n\nin\n\npkgs.stdenv.mkDerivation {\n  name = \"filtered_mtcars\";\n  buildInputs = [ pkgs.gawk ];\n  dontUnpack = true;\n  src = ./mtcars.csv;\n  installPhase = ''\n    mkdir -p $out\n    awk -F',' 'NR==1 || $9==\"1\" { print }' $src &gt; $out/filtered.csv\n  '';\n}\nI won’t go into details here, but what’s important is that this code uses awk, a common Unix data processing tool, to filter the mtcars.csv file to keep only rows where the 9th column (the am column) equals 1. As you can see, a significant amount of boilerplate code is required to perform this simple operation. However, this approach is completely reproducible: the dependencies are declared and pinned to a specific dated branch of our rstats-on-nix/nixpkgs fork (more on this later), and the only thing that could make this pipeline fail (though it’s a bit of a stretch to call this a pipeline) is if the mtcars.csv file is not provided to it. This expression can be instantiated into a derivation, and the derivation is then built into the actual output that interests us, namely the filtered mtcars data.\nThe derivation above uses the Nix builtin function mkDerivation: as its name implies, this function makes a derivation. But there is also mkShell, which is the function that builds a shell instead. Nix expressions that built a shell is the kind of expressions {rix} generates for you.\n\n\n1.3.3 Using {rix} to generate development environments\nIf you have successfully installed Nix, but don’t have yet R installed on your system, you could install R as you would usually do on your operating system, and then install the {rix} package, and from there, generate project-specific expressions and build them. But you could also install R using Nix. Actually, I would even recommend you uninstall R and delete all your packages from your computer and only manager R environments using Nix.\nRunning the following line in a terminal will drop you in an interactive R session that you can use to start generating expressions:\nnix-shell -p R rPackages.rix\nThis will drop you in a temporary shell with R and {rix} available. Navigate to an empty directory to help a project, call it rix-session-1:\nmkdir rix-session-1\nand start R and load {rix}:\nR\nlibrary(rix)\nyou can now generate an expression by running the following code:\nrix(\n  date = \"2025-08-04\",\n  r_pkgs = c(\"dplyr\", \"ggplot2\"), # add languageserver if you plan to use VS Code\n  py_conf = list(\n    py_version = \"3.13\",\n    py_pkgs = c(\"polars\", \"great-tables\")\n  ),\n  ide = \"none\",\n  project_path = \".\",\n  overwrite = TRUE\n)\nThis will write a file called default.nix in your project’s directory. This default.nix contains a Nix expression which will build a shell that comes with R, {dplyr} and {ggplot2} as they were on the the 4th of August 2025 on CRAN. This will also add Python 3.13 and the polars and great-tables Python packages as they were at the time in nixpkgs (more on this later). Finally, the ide argument is set to \"none\", because we don’t want to use Nix to manage an IDE. However, you should know that this is possible and useful in some cases. See this vignette for learning how to setup your IDE with Nix if you wish to do so. At the end of this chapter we will learn how to set up Positron.\n\n\n1.3.4 Using nix-shell to Launch Environments\nOnce your file is in place, simply run:\nnix-shell\nThis gives you an isolated shell session with all declared packages available. You can test code, explore APIs, or install further tools within this session.\nTo remove the packages that were installed, call nix-store --gc. This will call the garbage collector. If you want to avoid that an environment gets garbage-collected, use nix-build instead of nix-shell. This will create a symlink called result in your project’s root directory and nix-store --gc won’t garbage-collect this environment until you manually remove result.\n\n\n1.3.5 Pinning with nixpkgs\nTo ensure long-term reproducibility, a pinned the version of Nixpkgs is used:\nlet\n  pkgs = import (fetchTarball \"https://github.com/rstats-on-nix/nixpkgs/archive/2025-08-04.tar.gz\") {};\nin\n...\nThis is done automatically by {rix}. You could change the date manually if you prefer, but I would recommend to always regenerate the default.nix using {rix}.\n\n\n1.3.6 Installing Python packages not available via nixpkgs (impure)\nNot all Python packages can be installed through Nix; unlike CRAN, Pypi doesn’t get automatically mirrored and individual packages fixed by volunteers. Instead, specific Python packages get packaged individually for Nix. Thus, it could very well be the case that a specific Python package (or version of a Python package) that you need for a project is not available via nixpkgs.\nIn this case, it is still possible to use Python-specific package managers, like uv, to install packages. This is also useful if you work on a project with colleagues that use uv and that don’t want (yet) to use Nix. uv, is 10-100x faster than pip and also generates a lock file for improved reproducibility.\nThe idea is to install uv in your shell (but not any Python nor Python packages):\n\nrix(\n  ...\n  system_pkgs = c(\"uv\"),\n  ...\n)\n\nAnd then use uv from your shell as you would usually. We recommend specifying Python packages in a requirements.txt file, and specifying explicit versions (e.g., scanpy==1.11.4). Finally, we also recommend setting a shell hook to set up the virtual environment and install the packages from the requirements.txt when entering the shell (mind the quotes):\nrix(\n  ...\n  system_pkgs = c(\"uv\"),\n  shell_hook = \"\n      if [ ! -f pyproject.toml ]; then\n        uv init --python 3.13.5 # or whichever Python version you need\n      fi\n        uv add --requirements requirements.txt\n      # Create alias so python uses uvs environment\n      alias python='uv run python'\n  \",\n)\nAfter running nix-shell, uv should initalize a Python project with the specified Python version and install the packages listed in requirements.txt within the nix environment. This will take place each time nix-shell is called, however this will be cached and not installed each time.\nTo make sure everything works fine, you could simply start a Python interpreter and try to load numpy. This should work fine, but if it doesn’t, the following error could be raised:\nImportError:\n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.13 from \"/home/user/projects/rix_uv/.venv/bin/python3\"\n  * The NumPy version is: \"2.2.6\"\n\nand make sure that they are the versions you expect. Please carefully study the\ndocumentation linked above for further help.\n\nOriginal error was: libstdc++.so.6: cannot open shared object file: No such file or directory\nThat is an issue when using wheels (wheels are binaries of Python packages that get installed by default using uv). These wheels expect certain libraries to be in certain places. One way to solve this is to add the following to your shell hook:\nshellHook = ''\n   # Export LD_LIBRARY is required for python packages that dynamically load libraries, such as numpy \n   export LD_LIBRARY_PATH=\"${pkgs.lib.makeLibraryPath (with pkgs; [ zlib gcc.cc glibc stdenv.cc.cc ])}\":LD_LIBRARY_PATH;\n   ...\n   '';\nYour environment should now work.\nIf this seems complicated: yes, and that is actually exactly the type of problems that Nix aims to solve. However, there are just too many Python packages to automate their inclusion into nixpkgs like how it’s done for R. If you can, prefer using the Python packages included in nixpkgs.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Reproducibility with Nix</span>"
    ]
  },
  {
    "objectID": "02-intro-nix.html#configuring-your-ide",
    "href": "02-intro-nix.html#configuring-your-ide",
    "title": "1  Reproducibility with Nix",
    "section": "1.4 Configuring your IDE",
    "text": "1.4 Configuring your IDE\n\n1.4.1 Pre-requisites\nWe now need to configure an IDE to use both our Nix shells as development environments, and GitHub Copilot. You are free to use whatever IDE you want but the instructions below are going to focus on Positron, which is a fork of VS Code geared towards data science. It works well with both Python and R and makes it quite easy to choose the right R or Python interpreter (which you’ll have to do to make sure you’re using the one provided by Nix, see here).\nIf you want to use VS Code proper, you can follow all the instructions here, but you need to install the REditorSupport and the Python extension. You will also need to add the {languageserver} R package to the shell:\nrix(\n  date = \"2025-08-04\",\n  r_pkgs = c(\"dplyr\", \"ggplot2\", \"languageserver\"),\n  py_conf = list(\n    py_version = \"3.13\",\n    py_pkgs = c(\"polars\", \"great-tables\")\n  ),\n  ide = \"none\",\n  project_path = \".\",\n  overwrite = TRUE\n)\nand re-create the default.nix file.\nYou’ll need to carefully select the Python interpereter provided by Nix, and not the other environments that you might already have on your system. Also, we recommend you uninstall R if it’s installed system-wide and also remove your local library of packages and instead only use dedicated Nix shells to manage your projects. While we made our possible for Nix shells to not interfere with a system-installed R, we recommend users go into the habit of taking some minutes at the start of a project to properly set up their development environment.\nOn Windows, you need to install Positron on Windows, not inside WSL. Then, install the Open Remote extension right from Positron itself.\n\n\n1.4.2 direnv\nOnce Positron is installed, you need to install a piece of software called direnv: direnv will automatically load Nix shells when you open a project that contains a default.nix file in an editor. It works on any operating system and many editors support it, including Positron. If you’re using Windows, install direnv in WSL (even though you’ve just installed Positron for Windows). To install direnv run this command:\nnix-env -f '&lt;nixpkgs&gt;' -iA direnv\nThis will install direnv and make it available even outside of Nix shells!\nThen, we highly recommend to install the nix-direnv extension:\nnix-env -f '&lt;nixpkgs&gt;' -iA nix-direnv\nIt is not mandatory to use nix-direnv if you already have direnv, but it’ll make loading environments much faster and seamless.\nFinally, if you haven’t used direnv before, don’t forget this last step to make your terminal detected and load direnv automatically.\nThen, in Positron, install the direnv extension. Finally, add a file called .envrc and simply write the following two lines in it (this .envrc file should be in the same folder as your project’s default.nix):\nuse nix\nmkdir $TMP\nin it. On Windows, remotely connect to WSL first, but on other operating systems, simply open the project’s folder using File &gt; Open Folder... and you will see a pop-up stating direnv: /PATH/TO/PROJECT/.envrc is blocked and a button to allow it. Click Allow and then open an R script. You might get another pop-up asking you to restart the extension, so click Restart. Be aware that at this point, direnv will run nix-shell and so will start building the environment. If that particular environment hasn’t been built and cached yet, it might take some time before Code will be able to interact with it. You might get yet another popup, this time from the R Code extension complaining that R can’t be found. In this case, simply restart Code and open the project folder again: now it should work every time.\nSometimes, dependending on platform and installed packages, Positron might not be able to correctly load a Nix-managed Python interpreter. This happens when it tries to load its own version of ipykernal. To make sure this doesn’t cause issues, add these lines to the shell’s definition in default.nix:\nshellHook = ''\n   # Export LD_LIBRARY is required for python packages that dynamically load libraries, such as numpy \n   export LD_LIBRARY_PATH=\"${pkgs.lib.makeLibraryPath (with pkgs; [ zlib gcc.cc glibc stdenv.cc.cc ])}\":LD_LIBRARY_PATH;\n   ...\n   '';\n\n\n1.4.3 In summary and next steps\nFor a new project, simply repeat this process:\n\nGenerate the project’s default.nix file;\nBuild it using nix-build;\nCreate an .envrc and write the two lines from above in it;\nOpen the project’s folder in Code and click allow when prompted;\nRestart the extension and Code if necessary.\n\nAnother option is to create the .envrc file and write use nix in it, then open a terminal, navigate to the project’s folder, and run direnv allow. Doing this before opening Positron should not prompt you anymore.\nIf you’re on Windows, using Positron like this is particularly interesting, because it allows you to install Positron on Windows as usual, and then you can configure it to interact with a Nix shell, even if it’s running from WSL. This is a very seamless experience.\nNow configure Positron to use GitHub Copilot: click here.\nYou can enable LLM tab-completion if you want. I’m not a fan of this, I prefer simply using the interface and using LLMs as an advanced rubber duck: https://github.com/copilot/.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Reproducibility with Nix</span>"
    ]
  },
  {
    "objectID": "02-intro-nix.html#starting-a-new-project",
    "href": "02-intro-nix.html#starting-a-new-project",
    "title": "1  Reproducibility with Nix",
    "section": "1.5 Starting a new project:",
    "text": "1.5 Starting a new project:\nTo start a new project, create a new directory, and cd into it. Then, in that folder, create a new script called gen-env.R and add the following:\nlibrary(rix)\n\nrix(\n  date = \"2025-09-22\",\n  r_pkgs = c(\n    \"languageserver\", # if you want to use VS Code\n    \"tidyverse\" # or whatever packages\n  ),\n  py_conf = list(\n    py_version = \"3.13\",\n    py_pkgs = c(\"polars\", \"great-tables\")\n  ),\n  ide = \"none\",\n  project_path = \".\",\n  overwrite = TRUE,\n  print = TRUE\n)\nThen call:\nnix-shell -I nixpkgs=https://github.com/rstats-on-nix/nixpkgs/archive/refs/heads/2025-09-22.tar.gz -p R rPackages.rix\nproviding a url to my fork of nixpkgs makes sure that you get the latest rix version (change the date in the url to a more recent one if needed). Start R by typing R in the shell, and then source(\"gen-env.R\"). That will create a default.nix which you can then drop into using nix-shell. Then create an .envrc and add the lines from before, and you’re done!\nNote: if you want to use uv instead of Nix to manage the Python packages, follow the instructions above.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Reproducibility with Nix</span>"
    ]
  },
  {
    "objectID": "02-intro-nix.html#hands-on-exercises",
    "href": "02-intro-nix.html#hands-on-exercises",
    "title": "1  Reproducibility with Nix",
    "section": "1.6 Hands-On Exercises",
    "text": "1.6 Hands-On Exercises\n\nStart a temporary shell with R and {rix} again using nix-shell -p R rPackages.rix. Start an R session (by typing R) and then load the {rix} package (using library(rix)). Run the available_dates() function: using the latest available date, generate a new default.nix.\nInside of an activated shell, type which R and echo $PATH. Explore what is being added to your environment. What is the significance of paths like /nix/store/...?\nBreak it on purpose: generate a new environment with a wrong R package name, for example dplyrnaught. Try to build the environment. What happens?\nGo to https://search.nixos.org/packages and look for packages that you usually use for your projects to see if they are available.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Reproducibility with Nix</span>"
    ]
  },
  {
    "objectID": "03-git.html",
    "href": "03-git.html",
    "title": "2  Version Control with Git",
    "section": "",
    "text": "2.1 Introduction\nWhat you’ll learn by the end of this chapter:\nGit is a software for version control. Version control is absolutely essential in software engineering, or when setting up a RAP. If you don’t install a version control system such as Git, don’t even start trying to set up a RAP. But what does a version control system like Git actually do? The basic workflow of Git is as follows: you start by setting up a repository for a project. On your computer, this is nothing more than a folder with your scripts in it. However, if you’re using Git to keep track of what’s inside that folder, there will be a hidden .git folder with a bunch of files in it. You can forget about that folder, this is for Git’s own internal needs. What matters, is that when you make changes to your files, you can first commit these changes, and then push them back to a repository. Collaborators can copy this repository and synchronize their files saved on their computers with your changes. Your collaborators can then also work on the files, then commit and push the changes to the repository as well.\nYou can then pull back these changes onto your computer, add more code, commit, push, etc… Git makes it easy to collaborate on projects either with other people, or with future you. It is possible to roll back to previous versions of your code base, you can create new branches of your project to test new features (without affecting the main branch of your code), collaborators can submit patches that you can review and merge, and and and…\nIn my experience, learning Git is one of the most difficult things there is for students. And this is because Git solves a complex problem, and there is no easy way to solve a complex problem. But I would however say that Git is not unnescessarily complex, and in any case it’s absolutely essential in our line of work. It is simply not possible to not know at least some basics of Git. And this is what we’re going to do, learn the basics, it’ll keep us plenty busy already.\nBut for now, let’s pause for a brief moment and watch this video that explains in 2 minutes the general idea of Git.\nLet’s get started.\nYou might have heard of github.com: this is a website that allows programmers to set up repositories on which they can host their code. The way to interact with github.com is via Git; but there are many other website like github.com, such as gitlab.com and bitbucket.com.\nFor this course, you should create an account on github.com. This should be easy enough. Then you should install Git on your computer.\nAnother advantage of using GitHub is that, as students, you will have access to Copilot for free. We will be using Copilot as our LLM for pair programming throughout the rest of this course. Get GitHub education here.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Version Control with Git</span>"
    ]
  },
  {
    "objectID": "03-git.html#installing-git",
    "href": "03-git.html#installing-git",
    "title": "2  Version Control with Git",
    "section": "2.2 Installing Git",
    "text": "2.2 Installing Git\nInstalling Git is not hard; it installs like any piece of software on your computer. If you’re running a Linux distribution, chances are you already have Git installed. To check if it’s already installed on a Linux system, open a terminal and type which git. If a path gets returned, like usr/bin/git, congratulations, it’s installed, if the command returns nothing you’ll have to install it. On Ubuntu, type sudo apt-get install git and just wait a bit. If you’re using macOS or Windows, you will need to install it manually. For Windows, download the installer from here, and for macOS from here; you’ll see that there are several ways of installing it on macOS, if you’ve never heard of homebrew or macports then install the binary package from here.\nIt would also be possible to install it with Nix, but because Git is also useful outside of development shells, it is better to have it installed at the level of your operating system.\nNext, configure git:\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"your.email@example.com\"",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Version Control with Git</span>"
    ]
  },
  {
    "objectID": "03-git.html#setting-up-a-repo",
    "href": "03-git.html#setting-up-a-repo",
    "title": "2  Version Control with Git",
    "section": "2.3 Setting up a repo",
    "text": "2.3 Setting up a repo\nOk so now that Git is installed, we can actually start using it. First, let’s start by creating a new repository on github.com. As I’ve mentioned in the introductory paragraph, Git will allow you to interact with github.com, and you’ll see in what ways soon enough. For now, login to your github.com account, and create a new repository by clicking on the ‘plus’ sign in the top right corner of your profile page and then choose ‘New repository’:\n\n\n\n\n\n\n\n\n\n\n\nIn the next screen, choose a nice name for your repository and ignore the other options, they’re not important for now. Then click on ‘Create repository’:\n\n\n\n\n\n\n\n\n\n\n\nOk, we’re almost done with the easy part. The next screen tells us we can start interacting with the repository. For this, we’re first going to click on ‘README’:\n\n\n\n\n\n\n\n\n\n\n\nThis will add a README file that we can also edit from github.com directly:\n\n\n\n\n\n\n\n\n\n\n\nAdd some lines to the file, and then click on ‘Commit new file’. You’ll end up on the main page of your freshly created repository. We are now done with setting up the repository on github.com. We can now clone the repository onto our machines. For this, click on ‘Code’, then ‘SSH’ and then on the copy icon:\n\n\n\n\n\n\n\n\n\n\n\nNow we’re going to work exclusively from the command line. While graphical interfaces for Git exist, learning the command line is essential because:\n\nMost servers run Linux and only provide command line access\nThe command line gives you access to all Git features\nUnderstanding the command line makes you more versatile as a developer\nMany advanced Git operations can only be done from the command line",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Version Control with Git</span>"
    ]
  },
  {
    "objectID": "03-git.html#cloning-the-repository-onto-your-computer",
    "href": "03-git.html#cloning-the-repository-onto-your-computer",
    "title": "2  Version Control with Git",
    "section": "2.4 Cloning the repository onto your computer",
    "text": "2.4 Cloning the repository onto your computer\nOpen your terminal (Linux/macOS) or WSL2 if on Windows. First, let’s navigate to where we want to store our repository. For example, let’s create a directory for our projects:\nmkdir ~/Documents/projects\ncd ~/Documents/projects\nNow let’s clone the repository. Use the SSH URL you copied from GitHub:\ngit clone git@github.com:yourusername/your-repo-name.git\nReplace yourusername and your-repo-name with your actual GitHub username and repository name.\nAfter cloning, navigate into the repository:\ncd your-repo-name\nls -la\nYou should see the files from your repository, including the README file you created, plus a hidden .git directory that contains Git’s internal files.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Version Control with Git</span>"
    ]
  },
  {
    "objectID": "03-git.html#setting-up-ssh-authentication",
    "href": "03-git.html#setting-up-ssh-authentication",
    "title": "2  Version Control with Git",
    "section": "2.5 Setting up SSH authentication",
    "text": "2.5 Setting up SSH authentication\nBefore we can push code from our computer to GitHub, we need a way to prove that we are who we say we are. While you can use a username and password (HTTPS), a more secure and professional method is to use SSH (Secure Shell) keys.\nThink of it this way:\n\nHTTPS (Password): Like using a password to unlock a door. You have to type it in frequently.\nSSH (Key): Like having a special key that unlocks the door automatically. You set it up once, and it grants you access without needing to re-enter a password.\n\nWe will create a pair of digital keys: a public key that we will give to GitHub, and a private key that will stay on our computer. When we try to connect, GitHub will use our public key to check if we have the matching private key, proving our identity.\nLet’s generate our SSH key pair. We’ll use the modern and highly secure Ed25519 algorithm. Open your terminal (or WSL2 on Windows) and run the following command, replacing the email with the one you used for GitHub:\nssh-keygen -t ed25519 -C \"your_email@example.com\"\nYou will be prompted with a few questions. Here is what you’ll see and how to answer:\n# Press Enter to accept the default file location\n&gt; Enter a file in which to save the key (/home/your_username/.ssh/id_ed25519): [Press Enter]\n\n# You can optionally set a passphrase.\n&gt; Enter passphrase (empty for no passphrase): [Press Enter]\n&gt; Enter same passphrase again: [Press Enter]\nWhat about the passphrase? A passphrase adds an extra layer of security. If someone were to steal your computer, they still couldn’t use your SSH key without knowing the passphrase. However, you would have to type it every time you interact with GitHub. For this course, it is fine to leave it empty for convenience by simply pressing Enter.\nAfter running the command, two files have been created in a hidden directory in your home folder called .ssh:\n\nid_ed25519: This is your private key. NEVER share this file with anyone or upload it anywhere. It must remain secret on your computer.\nid_ed25519.pub: This is your public key. The .pub stands for “public”. This is the key you can safely share and will upload to GitHub in the next step.\n\n\nNote for Older Systems: If the ssh-keygen command gives an error about ed25519 being an “invalid option”, your system might be too old to support it. In that rare case, you can use the older RSA algorithm instead: ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"\n\nNow that we have our key pair, our next task is to give the public key to GitHub. Let’s display the public key:\ncat ~/.ssh/id_ed25519.pub\nCopy the entire output (starting with ssh-rsa and ending with your email).\nGo to GitHub.com, click on your profile picture, then Settings → SSH and GPG keys → New SSH key. Paste your public key and give it a descriptive title.\nLet’s test the connection:\nssh -T git@github.com\nYou should see a message confirming successful authentication.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Version Control with Git</span>"
    ]
  },
  {
    "objectID": "03-git.html#your-first-commit",
    "href": "03-git.html#your-first-commit",
    "title": "2  Version Control with Git",
    "section": "2.6 Your first commit",
    "text": "2.6 Your first commit\nLet’s create a simple script and add some code to it (in what follows, all the code is going to get written into files using the command line, but you can also use your text editor to do it):\necho 'print(\"Hello, Git!\")' &gt; hello.py\nOr create a more complex example:\ncat &gt; analysis.R &lt;&lt; 'EOF'\n# Load data\ndata(mtcars)\n\n# Create a simple plot\nplot(mtcars$mpg, mtcars$hp,\n     xlab = \"Miles per Gallon\",\n     ylab = \"Horsepower\",\n     main = \"MPG vs Horsepower\")\nEOF\nNow let’s check the status of our repository:\ngit status\nYou’ll see that Git has detected new untracked files. Let’s add them to the staging area:\ngit add .\nThe . adds all files in the current directory. You can also add specific files:\ngit add analysis.R\nLet’s check the differences before committing:\ngit diff --staged\nThis shows what changes are staged for commit. Now let’s commit with a descriptive message:\ngit commit -m \"Add initial analysis script with basic plot\"\nLet’s check our commit history:\ngit log --oneline\nFinally, push our changes to GitHub:\ngit push origin main",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Version Control with Git</span>"
    ]
  },
  {
    "objectID": "03-git.html#understanding-git-workflow-commands",
    "href": "03-git.html#understanding-git-workflow-commands",
    "title": "2  Version Control with Git",
    "section": "2.7 Understanding Git workflow commands",
    "text": "2.7 Understanding Git workflow commands\nHere are the essential Git commands you’ll use daily:\nChecking status and differences:\ngit status              # Show working directory status\ngit diff                # Show unstaged changes\ngit diff --staged       # Show staged changes\ngit diff HEAD~1         # Compare with previous commit\nAdding and committing:\ngit add filename        # Stage specific file\ngit add .               # Stage all changes\ngit commit -m \"message\" # Commit with message\ngit commit -am \"msg\"    # Add and commit tracked files\nWorking with remote repositories:\ngit push origin main    # Push to main branch\ngit pull origin main    # Pull latest changes\ngit fetch               # Download changes without merging\nViewing history:\ngit log                 # Show detailed commit history\ngit log --oneline       # Show abbreviated history\ngit log --graph         # Show branching history\ngit show commit-hash    # Show specific commit details",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Version Control with Git</span>"
    ]
  },
  {
    "objectID": "03-git.html#working-with-commit-history",
    "href": "03-git.html#working-with-commit-history",
    "title": "2  Version Control with Git",
    "section": "2.8 Working with commit history",
    "text": "2.8 Working with commit history\nLet’s explore how to work with previous versions. First, let’s make another change:\necho '# This is a new line' &gt;&gt; analysis.R\ngit add analysis.R\ngit commit -m \"Add comment to analysis script\"\nView the commit history:\ngit log --oneline\nTo view a previous version without changing anything:\ngit checkout &lt;commit-hash&gt;\ncat analysis.R  # View the file at that point in time\nYou’ll be in “detached HEAD” state. To return to the latest version:\ngit checkout main\nTo permanently revert a commit (creates a new commit that undoes changes):\ngit revert &lt;commit-hash&gt;",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Version Control with Git</span>"
    ]
  },
  {
    "objectID": "03-git.html#collaborating-and-handling-conflicts",
    "href": "03-git.html#collaborating-and-handling-conflicts",
    "title": "2  Version Control with Git",
    "section": "2.9 Collaborating and handling conflicts",
    "text": "2.9 Collaborating and handling conflicts\nLet’s set up collaboration. Have a colleague invite you to their repository, or invite someone to yours. On GitHub, go to Settings → Manage access → Invite a collaborator.\nOnce you’re both collaborators, try this workflow:\n\nBoth of you clone the repository\nOne person makes changes and pushes:\n\necho 'library(ggplot2)' &gt; new_analysis.R\ngit add new_analysis.R\ngit commit -m \"Add ggplot2 analysis\"\ngit push origin main\n\nThe other person attempts to push their own changes:\n\necho 'data(iris)' &gt; new_analysis.R\ngit add new_analysis.R\ngit commit -m \"Add iris analysis\"\ngit push origin main  # This will fail!\nYou’ll get an error like ! [rejected] main -&gt; main (non-fast-forward). This sounds scary, but it’s Git’s safe way of telling you: “The remote repository on GitHub has changes that you don’t have on your computer. I’m stopping you from pushing because you would overwrite those changes.”\nTo solve this, you must first pull the changes from the remote repository and combine them with your local work. Git gives you two primary ways to do this: merging and rebasing.\n\n2.9.1 Strategy 1: Merging (The Default)\nIf you just run git pull, Git will perform a merge. It looks at the remote changes and your local changes and creates a new, special “merge commit” to tie the two histories together.\nImagine the history looks like this:\n\nYour colleague pushed commit D.\nYou worked locally and created commit C.\n\n      C (Your local work)\n     /\nA---B ---D (Remote work on GitHub)\nA git pull (which is git fetch + git merge) will result in this:\n      C-------E (Merge commit)\n     /       /\nA---B-------D\nThe history is now non-linear. While this accurately records that two lines of work were merged, it can clutter up the project history with many “Merge branch ‘main’…” commits, making it harder to read.\n\n\n2.9.2 Strategy 2: Rebasing (The Cleaner Way)\nThe second strategy is to rebase. Rebasing does something clever. It says: “Let me temporarily put your local changes aside. I’ll download the latest remote changes first. Then, I’ll take your changes and re-apply them one-by-one on top of the new remote history.”\nUsing the same scenario:\n\nStart: C (Your local work)          /     A---B ---D (Remote work on GitHub)\nRunning git pull --rebase does this:\n\nIt “unplugs” your commit C.\nIt fast-forwards your main branch to include D.\nIt then “re-plays” your commit C on top of D, creating a new commit C'.\n\nThe final result is a clean, single, linear history: A---B---D---C' (Your work is now on top)\n\nYour project’s history now looks like you did your work after your colleague, even if you did it at the same time. This makes the log much easier to read and understand.\nFor its clean, linear history, rebasing is the preferred method in many professional workflows, and it’s the one we will use.\nNow, let’s do it. To pull the remote changes and place your local commits on top, run:\ngit pull --rebase origin main\nIf there are no conflicts, Git will automatically complete the rebase. Your local work will now be neatly stacked on top of the remote changes, and your git push will succeed.\nIf there are conflicts, Git will pause the rebase process and tell you which files have conflicts. This happens when you and a collaborator changed the same lines in the same file.\ngit status  # Shows \"You are currently rebasing.\" and lists conflicted files\nYour job is to be the surgeon. Open the conflicted files (e.g., analysis.R). You will see Git’s conflict markers:\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\n# This is my version of the code\ndata(iris)\n=======\n# This is their version from the server\ndata(mtcars)\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; a1b2c3d... Add mtcars analysis\nManually edit the file to resolve the conflict. You must delete the &lt;&lt;&lt;&lt;&lt;&lt;&lt;, =======, and &gt;&gt;&gt;&gt;&gt;&gt;&gt; markers and decide what the final, correct version of the code should be. For example:\n# I decided to keep both datasets for now\ndata(iris)\ndata(mtcars)\nOnce you have fixed the file and saved it, you need to tell Git you’re done:\n# Mark the conflict as resolved\ngit add conflicted-file.R\n\n# Continue the rebase process\ngit rebase --continue\nGit will continue applying your commits one by one. If you have another conflict, repeat the process. Once the rebase is complete, you can finally push your work.\nFinally, push your changes:\ngit push origin main\nThis time, it should succeed.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Version Control with Git</span>"
    ]
  },
  {
    "objectID": "03-git.html#working-with-branches",
    "href": "03-git.html#working-with-branches",
    "title": "2  Version Control with Git",
    "section": "2.10 Working with branches",
    "text": "2.10 Working with branches\nBranches allow you to work on features without affecting the main codebase:\n# Create and switch to a new branch\ngit checkout -b feature-new-plots\n\n# Or use the newer syntax\ngit switch -c feature-new-plots\nList all branches:\ngit branch\nWork on your feature:\necho 'boxplot(mtcars$mpg ~ mtcars$cyl)' &gt;&gt; analysis.R\ngit add analysis.R\ngit commit -m \"Add boxplot analysis\"\nPush the branch to GitHub:\ngit push origin feature-new-plots\nSwitch back to main and merge your feature:\ngit checkout main\ngit merge feature-new-plots\nIf you’re done with the branch, delete it:\ngit branch -d feature-new-plots             # Delete locally\ngit push origin --delete feature-new-plots  # Delete on GitHub",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Version Control with Git</span>"
    ]
  },
  {
    "objectID": "03-git.html#advanced-workflow-with-branches",
    "href": "03-git.html#advanced-workflow-with-branches",
    "title": "2  Version Control with Git",
    "section": "2.11 Advanced workflow with branches",
    "text": "2.11 Advanced workflow with branches\nFor more complex workflows, you might want to keep branches separate and use pull requests on GitHub instead of direct merging:\n# Create feature branch\ngit checkout -b feature-advanced-stats\necho 'summary(lm(mpg ~ hp + wt, data = mtcars))' &gt;&gt; analysis.R\ngit add analysis.R\ngit commit -m \"Add linear regression analysis\"\ngit push origin feature-advanced-stats\nThen go to GitHub and create a Pull Request from the web interface. This allows for code review before merging.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Version Control with Git</span>"
    ]
  },
  {
    "objectID": "03-git.html#essential-daily-workflow",
    "href": "03-git.html#essential-daily-workflow",
    "title": "2  Version Control with Git",
    "section": "2.12 Essential daily workflow",
    "text": "2.12 Essential daily workflow\nHere’s the typical daily workflow:\n\nStart your day: Pull latest changes\n\ngit pull origin main\n\nCreate a feature branch:\n\ngit checkout -b feature-description\n\nWork and commit frequently:\n\n# Make changes\ngit add .\ngit commit -m \"Descriptive commit message\"\n\nPush your branch:\n\ngit push origin feature-description\n\nWhen feature is complete: Merge or create pull request\n\ngit checkout main\ngit pull origin main  # Get latest changes\ngit merge feature-description\ngit push origin main",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Version Control with Git</span>"
    ]
  },
  {
    "objectID": "03-git.html#a-better-way-to-collaborate-trunk-based-development",
    "href": "03-git.html#a-better-way-to-collaborate-trunk-based-development",
    "title": "2  Version Control with Git",
    "section": "2.13 A Better Way to Collaborate: Trunk-Based Development",
    "text": "2.13 A Better Way to Collaborate: Trunk-Based Development\nThe “Essential Daily Workflow” you just learned is a great start, but it leaves one important question unanswered: how long should a feature branch live? Days? Weeks? Months?\nA common mistake for new teams is to let branches live for a very long time. A data scientist might create a branch called feature-big-analysis, work on it for three weeks, and then try to merge it back into main. The result is often what’s called “merge hell”: main has changed so much in three weeks that merging the branch back in creates dozens of conflicts and is a painful, stressful process.\nTo avoid this, many professional teams use a workflow called Trunk-Based Development (TBD). The philosophy is simple but powerful:\n\nAll developers integrate their work back into the main branch (the “trunk”) as frequently as possible—at least once a day.\n\nThis means that feature branches are incredibly short-lived. Instead of a single, massive feature branch that takes weeks, you create many tiny branches that each take a few hours or a day at most.\nThe goal is to keep the main branch constantly updated with the latest code from everyone on the team. This has huge benefits:\n\nFewer Merge Conflicts: Because you are merging small changes frequently, the chance of conflicting with a teammate’s work is dramatically lower.\nEasier Code Reviews: Reviewing a small change that adds one function is much easier and faster than reviewing a 1,000-line change that refactors an entire analysis.\nContinuous Integration: Everyone is working from the most up-to-date version of the project, which reduces integration problems and keeps the project moving forward.\n\n\n2.13.1 How to Work with Short-Lived Branches\nBut how can you merge something back into main if the feature isn’t finished? The main branch must always be stable and runnable. You can’t merge broken code.\nThe first way to solve this issue is to use feature flags.\nA feature flag is just a simple variable (like a TRUE/FALSE switch) that lets you turn a new, unfinished part of the code on or off. This allows you to merge the code into main while keeping it “off” until it’s ready.\nImagine you are adding a new, complex plot to analysis.R, but it will take a few days to get right.\n# At the top of your analysis.R script\n# --- Configuration ---\nuse_new_scatterplot &lt;- FALSE # Set to FALSE while in development\n\n# ... lots of existing, working code ...\n\n# --- New Feature Code ---\nif (use_new_scatterplot) {\n  # All your new, unfinished, possibly-buggy plotting code goes here.\n  # It won't run as long as the flag is FALSE.\n  library(scatterplot3d)\n  scatterplot3d(mtcars$mpg, mtcars$hp, mtcars$wt)\n}\nWith this if block, you can safely merge your changes into main. The new code is there, but it won’t execute and won’t break the existing analysis. Other developers can pull your changes and won’t even notice. Once you’ve finished the feature in subsequent small commits, the final change is just to flip the switch: use_new_scatterplot &lt;- TRUE.\nThe second strategy is to stack pull requests. This is useful when a feature is too big for one small change, but it can be broken down into a logical sequence of steps. For example, to add a new analysis, you might need to:\n\nAdd a new data cleaning function.\nUse that function to process the data.\nGenerate a new plot from the processed data.\n\nInstead of putting all this in one giant Pull Request (PR), you can “stack” them. A stacked PR is a PR that is based on another PR branch, not on main.\nHere’s the workflow:\n\nCreate the first branch from main for the first step.\ngit switch -c add-cleaning-function\n# ...do the work, commit, and push...\nCreate a Pull Request on GitHub for this branch (add-cleaning-function -&gt; main).\nCreate the second branch from the first branch. This is the key step.\n\ngit switch -c process-the-data\n# ...do the work that DEPENDS on the cleaning function...\nCreate a new PR for this branch. On GitHub, when you create the PR, manually change the base branch from main to add-cleaning-function. Now this PR only shows the changes for step 2.\n\nYour team can now review and approve add-cleaning-function first. Once it’s merged into main, you go to your process-the-data PR on GitHub and change its base back to main. It will now be ready to merge after a quick update.\nThis approach breaks down large features into small, logical, reviewable chunks, keeping your development velocity high while adhering to the TBD philosophy.\nBy embracing short-lived branches, feature flags, and stacked PRs, you can make collaboration smoother, less stressful, and far more productive.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Version Control with Git</span>"
    ]
  },
  {
    "objectID": "03-git.html#contributing-to-someone-elses-repository",
    "href": "03-git.html#contributing-to-someone-elses-repository",
    "title": "2  Version Control with Git",
    "section": "2.14 Contributing to someone else’s repository",
    "text": "2.14 Contributing to someone else’s repository\nTo contribute to repositories you don’t have write access to:\n\nFork the repository on GitHub (click the Fork button)\nClone your fork:\n\ngit clone git@github.com:yourusername/original-repo-name.git\ncd original-repo-name\n\nAdd the original repository as upstream:\n\ngit remote add upstream git@github.com:originalowner/original-repo-name.git\n\nCreate a feature branch:\n\ngit checkout -b fix-issue-123\n\nMake changes and commit:\n\n# Make your changes\ngit add .\ngit commit -m \"Fix issue #123: describe what you fixed\"\n\nPush to your fork:\n\ngit push origin fix-issue-123\n\nCreate a Pull Request on GitHub from your fork to the original repository\n\nThis workflow is fundamental for contributing to open source projects and collaborating in professional environments.\nThe command line approach to Git gives you complete control and understanding of the version control process, making you a more effective developer and collaborator.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Version Control with Git</span>"
    ]
  },
  {
    "objectID": "03-git.html#working-with-llms-and-git-managing-ai-generated-changes",
    "href": "03-git.html#working-with-llms-and-git-managing-ai-generated-changes",
    "title": "2  Version Control with Git",
    "section": "2.15 Working with LLMs and Git: Managing AI-Generated Changes",
    "text": "2.15 Working with LLMs and Git: Managing AI-Generated Changes\nWhen working with Large Language Models (LLMs) like GitHub Copilot, ChatGPT, or Claude to generate or modify code, it’s crucial to review changes carefully before committing them. Git provides excellent tools for examining and selectively accepting or rejecting AI-generated modifications.\n\n2.15.1 The LLM workflow with Git\nHere’s a recommended workflow when using LLMs to modify your code:\n\nAlways commit your working code first:\n\ngit add .\ngit commit -m \"Working state before LLM modifications\"\n\nApply LLM suggestions to your files (copy-paste, or use tools that directly modify files)\nReview changes chunk by chunk using Git’s tools\nSelectively accept or reject changes\nCommit accepted changes with descriptive messages\n\n\n\n2.15.2 Examining LLM changes\nAfter an LLM has modified your files, use Git to see exactly what changed:\n# See all modified files\ngit status\n\n# See all changes at once\ngit diff\n\n# See changes in a specific file\ngit diff analysis.R\n\n# See changes with more context (10 lines before/after)\ngit diff -U10 analysis.R\nFor a more visual review, you can use Git’s word-level diff:\n# Show word-by-word changes instead of line-by-line\ngit diff --word-diff analysis.R\n\n# Show character-level changes\ngit diff --word-diff=color --word-diff-regex=.\n\n\n2.15.3 Interactive staging: Accepting changes chunk by chunk\nGit’s interactive staging feature (git add -p) is perfect for reviewing LLM changes. It lets you review each “hunk” (chunk of changes) individually:\ngit add -p\nThis will show you each chunk of changes and prompt you with options: - y - stage this hunk - n - do not stage this hunk - q - quit; do not stage this hunk or any remaining ones - a - stage this hunk and all later hunks in the file - d - do not stage this hunk or any later hunks in the file - s - split the current hunk into smaller hunks - e - manually edit the current hunk - ? - print help\n\n\n2.15.4 Example: Reviewing LLM changes to an R script\nLet’s say an LLM modified your analysis.R file. Here’s how to review it:\n# First, see what files were modified\ngit status\n\n# Review the changes\ngit diff analysis.R\nYou might see output like:\n@@ -1,8 +1,12 @@\n # Load required libraries\n-library(ggplot2)\n+library(ggplot2)\n+library(dplyr)\n+library(tidyr)\n \n # Load data\n data(mtcars)\n+mtcars &lt;- mtcars %&gt;% \n+  mutate(efficiency = ifelse(mpg &gt; 20, \"High\", \"Low\"))\n \n-# Create a simple plot\n-plot(mtcars$mpg, mtcars$hp)\n+# Create an improved plot with ggplot2\n+ggplot(mtcars, aes(x = mpg, y = hp, color = efficiency)) +\n+  geom_point(size = 3) +\n+  theme_minimal()\nNow use interactive staging to review each change:\ngit add -p analysis.R\nGit will show you each hunk and ask what to do. For example:\n@@ -1,2 +1,4 @@\n # Load required libraries\n library(ggplot2)\n+library(dplyr)\n+library(tidyr)\nStage this hunk [y,n,q,a,d,s,e,?]?\nYou might decide:\n\ny if you want the additional libraries\nn if you think they’re unnecessary\ns to split this into smaller chunks if you want only one library\n\n\n\n2.15.5 Advanced chunk management\nSometimes hunks are too large. Use s to split them:\n# When prompted with a large hunk\nStage this hunk [y,n,q,a,d,s,e,?]? s\nIf Git can’t split automatically, use e to manually edit:\nStage this hunk [y,n,q,a,d,s,e,?]? e\nThis opens your editor where you can:\n\nRemove lines you don’t want (delete the entire line)\nKeep lines by leaving them as-is\nLines starting with + are additions\nLines starting with - are deletions\nLines starting with  (space) are context\n\n\n\n2.15.6 Creating meaningful commits after LLM review\nAfter selectively staging changes, commit with descriptive messages:\n# Commit the staged changes\ngit commit -m \"Add dplyr and efficiency categorization\n\n- Added dplyr for data manipulation\n- Created efficiency category based on mpg &gt; 20\n- LLM suggested changes reviewed and approved\"\n\n# If there are remaining unstaged changes you want to reject\ngit checkout -- analysis.R  # Revert unstaged changes\n\n\n2.15.7 Working with multiple files modified by LLM\nWhen an LLM modifies multiple files, review them systematically:\n# See all changed files\ngit status\n\n# Review each file individually\ngit diff analysis.R\ngit diff data_processing.R\ngit diff visualization.R\n\n# Use interactive staging for each file\ngit add -p analysis.R\ngit add -p data_processing.R\n# ... etc\nOr stage all changes interactively at once:\ngit add -p\n\n\n2.15.8 Handling LLM-generated new files\nWhen an LLM creates entirely new files:\n# See new files\ngit status\n\n# Review new file content\ncat new_functions.R\n\n# Add if you approve\ngit add new_functions.R\n\n# Or ignore if you don't want it\necho \"new_functions.R\" &gt;&gt; .gitignore\n\n\n2.15.9 Using Git to compare LLM suggestions\nCreate a branch to safely experiment with LLM suggestions:\n# Create a branch for LLM experiments\ngit checkout -b llm-suggestions\n\n# Apply LLM changes\n# ... make modifications ...\n\n# Commit the LLM suggestions\ngit add .\ngit commit -m \"LLM suggestions for code improvement\"\n\n# Compare with original\ngit diff main..llm-suggestions\n\n# If you like some but not all changes, cherry-pick specific commits\ngit checkout main\ngit cherry-pick --no-commit &lt;commit-hash&gt;\ngit add -p  # Selectively stage parts of the cherry-picked changes\ngit commit -m \"Selected improvements from LLM suggestions\"\n\n\n2.15.10 Best practices for LLM + Git workflow\n\nAlways commit working code before applying LLM suggestions\nNever blindly accept all LLM changes - review each modification\nUse descriptive commit messages that mention LLM involvement\nTest code after accepting LLM suggestions before final commit\nKeep LLM-generated changes in separate commits for easier tracking\nUse branches for experimental LLM suggestions\nDocument why you accepted or rejected specific suggestions\n\n\n\n2.15.11 Example complete workflow\n# 1. Save current working state\ngit add .\ngit commit -m \"Working analysis script before LLM optimization\"\n\n# 2. Apply LLM suggestions (manually copy-paste or use tools)\n# ... LLM modifies your files ...\n\n# 3. Review all changes\ngit status\ngit diff\n\n# 4. Interactively stage only the changes you want\ngit add -p\n\n# 5. Commit approved changes\ngit commit -m \"LLM improvements: added data validation and error handling\n\nReviewed and approved:\n- Input validation for data loading\n- Error handling for missing values\n- Improved variable naming\n\nRejected:\n- Overly complex optimization that hurt readability\"\n\n# 6. Discard remaining unwanted changes\ngit checkout .\n\n# 7. Test the code\nRscript analysis.R  # or python script.py\n\n# 8. Push if everything works\ngit push origin main\nThis workflow ensures you maintain full control over your codebase while benefiting from LLM assistance, with complete traceability of what changes were made and why.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Version Control with Git</span>"
    ]
  },
  {
    "objectID": "03-git.html#hands-on-exercises",
    "href": "03-git.html#hands-on-exercises",
    "title": "2  Version Control with Git",
    "section": "2.16 Hands-on Exercises",
    "text": "2.16 Hands-on Exercises\n\n2.16.1 Exercise 1: Setup and First Commit (skip if you followed along)\nGoal: To confirm that Git is installed and configured correctly, and to practice the fundamental clone, add, commit, and push cycle.\n\nOn your GitHub account, create a new public repository named data-analysis-project.\nInitialize the repository with a README.md file directly on GitHub.\nOn your local machine, navigate to a suitable projects directory (e.g., ~/Documents/projects).\nClone your new repository to your local machine using the SSH URL.\nCreate a new file in the local repository called hello.py that contains a single line: print(\"Hello, Git!\").\nUse the command line to stage the new hello.py file.\nCommit the staged file with the message “Add initial Python script”.\nPush your commit to the main branch on GitHub.\nVerify that the hello.py file appears in your repository on the GitHub website.\n\n\n2.16.1.1 Exercise 2: Exploring History\nGoal: To practice inspecting the project’s history and learn how to revert changes.\n\nIn your local data-analysis-project repository, modify the README.md file by adding a new line: “This is my project for analyzing interesting data.”\nCommit this change with the message “Update README with project description”.\nNext, create a new file named analysis.R and add the following code: R     # Initial data exploration     data(iris)     summary(iris)\nCommit this new file with the message “Add initial analysis for iris dataset”.\nUse git log --oneline to view your commit history. You should see at least three commits.\nFind the commit hash for your “Update README with project description” commit.\nNow, create a new commit that reverts the README change. Use the git revert &lt;commit-hash&gt; command. A text editor will open for the revert commit message; you can leave the default message and just save and close it.\nPush your changes to GitHub. On the GitHub history for the main branch, you should now see a new “Revert…” commit, and the README.md file should no longer contain the project description.\n\n\n\n2.16.1.2 Exercise 3: Working with Branches\nGoal: To understand the workflow of creating a feature branch to work in isolation and then merging it back into the main branch.\n\nIn your local repository, create a new branch called feature-visualization.\nSwitch to the feature-visualization branch.\nIn the analysis.R file, add the following code at the end of the file to create a plot: R     # Add a simple plot of the data     plot(iris$Sepal.Length, iris$Sepal.Width, main=\"Iris Sepal Dimensions\")\nCommit this change to the feature-visualization branch with the message “Feat: Add sepal dimensions plot”.\nPush the feature-visualization branch to GitHub.\nSwitch back to the main branch on your local machine.\nMerge the feature-visualization branch into main.\nPush the main branch to see the merged changes on GitHub.\n(Optional) Delete the feature-visualization branch both locally and on the remote repository.\n\n\n\n2.16.1.3 Exercise 4: Resolving Conflicts\nGoal: To learn how to handle the common “non-fast-forward” error by using git pull --rebase and resolving a merge conflict.\n\nOn the GitHub website, navigate to your analysis.R file and use the “Edit” button to change the comment # Initial data exploration to # Initial exploration of the Iris dataset. Commit this change directly on GitHub.\nNow, on your local machine (without pulling the remote changes yet), edit the exact same line in your analysis.R file to be # R-Script for Iris Data.\nStage and commit this change locally with the message “Refactor: Update analysis script comment”.\nTry to push your change using git push origin main. The push will be rejected because your local history has diverged from the remote’s history.\nFetch the remote changes and re-apply your local commit on top by running git pull --rebase origin main. This will trigger a merge conflict.\nOpen analysis.R in a text editor. You will see the conflict markers (&lt;&lt;&lt;&lt;&lt;&lt;&lt;, =======, &gt;&gt;&gt;&gt;&gt;&gt;&gt;).\nResolve the conflict by deleting the markers and choosing a final version for the line (e.g., # R-Script for the initial exploration of the Iris dataset).\nAfter saving the file, stage the resolved file using git add analysis.R.\nContinue the rebase with git rebase --continue.\nFinally, push your successfully rebased and conflict-free history to GitHub.\n\n\n\n2.16.1.4 Exercise 5: Reviewing AI-Generated Code\nGoal: To practice the safe workflow for reviewing and selectively accepting code suggested by an LLM using interactive staging.\n\nFirst, commit any outstanding work in your repository to ensure you have a clean state.\nPretend an LLM has suggested an “improvement” to your hello.py file. Manually edit the file to look like this:\nimport sys\n\n# A function to greet\ndef greet(name):\n    \"\"\"A function that prints a greeting.\"\"\"\n    print(f\"Hello, {name}!\") # The main change we want\n    # The LLM also added this logging, which we don't want\n    print(\"Function finished execution.\", file=sys.stderr)\n\nif __name__ == '__main__':\n    greet(\"Git\")\nUse git diff to see the changes. Notice the unwanted print statement to sys.stderr.\nInstead of staging the whole file, use interactive staging: git add -p hello.py.\nGit will show you the chunk of changes. When prompted Stage this hunk [y,n,q,a,d,s,e,?]?, press e to manually edit the hunk.\nYour text editor will open with the patch. Delete the line +print(\"Function finished execution.\", file=sys.stderr). Save and close the editor.\nYou have now staged only the parts of the change you approved. Commit the staged changes with a message like “Feat: Refactor hello.py into a function”.\nRun git status. You will see that hello.py is still listed as modified. Use git diff to see the remaining change (the one you rejected).\nDiscard the unwanted change permanently with git checkout -- hello.py. Your working directory is now clean, and only the approved changes are in your commit history.\n\n\n\n2.16.1.5 Exercise 6: Contributing to another Project (Fork & Pull Request)\nGoal: To simulate contributing to an open-source project by forking a repository and submitting a Pull Request. For this exercise, use this repo to contribute to.\n\nWith your browser, navigate to the GitHub repository you want to contribute to (your partner’s or instructor’s).\nClick the Fork button in the top-right corner to create a copy of the repository under your own account.\nClone your fork to your local machine.\nAdd the original repository as a new remote called upstream. You can do this with: git remote add upstream git@github.com:original-owner/original-repo-name.git.\nCreate a new branch for your contribution, for example, fix-spelling-error.\nMake a small, helpful change. For example, find a typo in their README.md and fix it. Or add a new file CONTRIBUTORS.md and add your name.\nCommit your change with a clear commit message.\nPush your branch to your fork (origin), not upstream.\nOn GitHub, navigate to your forked repository. You should see a banner prompting you to “Contribute” and “Open a pull request”.\nOpen a Pull Request. Ensure the base repository is the original project’s main branch and the head repository is your fix-spelling-error branch. Write a short title and description for your change and submit it. The owner of the original repository can now review your work.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Version Control with Git</span>"
    ]
  },
  {
    "objectID": "04-functional-programming.html",
    "href": "04-functional-programming.html",
    "title": "3  Functional Programming",
    "section": "",
    "text": "3.1 Introduction: From Scripts to Functions\nWhat you’ll learn by the end of this chapter:\nSo far, we’ve established two pillars of reproducible data science:\nNow we turn to the third and arguably most important pillar: writing reproducible code itself. A common way to start a data analysis is by writing a script: a sequence of commands that are executed from top to bottom.\nThis works, but it has a hidden, dangerous property: state. The script relies on variables like heavy_cars existing in the environment, making the code hard to reason about, debug, and test. If scripting with state is a crack in the foundation of reproducibility, then using computational notebooks is a gaping hole.\nNotebooks like Jupyter introduce an even more insidious form of state: the cell execution order. You can execute cells out of order, meaning the visual layout of your code has no relation to how it actually ran. This is a recipe for non-reproducible results and a primary cause of the “it worked yesterday, why is it broken today?” problem.\nThe solution to this chaos is to embrace a paradigm that minimizes state: Functional Programming (FP). Instead of a linear script, we structure our code as a collection of self-contained, predictable functions. To support this, we will work exclusively in plain text files (.R, .py), which enforce a predictable, top-to-bottom execution, and use literate programming (using Quarto). The power of FP comes from the concept of purity, borrowed from mathematics. A mathematical function has a beautiful property: for a given input, it always returns the same output. sqrt(4) is always 2. Its result doesn’t depend on what you calculated before or on a random internet connection. Our Nix environments handle the “right library” problem; purity handles the “right logic” problem. Our goal is to write our analysis code with this same level of rock-solid predictability.\nTo appreciate what FP brings, it helps to contrast it with object-oriented programming (OOP) which is arguably the dominant paradigm in many software systems. OOP grew out of large industrial projects where different teams or departments managed different parts of a system. Each team could define the behavior of its component (an “object”) and expose a limited interface to the rest of the organization. The core idea was message passing: you send a message to an object, asking it to perform an action, without needing to know how it works internally. OOP organizes computation around who does what, a network of objects communicating with each other and managing their own internal state.\nFunctional programming, by contrast, organizes computation around how data changes. It replaces a network of interacting objects with a flow of transformations: data goes in, data comes out, and nothing else changes in the process. Rather than objects that carry both data and behavior, FP favors pure functions that take data as input and return new data as output. The emphasis shifts from actors to transformations — from modeling the world as a set of entities that act, to modeling it as a sequence of transformations applied to values.\nThis shift is especially powerful in data science. Analyses are naturally expressed as pipelines of transformations (cleaning, filtering, aggregating, modeling) where each step should depend only on its input, not on hidden state elsewhere in the program. Functional programming aligns perfectly with this workflow. Pure functions make results reproducible, because the same inputs always yield the same outputs. Immutability prevents accidental side effects on shared data. And because transformations can be composed, tested, and reused independently, FP encourages modular, maintainable analysis code. Finally, because pure functions are self-contained, they can be easily parallelized, making it easier to scale analyses across datasets or machines.\nIn short, OOP helps us build complex systems by modeling actors that exchange messages, while FP helps us build robust analyses by composing transformations of data. For data science, where clarity, reproducibility, and data flow matter most, the functional approach offers the stronger foundation.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Functional Programming</span>"
    ]
  },
  {
    "objectID": "04-functional-programming.html#introduction-from-scripts-to-functions",
    "href": "04-functional-programming.html#introduction-from-scripts-to-functions",
    "title": "3  Functional Programming",
    "section": "",
    "text": "Reproducible Environments (with Nix): Ensuring everyone has the exact same tools (R, Python, system libraries) to run the code.\nReproducible History (with Git): Ensuring everyone has the exact same version of the code and can collaborate effectively.\n\n\n# R script example\nlibrary(dplyr)\ndata(mtcars)\nheavy_cars &lt;- filter(mtcars, wt &gt; 4)\nmean_mpg_heavy &lt;- mean(heavy_cars$mpg)\nprint(mean_mpg_heavy)\n# Python script example\nimport pandas as pd\nmtcars = pd.read_csv(\"mtcars.csv\") # Assume the file exists\nheavy_cars = mtcars[mtcars['wt'] &gt; 4]\nmean_mpg_heavy = heavy_cars['mpg'].mean()\nprint(mean_mpg_heavy)\n\n\n\n\n\n\n\n\n3.1.1 Why Does This Matter for Data Science?\nAdopting a functional style brings massive benefits that directly connect to our previous chapters:\n\nUnit Testing is Now Possible: You can’t easily test a 200-line script. But you can easily test a small function that does one thing. Does calculate_mean_mpg(data) return the correct value for a sample dataset? This makes your code more reliable.\nCode Review is Easier (Git Workflow): As we saw in the Git chapter, reviewing a small, self-contained change is much easier than reviewing a giant, sprawling one. A Pull Request that just adds or modifies a single function is simple for your collaborators to understand and approve.\nWorking with LLMs is More Effective: It’s difficult to ask an LLM to “fix my 500-line analysis script.” It’s incredibly effective to ask, “Write a Python function that takes a pandas DataFrame and a column name, and returns the mean of that column, handling missing values. Also, write three pytest unit tests for it.” Functions provide the clear boundaries and contracts that LLMs excel at working with.\nReadability and Maintainability: Well-named functions are self-documenting.\nstarwars %&gt;% \n  group_by(species) %&gt;% \n  summarize(mean_height = mean(height))\nis instantly understandable. The equivalent for loop is a puzzle you have to solve.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Functional Programming</span>"
    ]
  },
  {
    "objectID": "04-functional-programming.html#purity-and-side-effects",
    "href": "04-functional-programming.html#purity-and-side-effects",
    "title": "3  Functional Programming",
    "section": "3.2 Purity and Side Effects",
    "text": "3.2 Purity and Side Effects\nA pure function has two rules:\n\nIt only depends on its inputs. It doesn’t use any “global” variables defined outside the function.\nIt doesn’t change anything outside of its own scope. It doesn’t modify a global variable or write a file to disk. This is called having “no side effects.”\n\nConsider this “impure” function in Python:\n# IMPURE: Relies on a global variable\ndiscount_rate = 0.10\n\ndef calculate_discounted_price(price):\n  return price * (1 - discount_rate) # What if discount_rate changes?\n\nprint(calculate_discounted_price(100))\n# &gt; 90.0\ndiscount_rate = 0.20 # Someone changes the state\nprint(calculate_discounted_price(100))\n# &gt; 80.0  -- Same input, different output!\nThe pure version passes all its dependencies as arguments:\n# PURE: All inputs are explicit arguments\ndef calculate_discounted_price_pure(price, rate):\n  return price * (1 - rate)\n\nprint(calculate_discounted_price_pure(100, 0.10))\n# &gt; 90.0\nprint(calculate_discounted_price_pure(100, 0.20))\n# &gt; 80.0\nNow the function is predictable and self-contained.\n\n3.2.1 Handling “Impure” Operations like Randomness\nSome operations, like generating random numbers, are inherently impure. Each time you run rnorm(10) or numpy.random.rand(10), you get a different result.\nThe functional approach is not to avoid this, but to control it by making the source of impurity (the random seed) an explicit input.\nIn R, the {withr} package helps create a temporary, controlled context:\nlibrary(withr)\n\n# This function is now pure! For a given seed, the output is always the same.\npure_rnorm &lt;- function(n, seed) {\n  with_seed(seed, {\n    rnorm(n)\n  })\n}\n\npure_rnorm(n = 5, seed = 123)\npure_rnorm(n = 5, seed = 123)\nIn Python, numpy provides a more modern, object-oriented way to handle this, which is naturally functional:\nimport numpy as np\n\n# Create a random number generator instance with a seed\nrng = np.random.default_rng(seed=123)\n\n# Now, calls on this 'rng' object are deterministic within its context\nprint(rng.standard_normal(5))\n\n# If we re-create the same generator, we get the same numbers\nrng2 = np.random.default_rng(seed=123)\nprint(rng2.standard_normal(5))\nThe key is the same: the “state” (the seed) is explicitly managed, not hidden globally.\nHowever, this introduces a concept from another programming paradigm: Object-Oriented Programming (OOP). The rng variable is not just a value; it’s an object that bundles together data (its internal seed state) and methods that operate on that data (.standard_normal()). This is called encapsulation. This is a double-edged sword for reproducibility. On one hand, it’s a huge improvement over hidden global state. On the other, the rng object itself is now a stateful entity. If we called rng.standard_normal(5) a second time, it would produce different numbers because its internal state would have been mutated by the first call.\nIn a purely functional world, we would avoid creating such stateful objects. However, in the pragmatic world of Python data science, this is often unavoidable. Core libraries like pandas, scikit-learn, and matplotlib are fundamentally object-oriented. You create DataFrame objects, model objects, and plot objects, all of which encapsulate state. Our guiding principle, therefore, must be one of careful management: use functions for the flow and logic of your analysis, and treat objects from libraries as values that are passed between these functions. Avoid building your own complex classes with hidden state for your data pipeline. A pipeline composed of functions (df2 = clean_data(df1); df3 = analyze_data(df2)) is almost always more transparent and reproducible than an object-oriented one (pipeline.load(); pipeline.clean(); pipeline.analyze()).\n\n\n3.2.2 Can We Make This Truly Pure?\nThis naturally raises this next question: can we force this numpy example to be truly pure? A pure function cannot have side effects, which means it cannot mutate the rng object’s internal state. To achieve this, our function must take the generator’s current state as an explicit input and return a tuple containing both the desired random numbers and the new, updated state of the generator.\nLet’s write a wrapper function that does exactly this:\nimport numpy as np\n\ndef pure_standard_normal(generator_state, n_samples):\n    \"\"\"\n    A pure function to generate standard normal random numbers.\n\n    Args:\n        generator_state: The current state of a numpy BitGenerator.\n        n_samples: The number of samples to generate.\n\n    Returns:\n        A tuple containing (random_numbers, new_generator_state).\n    \"\"\"\n    # 1. Create a temporary generator instance from the input state\n    temp_rng = np.random.Generator(np.random.PCG64(generator_state))\n\n    # 2. Generate the numbers (this mutates the *temporary* generator)\n    numbers = temp_rng.standard_normal(n_samples)\n\n    # 3. Extract the new state from the temporary generator\n    new_state = temp_rng.bit_generator.state\n\n    # 4. Return both the result and the new state\n    return (numbers, new_state)\n\n# --- How to use this pure function ---\n\n# 1. Get an initial state from a seed\ninitial_state = np.random.PCG64(123).state\n\n# 2. First call: provide the state, get back numbers and a *new* state\nfirst_numbers, state_after_first_call = pure_standard_normal(initial_state, 5)\nprint(\"First call results:\", first_numbers)\n\n# 3. Second call: MUST use the new state from the previous call\nsecond_numbers, state_after_second_call = pure_standard_normal(state_after_first_call, 5)\nprint(\"Second call results:\", second_numbers)\n\n# Proof of purity: If we re-use the initial state, we get the exact same \"first\" result\nproof_numbers, _ = pure_standard_normal(initial_state, 5)\nprint(\"Proof call results:\", proof_numbers)\nAs you can see, this is now 100% pure and predictable. The function pure_standard_normal will always produce the same output tuple for the same input tuple.\n\n3.2.2.1 Is This Feasible in Practice?\nWhile this is a powerful demonstration of functional principles, it is often not practical for day-to-day data science in Python. Manually passing the state variable from one function to the next throughout an entire analysis script (state_1, state_2, state_3…) would be extremely verbose and cumbersome.\nThe key takeaway is understanding the trade-off. The object-oriented approach (rng = np.random.default_rng(seed=123)) is a pragmatic compromise. It encapsulates the state in a predictable way, which is a vast improvement over hidden global state, even if it’s not technically “pure”. If you have to use Python: treat stateful library objects like rng as values that are created once with a fixed seed and passed into your pure analysis functions. This gives you 99% of the benefit of reproducibility with a fraction of the complexity.\nThis difference in the “feel” of functional composition between R’s pipe and Python’s method chaining is no accident; it reflects the deep-seated design philosophies of each language. This context is crucial for understanding why certain patterns feel more “natural” in each environment. R’s lineage traces back to the S language, which was itself heavily influenced by Scheme, a dialect of Lisp and a bastion of functional programming. Consequently, treating data operations as a series of function transformations is baked into R’s DNA. The entire Tidyverse ecosystem, with its ubiquitous pipe, is a modern implementation of this functional heritage.\nPython, in contrast, was designed with a different set of priorities, famously summarized in its Zen: “There should be one—and preferably only one—obvious way to do it.” Its creator, Guido van Rossum, historically argued that explicit for loops and list comprehensions were more readable and “Pythonic” than functional constructs like map and lambda. He was so committed to this principle of one clear path that he even proposed removing these functions from the language entirely at one point.\nR is fundamentally a functional language that has acquired object-oriented features, while Python is a quintessential object-oriented language with powerful functional capabilities. Recognizing this history helps explain why a chain of functions feels native in R, while method chaining on objects is the default in pandas and polars. My goal in this course is for you to master the functional paradigm so you can apply it effectively in either language, leveraging the native strengths of each.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Functional Programming</span>"
    ]
  },
  {
    "objectID": "04-functional-programming.html#writing-your-own-functions",
    "href": "04-functional-programming.html#writing-your-own-functions",
    "title": "3  Functional Programming",
    "section": "3.3 Writing Your Own Functions",
    "text": "3.3 Writing Your Own Functions\nLet’s learn the syntax. The goal is always to encapsulate a single, logical piece of work.\n\n3.3.0.1 In R\nR functions are first-class citizens. You can assign them to variables and pass them to other functions.\n# A simple function\ncalculate_ci &lt;- function(x, level = 0.95) {\n  # Calculate the mean and standard error\n  se &lt;- sd(x, na.rm = TRUE) / sqrt(length(x))\n  mean_val &lt;- mean(x, na.rm = TRUE)\n\n  # Calculate the confidence interval bounds\n  alpha &lt;- 1 - level\n  lower &lt;- mean_val - qnorm(1 - alpha/2) * se\n  upper &lt;- mean_val + qnorm(1 - alpha/2) * se\n\n  # Return a named vector\n  # the `return()` statement is not needed at the end\n  # but can be useful for early returning a result\n  c(mean = mean_val, lower = lower, upper = upper)\n}\n\n# Use it\ndata &lt;- c(1.2, 1.5, 1.8, 1.3, 1.6, 1.7)\ncalculate_ci(data)\nFor data analysis, you’ll often want to write functions that work with data frames and column names. The {dplyr} package uses a special technique called “tidy evaluation” for this.\nlibrary(dplyr)\n\n# A function that summarizes a column in a dataset\nsummarize_variable &lt;- function(dataset, var_to_summarize) {\n  dataset %&gt;%\n    summarise(\n      n = n(),\n      mean = mean({{ var_to_summarize }}, na.rm = TRUE),\n      sd = sd({{ var_to_summarize }}, na.rm = TRUE)\n    )\n}\n\n# The {{ }} (curly-curly) syntax tells dplyr to use the column name\n# passed into the function.\nstarwars %&gt;%\n  group_by(species) %&gt;%\n  summarize_variable(height)\nThis is incredibly powerful for creating reusable analysis snippets. To learn more, read about programming with {dplyr} here.\n\n\n3.3.0.2 In Python\nPython’s syntax is similar, using the def keyword. Type hints are a best practice for clarity.\nimport pandas as pd\nimport numpy as np\n\n# A function to summarize a column in a DataFrame\ndef summarize_variable_py(dataset: pd.DataFrame, var_to_summarize: str) -&gt; pd.DataFrame:\n  \"\"\"Calculates summary statistics for a given column.\"\"\"\n  summary = dataset.groupby('species').agg(\n      n=(var_to_summarize, 'size'),\n      mean=(var_to_summarize, 'mean'),\n      sd=(var_to_summarize, 'std')\n  ).reset_index()\n  return summary\n\n# Load data (assuming starwars.csv exists)\n# starwars_py = pd.read_csv(\"starwars.csv\")\n# summarize_variable_py(starwars_py, 'height')",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Functional Programming</span>"
    ]
  },
  {
    "objectID": "04-functional-programming.html#the-functional-toolkit-map-filter-and-reduce",
    "href": "04-functional-programming.html#the-functional-toolkit-map-filter-and-reduce",
    "title": "3  Functional Programming",
    "section": "3.4 The Functional Toolkit: Map, Filter, and Reduce",
    "text": "3.4 The Functional Toolkit: Map, Filter, and Reduce\nOnce you start thinking in functions, you’ll notice common patterns emerge. Most for loops can be replaced by one of three core functional concepts: mapping, filtering, or reducing. These operations are handled by “higher-order functions”—functions that take other functions as arguments. Mastering them is key to writing elegant, declarative code.\n\n3.4.1 1. Mapping: Applying a Function to Each Element\nThe pattern: You have a list of things, and you want to perform the same action on each element, producing a new list of the same length.\nThis is the most common replacement for a for loop. Instead of manually iterating and storing results, you just state your intent: “map this function over this list.”\n\n3.4.1.1 In R with purrr::map()\nThe {purrr} package is the gold standard for functional programming in R. The map() family is its workhorse.\n\nmap(): Always returns a list.\nmap_dbl(): Returns a vector of doubles (numeric).\nmap_chr(): Returns a vector of characters (strings).\nmap_lgl(): Returns a vector of logicals (booleans).\nmap_dfr(): Returns a data frame by row-binding the results.\n\nIn base R, we have lapply(), vapply(), apply(), but the {purrr} functions provide a more homogenous interface.\nExample: Calculate the mean of every column in a data frame.\nlibrary(purrr)\n\n# The classic for-loop way (verbose and clunky)\n# Allocate an empty vector with the right size\nmeans_loop &lt;- vector(\"double\", ncol(mtcars))\n\nfor (i in seq_along(mtcars)) {\n  means_loop[[i]] &lt;- mean(mtcars[[i]], na.rm = TRUE)\n}\n\nprint(means_loop)\n\n# The functional way with map_dbl()\nmeans_functional &lt;- map_dbl(mtcars, mean, na.rm = TRUE)\n\nprint(means_functional)\nThe map() version is not just shorter; it’s safer. You can’t make an off-by-one error, and you don’t have to pre-allocate means_loop. The code clearly states its purpose.\n\n\n3.4.1.2 In Python with List Comprehensions and map()\nPython’s most idiomatic tool for mapping is the list comprehension, which we saw earlier. It’s concise and highly readable.\nnumbers = [1, 2, 3, 4, 5]\nsquares = [n**2 for n in numbers]\n# &gt; [1, 4, 9, 16, 25]\nPython also has a built-in map() function, which returns a “map object” (an iterator). You usually wrap it in list() to see the results. It’s most useful when you already have a function defined.\ndef to_upper_case(s: str) -&gt; str:\n    return s.upper()\n\nwords = [\"hello\", \"world\"]\nupper_words = list(map(to_upper_case, words))\n# &gt; ['HELLO', 'WORLD']\n\n\n\n3.4.2 2. Filtering: Keeping Elements That Match a Condition\nThe pattern: You have a list of things, and you want to keep only the elements that satisfy a certain condition. The condition is defined by a function that returns TRUE or FALSE.\n\n3.4.2.1 In R with purrr::keep() or purrr::discard()\nkeep() retains elements where the function returns TRUE. discard() does the opposite. The base function is Filter().\nExample: From a list of data frames, keep only the ones with more than 100 rows.\n# setup: create a list of data frames\ndf1 &lt;- data.frame(x = 1:50)\ndf2 &lt;- data.frame(x = 1:200)\ndf3 &lt;- data.frame(x = 1:75)\nlist_of_dfs &lt;- list(a = df1, b = df2, c = df3)\n\n# The functional way to filter the list\nlarge_dfs &lt;- keep(list_of_dfs, ~ nrow(.x) &gt; 100)\nprint(names(large_dfs))\n\n\n3.4.2.2 In Python with List Comprehensions\nList comprehensions have a built-in if clause that makes filtering incredibly natural.\nnumbers = [1, 10, 5, 20, 15, 30]\n\n# Keep only numbers greater than 10\nlarge_numbers = [n for n in numbers if n &gt; 10]\n# &gt; [20, 15, 30]\nPython also has a built-in filter() function, which, like map(), returns an iterator.\ndef is_even(n: int) -&gt; bool:\n    return n % 2 == 0\n\nnumbers = [1, 2, 3, 4, 5, 6]\neven_numbers = list(filter(is_even, numbers))\n# &gt; [2, 4, 6]\n\n\n\n3.4.3 3. Reducing: Combining All Elements into a Single Value\nThe pattern: You have a list of things, and you want to iteratively combine them into a single summary value. You start with an initial value and repeatedly apply a function that takes the “current total” and the “next element.”\nThis is the most complex of the three but is powerful for things like summing, finding intersections, or joining a list of data frames.\n\n3.4.3.1 In R with purrr::reduce()\nExample: Find the total sum of a vector of numbers.\n# reduce() will take the first two elements (1, 2), apply `+` to get 3.\n# Then it takes the result (3) and the next element (3), applies `+` to get 6.\n# And so on.\ntotal_sum &lt;- reduce(c(1, 2, 3, 4, 5), `+`)\n\n# This is equivalent to 1 + 2 + 3 + 4 + 5\nprint(total_sum)\nThe base R function is called Reduce().\nA more practical data science example: find all the column names that are common to a list of data frames.\n# Get the column names of each df in the list\nlist_of_colnames &lt;- map(list_of_dfs, names)\nprint(list_of_colnames)\n\n# Use reduce with the `intersect` function to find common elements\ncommon_cols &lt;- reduce(list_of_colnames, intersect)\nprint(common_cols)\n\n\n3.4.3.2 In Python with functools.reduce\nThe reduce function was moved out of the built-ins and into the functools module in Python 3 because it’s often less readable than an explicit for loop for simple operations like summing (well, at least according to Python users…). However, it’s still the right tool for more complex iterative combinations.\nfrom functools import reduce\nimport operator\n\nnumbers = [1, 2, 3, 4, 5]\n\n# Use reduce with the addition operator to sum the list\ntotal_sum_py = reduce(operator.add, numbers)\n# &gt; 15\n\n# You can also use a lambda function\ntotal_product = reduce(lambda x, y: x * y, numbers)\n# &gt; 120",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Functional Programming</span>"
    ]
  },
  {
    "objectID": "04-functional-programming.html#the-power-of-composition",
    "href": "04-functional-programming.html#the-power-of-composition",
    "title": "3  Functional Programming",
    "section": "3.5 The Power of Composition",
    "text": "3.5 The Power of Composition\nThe final, beautiful consequence of a functional style is composition. You can chain functions together to build complex workflows from simple, reusable parts. This is exactly what the pipe operators (|&gt; in R, %&gt;% from {magrittr}) and method chaining (the . in pandas) are designed for.\nThis R code is a sequence of function compositions:\nstarwars %&gt;%\n  filter(!is.na(mass)) %&gt;%\n  select(species, sex, mass) %&gt;%\n  group_by(sex, species) %&gt;%\n  summarise(mean_mass = mean(mass), .groups = \"drop\")\nThis is equivalent to summarise(group_by(select(filter(starwars, ...)))). The pipe makes it readable.\nThe same idea applies in Python with pandas:\n# (starwars_py\n#  .dropna(subset=['mass'])\n#  .filter(items=['species', 'sex', 'mass'])\n#  .groupby(['sex', 'species'])\n#  ['mass'].mean()\n#  .reset_index()\n# )\nThe issue with method chaining though, is that this only works within the methods that are available for pandas.DataFrame objects. You could apply another function using pandas.DataFrame.apply() function, but you can’t pipe functions from different packages like you could in R (more on this in the next subsection).\nEach step is a function that takes a data frame and returns a new, transformed data frame. By combining map, filter, and reduce with this compositional style, you can express complex data manipulation pipelines without writing a single for loop. This makes your code more declarative, less prone to bugs, and easier to reason about—a perfect fit for a reproducible workflow.\n\n3.5.1 The Challenge of Composition in Python\nThe difficulty of function composition in Python, as you’ve noted, stems from fundamental differences in its primary programming paradigm compared to R. While both languages are powerful and versatile, their core designs influence how naturally they support the seamless chaining of functions. The crux of the matter lies in Python’s primarily encapsulated Object-Oriented Programming (OOP) versus R’s functional OOP and its use of polymorphic functions.\n\n3.5.1.1 Python’s Encapsulated OOP: Methods Belong to Objects\nAs already mention earlier in the chapter, Python is predominantly an object-oriented language where data and the functions that operate on that data are bundled together into objects. This concept is known as encapsulation. A class defines the blueprint for an object, and the functions defined within a class are called methods. These methods are intrinsically tied to the object’s class and are typically invoked using dot notation (.), as seen in the pandas example.\nThis tight coupling of methods to specific object types is the main reason why fluid composition can be challenging. Method chaining, while elegant, is limited to the methods that have been explicitly defined for a particular class. To apply a function from a different library or a user-defined function that isn’t a method of the object, you often need to use workarounds like apply() in pandas, which can break the intuitive flow of a composition chain.\nFurthermore, while Python supports functional programming concepts, they are not always as central to the language’s design as its OOP features. For instance, lambda functions in Python are limited to a single expression, which can make defining complex on-the-fly functions cumbersome.\n\n\n3.5.1.2 R’s Functional OOP: Functions are Polymorphic and Independent\nIn contrast, R was designed with a strong emphasis on functional programming. Its approach to object-orientation is described as functional OOP. In this paradigm, methods are not encapsulated within class definitions. Instead, functions are often “generic,” meaning they can behave differently depending on the class of the object passed to them. This is a form of polymorphism.\nThis design choice has a profound impact on composition. Because functions are not strictly owned by objects, they can be more easily and flexibly combined. The pipe operators in R (|&gt; and %&gt;%) are a testament to this, allowing for the creation of highly readable and complex data manipulation pipelines by passing data through a series of independent functions. Each function takes data as an input and returns a transformed version, which is then passed to the next function in the chain.\n\n\n3.5.1.3 The Core Distinction: “Has-a” vs. “Is-a” and its Impact on Composition\nThe principle of “favor composition over inheritance” is a well-known software design guideline. Inheritance models an “is-a” relationship (a Dog is an Animal), while composition models a “has-a” relationship (a Car has an Engine).\n\nPython’s encapsulated OOP often encourages the use of inheritance, where a class inherits methods from a parent class. While powerful, this can lead to rigid hierarchies.\nR’s functional approach naturally favors a compositional style. You build complex operations by combining simpler, single-purpose functions. This aligns well with the “has-a” model, where a data analysis pipeline “has a” filtering step, a selection step, and a summarization step.\n\nIn essence, Python’s strength lies in creating well-defined, encapsulated objects with specific behaviors. R’s strength, particularly in data analysis, is in its ability to fluidly combine and apply functions to data. This makes the “compositional style” a more natural fit for the R ecosystem. While Python can achieve similar results, it often requires more deliberate effort to break out of the strict object-method paradigm to achieve the same level of compositional freedom.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Functional Programming</span>"
    ]
  },
  {
    "objectID": "04-functional-programming.html#conclusion-functions-as-the-bedrock-of-reproducibility",
    "href": "04-functional-programming.html#conclusion-functions-as-the-bedrock-of-reproducibility",
    "title": "3  Functional Programming",
    "section": "3.6 Conclusion: Functions as the Bedrock of Reproducibility",
    "text": "3.6 Conclusion: Functions as the Bedrock of Reproducibility\nThis chapter has laid the groundwork for the most critical pillar of reproducible data science: writing reproducible code. We have moved beyond the ephemeral, state-dependent nature of scripts and computational notebooks to embrace the discipline and predictability of Functional Programming.\nBy treating functions as our primary unit of work, we unlock a cascade of benefits. Pure functions, which guarantee the same output for the same input, form the core of this approach. They are transparent, easy to reason about, and, most importantly, testable. When we encounter inherently “impure” operations like random number generation, we’ve learned to control the impurity by making the source of impurity (in this example the random seed) an explicit and managed input, rather than a hidden global state.\nWe’ve replaced verbose and error-prone for loops with the powerful functional trio of map, filter, and reduce. These higher-order functions allow us to express complex data manipulations declaratively, stating what we want to do rather than detailing how to do it. This leads to code that is not only more concise but also less prone to bugs.\nFinally, we explored composition, the elegant chaining of these simple functions to build sophisticated analysis pipelines. We saw how this concept manifests differently in R and Python, a direct reflection of their core design philosophies. R’s functional heritage makes composition via the pipe (%&gt;% or |&gt;) a natural and seamless experience. Python’s object-oriented nature favors method chaining on objects like pandas DataFrames, a powerful but more constrained form of composition.\nUnderstanding this distinction is key to becoming an effective data scientist in any language. By mastering the functional paradigm, you are not just learning a new coding style; you are adopting a new way of thinking. You are building a foundation for code that is robust, easy to review, simple to debug, and truly reproducible—the ultimate goal of any serious analytical project.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Functional Programming</span>"
    ]
  },
  {
    "objectID": "04-functional-programming.html#exercises",
    "href": "04-functional-programming.html#exercises",
    "title": "3  Functional Programming",
    "section": "3.7 Exercises",
    "text": "3.7 Exercises\nThe following exercises will help you solidify your understanding of functional programming concepts in both R and Python. Use built-in datasets like iris or mtcars for R, and you can load them into pandas DataFrames for the Python exercises.\n\n3.7.1 1. From Impure to Pure\nGoal: Refactor a function that relies on global state into a pure function.\n\nR: The following function filters mtcars to find cars with a miles-per-gallon (MPG) above a certain threshold, but the threshold is a global variable. Rewrite it so that it becomes a pure function.\n# Impure function\nmpg_threshold &lt;- 20\n\nget_efficient_cars_impure &lt;- function(dataset) {\n  dplyr::filter(dataset, mpg &gt; mpg_threshold)\n}\n\n# Your task: Create a pure function `get_efficient_cars_pure`\n# that takes the dataset and the threshold as arguments.\n# Then, call it with a threshold of 25.\nPython: Do the same for the Python equivalent. Rewrite the impure function into a pure one (you can find the mtcars dataset as a csv here.\nimport pandas as pd\nmtcars = pd.read_csv(\"path/to/mtcars.csv\")\n\n# Impure function\nmpg_threshold &lt;- 20\n\ndef get_efficient_cars_impure(df):\n    return df[df['mpg'] &gt; mpg_threshold]\n\n# Your task: Create a pure function `get_efficient_cars_pure`\n# that takes the DataFrame and the threshold as arguments.\n# Then, call it with a threshold of 25.\n\n\n\n3.7.2 2. Mapping\nGoal: Use mapping to apply a function to multiple elements of a list or data frame.\n\nR: Using the iris dataset, calculate the number of distinct values for each of its four numeric columns (Sepal.Length, Sepal.Width, Petal.Length, Petal.Width).\n\nHint: Use purrr::map_int() and the n_distinct() function from dplyr.\n\nPython: You are given a list of strings. Use a list comprehension to create a new list containing the length of each string.\nwords = [\"functional\", \"programming\", \"is\", \"powerful\"]\n# Your task: create a list `word_lengths` containing [10, 11, 2, 8]\n\n\n3.7.2.1 3. Filtering\nGoal: Use filtering to select elements from a list based on a condition.\n\nR: You have a list of vectors. Use purrr::keep() to select only the vectors whose mean is greater than 5.\nlist_of_vectors &lt;- list(\n  a = c(1, 2, 9),\n  b = c(8, 8, 9),\n  c = c(1, 1, 2)\n)\n# Your task: create a list `high_mean_vectors` that contains only vectors a and b.\nPython: You have a list of dictionaries, where each dictionary represents a product. Use a list comprehension with an if clause to filter for products that are on sale.\nproducts = [\n    {'name': 'Laptop', 'price': 1200, 'on_sale': False},\n    {'name': 'Mouse', 'price': 25, 'on_sale': True},\n    {'name': 'Keyboard', 'price': 75, 'on_sale': True}\n]\n# Your task: create a list `sale_items` containing only the mouse and keyboard dicts.\n\n\n\n3.7.2.2 4. Reducing\nGoal: Use a reduce operation to aggregate a list into a single value.\n\nR: You are given three small data frames. Use purrr::reduce() and a dplyr::full_join() to combine them into a single data frame.\ndf1 &lt;- data.frame(id = c(\"a\", \"b\"), val1 = c(1, 2))\ndf2 &lt;- data.frame(id = c(\"a\", \"c\"), val2 = c(3, 4))\ndf3 &lt;- data.frame(id = c(\"b\", \"c\"), val3 = c(5, 6))\nlist_of_dfs &lt;- list(df1, df2, df3)\n\n# Your task: use reduce to join them all by the 'id' column.\nPython: Given a list of lists (a 2D matrix), use functools.reduce to “flatten” it into a single list.\nfrom functools import reduce\nmatrix = [[1, 2, 3], [4, 5], [6]]\n# Your task: use reduce to produce the list [1, 2, 3, 4, 5, 6]\n# Hint: `operator.add` can concatenate lists.\n\n\n\n3.7.2.3 5. Composition Challenge: A Reusable Analysis Function\nGoal: Write a reusable function that encapsulates a common data analysis task.\n\nR: Write a single function named summarize_by_group that takes three arguments: a data frame (dataset), a categorical column to group by (grouping_var), and a numeric column to summarize (summary_var). The function should return a summarized data frame with the count, mean, and standard deviation for the summary_var within each group. Use {dplyr} and the { } syntax. Test it on the iris dataset by summarizing Sepal.Length grouped by Species.\nPython: Write the equivalent function in Python named summarize_by_group_py. It should take a pandas DataFrame, a grouping_var name (string), and a summary_var name (string). Use .groupby() and .agg() to produce the same summary table (count, mean, sd). Test it on the penguins dataset by summarizing body_mass_g grouped by species.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Functional Programming</span>"
    ]
  },
  {
    "objectID": "05-unit-testing.html",
    "href": "05-unit-testing.html",
    "title": "4  Unit Testing: The Safety Net for Your Code",
    "section": "",
    "text": "4.1 Introduction: Proving Your Code Works\nWhat you’ll learn by the end of this chapter:\nI hope you are starting to see the pieces of our reproducible workflow coming together. We now have:\nThis brings us to the final, crucial question: How do we prove that our functions actually do what we claim they do?\nThe answer is unit testing. A unit test is a piece of code whose sole job is to check that another piece of code, a “unit”, works correctly. In our functional world, the “unit” is almost always a single function. This is why we spent so much time on FP in the previous chapter. Small, pure functions are not just easy to reason about; they are incredibly easy to test.\nWriting tests is your contract with your collaborators and your future self. It’s a formal promise that your function, calculate_mean_mpg(), given a specific input, will always produce a specific, correct output. It’s the safety net that catches bugs before they make it into your final analysis and the tool that gives you the confidence to refactor and improve your code without breaking it.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Unit Testing: The Safety Net for Your Code</span>"
    ]
  },
  {
    "objectID": "05-unit-testing.html#introduction-proving-your-code-works",
    "href": "05-unit-testing.html#introduction-proving-your-code-works",
    "title": "4  Unit Testing: The Safety Net for Your Code",
    "section": "",
    "text": "Reproducible Environments (Nix): The correct tools for everyone.\nReproducible History (Git): The correct version of the code for everyone.\nReproducible Logic (Functional Programming): A philosophy for writing clean, predictable, and self-contained code.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Unit Testing: The Safety Net for Your Code</span>"
    ]
  },
  {
    "objectID": "05-unit-testing.html#the-philosophy-of-a-good-unit-test",
    "href": "05-unit-testing.html#the-philosophy-of-a-good-unit-test",
    "title": "4  Unit Testing: The Safety Net for Your Code",
    "section": "4.2 The Philosophy of a Good Unit Test",
    "text": "4.2 The Philosophy of a Good Unit Test\nSo, what should we test? Writing good tests is a skill, but it revolves around answering a few key questions about your function. For any function you write, you should have tests that cover:\n\nThe “Happy Path”: Does the function return the expected, correct value for a typical, valid input?\nBad Inputs: Does the function fail gracefully or throw an informative error when given garbage input (e.g., a string instead of a number, a data frame with the wrong columns)?\nEdge Cases: How does the function handle tricky but valid inputs? For example, what happens if it receives an empty data frame, a vector with NA values, or a vector where all the numbers are the same?\n\nWriting tests forces you to think through these scenarios, and in doing so, almost always leads you to write more robust and well-designed functions.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Unit Testing: The Safety Net for Your Code</span>"
    ]
  },
  {
    "objectID": "05-unit-testing.html#unit-testing-in-practice",
    "href": "05-unit-testing.html#unit-testing-in-practice",
    "title": "4  Unit Testing: The Safety Net for Your Code",
    "section": "4.3 Unit Testing in Practice",
    "text": "4.3 Unit Testing in Practice\nLet’s imagine we’ve written a simple helper function to normalize a numeric vector (i.e., scale it to have a mean of 0 and a standard deviation of 1). We’ll save this in a file named utils.R or utils.py.\nR version (utils.R):\nnormalize_vector &lt;- function(x) {\n  (x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE)\n}\nPython version (utils.py):\nimport numpy as np\n\ndef normalize_vector(x):\n  return (x - np.nanmean(x)) / np.nanstd(x)\nNow, let’s write tests for it.\n\n4.3.1 Testing in R with {testthat}\nIn R, the standard for unit testing is the {testthat} package. The convention is to create a tests/testthat/ directory in your project, and for a script utils.R, you would create a test file named test-utils.R.\nInside test-utils.R, we use the test_that() function to group related expectations.\n# In file: tests/testthat/test-utils.R\n\n# First, we need to load the function we want to test\nsource(\"../../utils.R\")\n\nlibrary(testthat)\n\ntest_that(\"Normalization works on a simple vector (the happy path)\", {\n  # 1. Setup: Create input and expected output\n  input_vector &lt;- c(10, 20, 30)\n  expected_output &lt;- c(-1, 0, 1)\n\n  # 2. Action: Run the function\n  actual_output &lt;- normalize_vector(input_vector)\n\n  # 3. Expectation: Check if the actual output matches the expected output\n  expect_equal(actual_output, expected_output)\n})\n\ntest_that(\"Normalization handles NA values correctly\", {\n  input_with_na &lt;- c(10, 20, 30, NA)\n  expected_output &lt;- c(-1, 0, 1, NA)\n\n  actual_output &lt;- normalize_vector(input_with_na)\n\n  # We need to use expect_equal because it knows how to compare NAs\n  expect_equal(actual_output, expected_output)\n})\nThe expect_equal() function checks for near-exact equality. {testthat} has many other expect_*() functions, like expect_error() to check that a function fails correctly, or expect_warning() to check for warnings.\n\n\n4.3.2 Testing in Python with pytest\nIn Python, the de facto standard is pytest. It’s incredibly simple and powerful. The convention is to create a tests/ directory, and your test files should be named test_*.py. Inside, you just write functions whose names start with test_ and use Python’s standard assert keyword.\n# In file: tests/test_utils.py\n\nimport numpy as np\nfrom utils import normalize_vector # Import our function\n\ndef test_normalize_vector_happy_path():\n    # 1. Setup\n    input_vector = np.array([10, 20, 30])\n    expected_output = np.array([-1.0, 0.0, 1.0])\n\n    # 2. Action\n    actual_output = normalize_vector(input_vector)\n\n    # 3. Expectation\n    # For floating point numbers, it's better to check for \"close enough\"\n    assert np.allclose(actual_output, expected_output)\n\ndef test_normalize_vector_with_nas():\n    input_with_na = np.array([10, 20, 30, np.nan])\n    expected_output = np.array([-1.0, 0.0, 1.0, np.nan])\n\n    actual_output = normalize_vector(input_with_na)\n\n    # `np.allclose` doesn't handle NaNs, but `np.testing.assert_allclose` does!\n    np.testing.assert_allclose(actual_output, expected_output)\nBecause this isn’t a package though, but a simple project with scripts, you also need to create another file called pytest.ini, which will tell pytest where to find the tests:\n[pytest]\n# Discover tests in the tests/ directory\ntestpaths = tests/\n\n# Include default patterns and your hyphenated pattern\npython_files = test_*.py *_test.py test-*.py\n\n# Adds the root directory to the pythonpath, without this\n# it’ll be impossible to import normalize_vector() from\n# utils.py\npythonpath = .\nTo run your tests, you simply navigate to your project’s root directory in the terminal and run the command pytest. It will automatically discover and run all your tests for you. That being said, you will get an error:\n&gt;       assert np.allclose(actual_output, expected_output)\nE       assert False\nE        +  where False = &lt;function allclose at 0x7f0e81c959f0&gt;(array([-1.22474487,  0.        ,  1.22474487]), array([-1.,  0.,  1.])    def test_normalize_vector_with_nas():\n        input_with_na = np.array([10, 20, 30, np.nan])\n        expected_output = np.array([-1.0, 0.0, 1.0, np.nan])\n\n        actual_output = normalize_vector(input_with_na)\n\n        # `np.allclose` doesn't handle NaNs, but `np.testing.assert_allclose` does!\n&gt;       np.testing.assert_allclose(actual_output, expected_output)\nE       AssertionError:\nE       Not equal to tolerance rtol=1e-07, atol=0\nE\nE       Mismatched elements: 2 / 4 (50%)\nE       Max absolute difference among violations: 0.22474487\nE       Max relative difference among violations: 0.22474487\nE        ACTUAL: array([-1.224745,  0.      ,  1.224745,       nan])\nE        DESIRED: array([-1.,  0.,  1., nan])\n\ntests/test_utils.py:25: AssertionError\n====================================================== short test summary info =======================================================\nFAILED tests/test_utils.py::test_normalize_vector_happy_path - assert False\nFAILED tests/test_utils.py::test_normalize_vector_with_nas - AssertionError:\n========================================================= 2 failed in 0.15s ==========================================================\nYou don’t encounter this issue in R and understanding why reveals how valuable unit tests can be. (Hint: how does the implementation of the sd function, which computes the standard deviation, differ between R and NumPy?)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Unit Testing: The Safety Net for Your Code</span>"
    ]
  },
  {
    "objectID": "05-unit-testing.html#testing-as-a-design-tool",
    "href": "05-unit-testing.html#testing-as-a-design-tool",
    "title": "4  Unit Testing: The Safety Net for Your Code",
    "section": "4.4 Testing as a Design Tool",
    "text": "4.4 Testing as a Design Tool\nTesting can also help you with programming, by thinking about edge cases. For example, what happens if we try to normalize a vector where all the elements are the same?\nLet’s write a test for this edge case first.\npytest version:\n# tests/test_utils.py\ndef test_normalize_vector_with_zero_std():\n    input_vector = np.array([5, 5, 5, 5])\n    actual_output = normalize_vector(input_vector)\n    # The current function will return `[nan, nan, nan, nan]`\n    # Let's assert that we expect a vector of zeros instead.\n    assert np.allclose(actual_output, np.array([0, 0, 0, 0]))\nIf we run pytest now, this test will fail. Our test has just revealed a flaw in our function’s design. This process is a core part of Test-Driven Development (TDD): write a failing test, then write the code to make it pass.\nLet’s improve our function:\nImproved Python version (utils.py):\nimport numpy as np\n\ndef normalize_vector(x):\n  std_dev = np.nanstd(x)\n  if std_dev == 0:\n    # If std is 0, all elements are the mean. Return a vector of zeros.\n    return np.zeros_like(x, dtype=float)\n  return (x - np.nanmean(x)) / std_dev\nNow, if we run pytest again, our new test will pass. We used testing not just to verify our code, but to actively make it more robust and thoughtful.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Unit Testing: The Safety Net for Your Code</span>"
    ]
  },
  {
    "objectID": "05-unit-testing.html#the-modern-data-scientists-role-reviewer-and-ai-collaborator",
    "href": "05-unit-testing.html#the-modern-data-scientists-role-reviewer-and-ai-collaborator",
    "title": "4  Unit Testing: The Safety Net for Your Code",
    "section": "4.5 The Modern Data Scientist’s Role: Reviewer and AI Collaborator",
    "text": "4.5 The Modern Data Scientist’s Role: Reviewer and AI Collaborator\nIn the past, writing tests was often seen as a chore. Today, LLMs make this process very easy.\n\n4.5.1 Using LLMs to Write Tests\nLLMs are fantastic at writing unit tests. They are good at handling boilerplate code and thinking of edge cases. You can provide your function to an LLM and give it a prompt like this:\n\nPrompt: “Here is my Python function normalize_vector. Please write three pytest unit tests for it. Include a test for the happy path with a simple array, a test for an array containing np.nan, and a test for the edge case where all elements in the array are identical.”\n\nThe LLM will likely generate high-quality test code that is very similar to what we wrote above. This is a massive productivity boost. However, this introduces a new, critical role for the data scientist: you are the reviewer.\nAn LLM does not write your tests; it generates a draft. It is your professional responsibility to:\n\nRead and understand every line of the test code.\nVerify that the expected_output is actually correct.\nConfirm that the tests cover the cases you care about.\nCommit that code under your name, taking full ownership of it.\n\n“A COMPUTER CAN NEVER BE HELD ACCOUNTABLE THEREFORE A COMPUTER MUST NEVER MAKE A MANAGEMENT DECISION” – IBM Training Manual, 1979.\nIf I ask you why you did something, and your answer is something to the effect of “I dunno, the LLM generated it”, be glad we’re not in the USA where I could just fire you, because that’s what I’d do.\n\n\n4.5.2 Testing and Code Review\nThis role as a reviewer is central to modern collaborative data science. When a teammate (or your future self) submits a Pull Request on GitHub, the tests are your first line of defense. A PR that changes logic but doesn’t update the tests is a major red flag. A PR that adds a new feature without adding any tests should be rejected until tests are included.\nEven as a junior member of a team, one of the most valuable contributions you can make during a code review is to ask: “This looks great, but what happens if the input is NA? Could we add a test for that case?” This moves the quality of the entire project forward.\nBy embracing testing, you are not just writing better code; you are becoming a better collaborator and a more responsible data scientist.\n\n\n4.5.3 A Note on Packaging and Project Structure\nThroughout this chapter, we’ve focused on testing individual functions within a simple project structure (utils.R and tests/test-utils.R). This is the fundamental skill. It’s important to recognize, however, that this entire process becomes even more streamlined and robust when your code is organized into a formal package.\nPackaging your code provides a standardized structure for your functions, documentation, and tests. It solves many logistical problems automatically: testing frameworks know exactly where to find your source code without needing manual source() or from utils import ... statements, and tools can easily run all tests with a single command. It also makes your code installable, versionable, and distributable, which is the ultimate form of reproducibility.\nIn chapter 7, we will learn some packaging basics for Python and R.\n\n\n4.5.4 Hands-On Exercises\nFor these exercises, create a project directory with a tests/ subdirectory. Place your function code in a script in the root directory (e.g., my_functions.R or my_functions.py) and your test code inside the tests/ directory (e.g., tests/test_my_functions.R or tests/test_my_functions.py).\n\n4.5.4.1 Exercise 1: Testing the “Happy Path”\nThe median of a list of numbers is a common calculation. However, the logic is slightly different depending on whether the list has an odd or even number of elements. Your task is to test both of these “happy paths.”\nHere is the function in R and Python.\nR (my_functions.R):\ncalculate_median &lt;- function(x) {\n  sorted_x &lt;- sort(x)\n  n &lt;- length(sorted_x)\n  mid &lt;- floor(n / 2)\n\n  if (n %% 2 == 1) {\n    # Odd number of elements\n    return(sorted_x[mid + 1])\n  } else {\n    # Even number of elements\n    return(mean(c(sorted_x[mid], sorted_x[mid + 1])))\n  }\n}\nPython (my_functions.py):\nimport numpy as np\n\ndef calculate_median(x):\n  sorted_x = np.sort(np.array(x))\n  n = len(sorted_x)\n  mid = n // 2\n\n  if n % 2 == 1:\n    # Odd number of elements\n    return sorted_x[mid]\n  else:\n    # Even number of elements\n    return (sorted_x[mid - 1] + sorted_x[mid]) / 2.0\nYour Task:\n\nCreate a test file (test-my_functions.R or tests/test_my_functions.py).\nWrite a test that checks if calculate_median gives the correct result for a vector with an odd number of elements (e.g., c(10, 20, 40)).\nWrite a second test that checks if calculate_median gives the correct result for a vector with an even number of elements (e.g., [1, 2, 8,     10]).\n\n\n\n4.5.4.2 Exercise 2: Testing Edge Cases and Expected Errors\nThe geometric mean is another way to calculate an average, but it has strict requirements: it only works with non-negative numbers. This makes it a great candidate for testing edge cases and expected failures.\nR (my_functions.R):\ncalculate_geometric_mean &lt;- function(x) {\n  if (any(x &lt; 0)) {\n    stop(\"Geometric mean is not defined for negative numbers.\")\n  }\n  return(prod(x)^(1 / length(x)))\n}\nPython (my_functions.py):\nimport numpy as np\n\ndef calculate_geometric_mean(x):\n  if np.any(np.array(x) &lt; 0):\n    raise ValueError(\"Geometric mean is not defined for negative numbers.\")\n  return np.prod(x)**(1 / len(x))\nYour Task:\nWrite three tests for this function:\n\nA “happy path” test with a simple vector of positive numbers (e.g., c(1, 2,     4) should result in 2).\nAn edge case test for a vector that includes 0. The expected result should be 0.\nAn error test that confirms the function fails correctly when given a vector with a negative number.\n\nIn R, use testthat::expect_error().\nIn Python, use pytest.raises(). Example: with     pytest.raises(ValueError): your_function_call()\n\n\n\n\n4.5.4.3 Exercise 3: Test-Driven Development (in miniature)\nTesting can help you design better functions. Here is a simple function that is slightly flawed. Your task is to use testing to find the flaw and fix it.\nR (my_functions.R):\n# Initial flawed version\nfind_longest_string &lt;- function(string_vector) {\n  # This will break on an empty vector!\n  string_vector[which.max(nchar(string_vector))]\n}\nPython (my_functions.py):\n# Initial flawed version\ndef find_longest_string(string_list):\n  # This will break on an empty list!\n  return max(string_list, key=len)\nYour Task:\n\nPart A: Write a simple test to prove the function works for a standard case (e.g., c(\"a\", \"b\", \"abc\") should return \"abc\").\nPart B: Write a new test for an empty input (c() or []). Run your tests. This test should fail with an error.\nPart C: Modify the original find_longest_string function in your source file to handle the empty input gracefully (e.g., it could return NULL in R, or None in Python).\nRun your tests again. Now all tests should pass. You have just completed a mini-cycle of Test-Driven Development!\n\n\n\n4.5.4.4 Exercise 4: The AI Collaborator\nOne of the most powerful uses of LLMs is to accelerate the creation of tests. Your job is to act as the senior reviewer for the code an LLM generates.\nHere is a simple data cleaning function in Python.\nPython (my_functions.py):\nimport pandas as pd\n\ndef clean_sales_data(df: pd.DataFrame) -&gt; pd.DataFrame:\n  \"\"\"\n  Cleans a raw sales DataFrame.\n  - Renames 'ts' column to 'timestamp'.\n  - Converts 'timestamp' column to datetime objects.\n  - Ensures 'sale_value' is a numeric type.\n  \"\"\"\n  if 'ts' not in df.columns:\n    raise KeyError(\"Input DataFrame must contain a 'ts' column.\")\n  \n  df = df.rename(columns={'ts': 'timestamp'})\n  df['timestamp'] = pd.to_datetime(df['timestamp'])\n  df['sale_value'] = pd.to_numeric(df['sale_value'])\n  return df\nYour Task:\n\nPrompt your LLM: Copy the function above and give your LLM a prompt like this: &gt; “You are a helpful assistant writing tests for a Python data science project. Here is a function. Please write a pytest test file for it. Include a test for the happy path where everything works correctly. Also, include a test that verifies the function raises a KeyError if the ‘ts’ column is missing.”\nAct as the Reviewer:\n\nCreate a new test file (tests/test_data_cleaning.py) and paste the LLM’s response.\nRead every line of the generated test code. Is the logic correct? Is the expected_output data frame what you would actually expect?\nRun the tests using pytest. Do they pass? If not, debug and fix them. It is your responsibility to ensure the final committed code is correct.\nAdd a comment at the top of the test file describing one thing the LLM did well and one thing you had to change or fix (e.g., # LLM correctly     set up the test for the KeyError, but I had to correct the expected data     type in the happy path test.).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Unit Testing: The Safety Net for Your Code</span>"
    ]
  },
  {
    "objectID": "06-packages.html",
    "href": "06-packages.html",
    "title": "5  A Short Intro to Packaging Your Code in R and Python",
    "section": "",
    "text": "5.1 Introduction: Why Bother Packaging?\nWhat you’ll learn by the end of this chapter:\nSo far, we have built a robust workflow based on three pillars: reproducible environments with Nix, reproducible history with Git, and reproducible logic with functional programming. We’ve organized our code into functions, which are a massive improvement over messy scripts.\nThe final, logical step in this journey is to treat our collection of functions not just as a set of helper scripts, but as a formal package. A package is more than just a folder of code; it’s a self-contained, distributable, and installable unit of software that bundles together code, data, documentation, and tests.\nYou might think, “I’m a data scientist, not a software engineer. Isn’t this overkill?” The answer is a definitive no. Packaging your code, even for an internal analysis project, provides enormous benefits:\nIn this chapter, we will walk through the process of creating a simple package in both R and Python. The goal is not to become an expert package developer, but to understand the structure and benefits so you can apply this powerful “packaging mindset” to all your future projects.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>A Short Intro to Packaging Your Code in R and Python</span>"
    ]
  },
  {
    "objectID": "06-packages.html#introduction-why-bother-packaging",
    "href": "06-packages.html#introduction-why-bother-packaging",
    "title": "5  A Short Intro to Packaging Your Code in R and Python",
    "section": "",
    "text": "Reusability: Instead of copying and pasting your clean_data() function from project to project, you can simply import mypackage or library(mypackage) and use a single, trusted version.\nDistribution & Collaboration: How do you share your work with a colleague? Emailing a zip file of scripts is a recipe for disaster. Sending them a single command—devtools::install_github(\"my_repo\")—is robust and professional.\nDocumentation: Packaging forces you into a standardized way of documenting your functions. This makes your code understandable to others and, more importantly, to yourself six months from now.\nTesting: A package provides a formal framework for running unit tests, ensuring that your functions work as expected and giving you the confidence to make changes without breaking things.\nDependency Management: A package explicitly declares all of its dependencies (e.g., “this package needs dplyr version 1.1.0 or newer”). This solves a huge source of reproducibility errors.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>A Short Intro to Packaging Your Code in R and Python</span>"
    ]
  },
  {
    "objectID": "06-packages.html#part-1-creating-an-r-package-with-usethis-and-devtools",
    "href": "06-packages.html#part-1-creating-an-r-package-with-usethis-and-devtools",
    "title": "5  A Short Intro to Packaging Your Code in R and Python",
    "section": "5.2 Part 1: Creating an R Package with {usethis} and {devtools}",
    "text": "5.2 Part 1: Creating an R Package with {usethis} and {devtools}\nThe R community has developed an outstanding set of tools that make package development incredibly streamlined. The two essential packages are:\n\n{devtools}: Provides core development tools like install(), test(), and check().\n{usethis}: A workflow package that automates all the boilerplate. It creates files, sets up infrastructure, and guides you through the process.\n\nLet’s build a package called cleanR, which will contain a function to standardize column names. Create folder called cleanR and cd into it.\n\n5.2.1 Step 1: Project Setup\nFirst, make sure you have the necessary tools installed. Create a Nix environment that contains the following R packages: {devtools}, {usethis} and {roxygen2}.\nDrop into this Nix shell, and let {usethis} create the package structure for you. From your R console, run:\nusethis::create_package(\"~/Documents/projects/cleanR\")\nThis will create a new cleanR directory with all the necessary files and subdirectories. It will also open a new RStudio session for that project. The key components are:\n\nR/: This is where your R source code files will live.\nDESCRIPTION: A metadata file describing your package, its author, license, and dependencies.\nNAMESPACE: A file that declares which functions your package exports for users and which functions it imports from other packages. You should never edit this file by hand. {roxygen2} will manage it for you.\n\n\n\n5.2.2 Step 2: Write and Document a Function\nLet’s create our function. {usethis} helps with this too:\nusethis::use_r(\"clean_names\")\nThis creates a new file R/clean_names.R and opens it for editing. Let’s add our function, including special comments for documentation. These #' comments are used by the {roxygen2} package to automatically generate the official documentation.\n# In R/clean_names.R\n\n#' Clean and Standardize Column Names\n#'\n#' This function takes a data frame and returns a new data frame with\n#' cleaned-up column names (lowercase, with underscores instead of spaces\n#' or periods).\n#'\n#' @param df A data frame.\n#' @return A data frame with standardized column names.\n#' @export\n#' @examples\n#' messy_df &lt;- data.frame(\"First Name\" = c(\"Ada\", \"Bob\"), \"Last.Name\" = c(\"Lovelace\", \"Ross\"))\n#' clean_names(messy_df)\nclean_names &lt;- function(df) {\n  old_names &lt;- names(df)\n  new_names &lt;- tolower(old_names)\n  new_names &lt;- gsub(\"[ .]\", \"_\", new_names)\n  names(df) &lt;- new_names\n  return(df)\n}\nThe key tags here are:\n\n@param: Describes a function argument.\n@return: Describes what the function returns.\n@export: This is crucial. It tells R that you want this function to be available to users when they load your package with library(cleanR).\n@examples: Provides runnable examples that will appear in the help file.\n\nNow, run the magic command to process these comments:\ndevtools::document()\nThis updates the NAMESPACE file and creates the help file (man/clean_names.Rd). You can now see your function’s help page with ?clean_names.\n\n\n5.2.3 Step 3: Add Unit Tests\nA package without tests is a package waiting to break. {usethis} makes setting up tests trivial.\nusethis::use_testthat() # Sets up the tests/testthat/ directory\nusethis::use_test(\"clean_names\") # Creates tests/testthat/test-clean_names.R\nNow, edit the test file to add your expectations.\n# In tests/testthat/test-clean_names.R\ntest_that(\"clean_names works with spaces and periods\", {\n  messy_df &lt;- data.frame(\"First Name\" = c(\"A\"), \"Last.Name\" = c(\"B\"))\n  cleaned_df &lt;- clean_names(messy_df)\n\n  expected_names &lt;- c(\"first_name\", \"last_name\")\n\n  expect_equal(names(cleaned_df), expected_names)\n})\n\ntest_that(\"clean_names handles already clean names\", {\n  clean_df &lt;- data.frame(a = 1, b = 2)\n  # The function should not change anything\n  expect_equal(names(clean_names(clean_df)), c(\"a\", \"b\"))\n})\nTo run all the tests for your package, use:\ndevtools::test()\n\n\n5.2.4 Step 4: Check and Install\nThe final step before sharing is to run the official R CMD check, the gold standard for package quality. This command runs all tests, checks documentation, and looks for common problems.\ndevtools::check()\nIf your package passes with 0 errors, 0 warnings, and 0 notes, you are in great shape. If your package raises NOTEs or WARNINGs during the check phase, you can most of the time safely ignore these, especially if the package is only intented for internal usage. However, I would recommend that you still take care of the WARNINGs at the very least.\nAs a next step, you could edit the DESCRIPTION file. This is where you will list yourself as the package author, list the dependencies of the package and so on. I won’t got into detail here, but learning how to edit the DESCRIPTION file is important for actual package development (especially listing dependencies is key).\nTo use your package within a project, the simplest way is to host it on GitHub or build a .tar.gz file and install it locally.\n\n\n5.2.5 Step 5: Install from GitHub\nIf you can publicly host your package, hosting it on GitHub is a good way to easily share your code, and install the package in your projects without needed to publish it on CRAN.\n\nCreate a new, empty repository on GitHub (e.g., cleanR).\nIn your local project, follow the instructions GitHub provides to link your local repository and push your code. This usually involves commands like:\ngit remote add origin git@github.com:yourusername/cleanR.git\ngit branch -M main\ngit push -u origin main\nNow, anyone (including you on a different machine) can install your package with a single command (if you don’t use Nix):\n# You might need to install {remotes} first\n# install.packages(\"remotes\")\nremotes::install_github(\"yourusername/cleanR\")\n\nIf you want to create an environment using Nix that includes this package, use the git_pkgs argument of rix::rix() to generate the right default.nix file.\nCongratulations, you have created and shared a fully functional R package!\n\n\n5.2.6 Step 5bis: Install it locally\nIf you can’t share your package on GitHub, the alternative is to build it locally using:\ndevtools::build()\nwhich will create a .tar.gz package. You can then install it using either devtools::install_local() if you don’t use Nix or the local_r_pkgs argument of rix::rix().",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>A Short Intro to Packaging Your Code in R and Python</span>"
    ]
  },
  {
    "objectID": "06-packages.html#part-2-creating-a-minimal-python-package-with-uv",
    "href": "06-packages.html#part-2-creating-a-minimal-python-package-with-uv",
    "title": "5  A Short Intro to Packaging Your Code in R and Python",
    "section": "5.3 Part 2: Creating a Minimal Python Package with uv",
    "text": "5.3 Part 2: Creating a Minimal Python Package with uv\nThe Python packaging ecosystem is rapidly modernizing. While we use Nix to manage our overall environment, we still need to define the metadata and structure for our Python package. We will use uv, an extremely fast and modern tool, for one specific purpose: initializing our project’s configuration file. We will not use uv to manage a virtual environment, as Nix already handles that for us (unless you absolutely want to: however, you should then make sure that uv itself is being managed by Nix to ensure reproducibility).\nLet’s build a Python package called pyclean, the equivalent of our R package.\n\n5.3.1 Step 1: Project Setup with uv\nFirst, ensure uv is installed in your Nix environment:\nlibrary(rix)\n\nrix(\n  date = \"2025-10-07\",\n  py_conf = list(\n    py_version = \"3.13\",\n    py_pkgs = c(\"pytest\", \"pandas\")\n  ),\n  system_pkgs = \"uv\",\n  ide = \"none\",\n  project_path = \".\",\n  overwrite = TRUE\n)\nThen, create a directory for your new package and initialize it:\nmkdir pyclean\ncd pyclean\nuv init --bare\nThe --bare flag is perfect for our Nix workflow. It creates only the essential pyproject.toml file without creating a virtual environment or extra directories. This leaves us with a clean slate.\nNow, we must create the source and test directories manually. We’ll use the standard src layout:\nmkdir -p src/pyclean\nmkdir tests\ntouch src/pyclean/__init__.py\nYour project structure should now look like this (check it using the tree command):\npyclean/\n├── pyproject.toml\n├── src/\n│   └── pyclean/\n│       └── __init__.py\n└── tests/\n\n\n5.3.2 Step 2: Write a Function and Declare Dependencies\nLet’s create our clean_names function inside a new file, src/pyclean/formatters.py.\n# In src/pyclean/formatters.py\nimport pandas as pd\n\ndef clean_names(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Clean and standardize column names of a DataFrame.\n\n    Args:\n        df: The input pandas DataFrame.\n\n    Returns:\n        A pandas DataFrame with standardized column names.\n    \"\"\"\n    new_df = df.copy()\n    new_cols = {col: col.lower().replace(\" \", \"_\").replace(\".\", \"_\") for col in new_df.columns}\n    new_df = new_df.rename(columns=new_cols)\n    return new_df\nTo make this function easily importable, we expose it in src/pyclean/__init__.py:\n# In src/pyclean/__init__.py\nfrom .formatters import clean_names\n\n__all__ = [\"clean_names\"]\nNext, we must declare our dependencies by manually editing pyproject.toml. We need pandas for our function and pytest for our tests.\n# In pyproject.toml\n[project]\nname = \"pyclean\"\nversion = \"0.1.0\"\ndescription = \"A simple package to clean data.\"\ndependencies = [\n    \"pandas&gt;=2.0.0\",\n]\n\n[project.optional-dependencies]\ntest = [\n    \"pytest\",\n]\n\n[tool.pytest.ini_options]\npythonpath = [\n  \"src\"\n]\nThe pythonpath = [\"src\"] line is the key. Without it, you’d first need to install your pyclean library in editable mode using pip before running the tests. By adding this block, simply running pytest from the command line will work.\n\n\n5.3.3 Step 3: Add Unit Tests\nCreate a new test file, tests/test_formatters.py, and add your tests.\n# In tests/test_formatters.py\nimport pandas as pd\nfrom pyclean import clean_names\n\ndef test_clean_names_happy_path():\n    messy_df = pd.DataFrame({\"First Name\": [\"Ada\"], \"Last.Name\": [\"Lovelace\"]})\n    cleaned_df = clean_names(messy_df)\n    expected_cols = [\"first_name\", \"last_name\"]\n    assert list(cleaned_df.columns) == expected_cols\n\ndef test_clean_names_is_idempotent():\n    clean_df = pd.DataFrame({\"first_name\": [\"a\"], \"last_name\": [\"b\"]})\n    still_clean_df = clean_names(clean_df)\n    assert list(still_clean_df.columns) == list(clean_df.columns)\nSince your Nix environment provides all the tools, you can run tests directly from your terminal:\npytest\n\n\n5.3.4 Step 4: Build and Install\nTo package your code, you need a build tool. It turns out that uv bundles a build tool with it, so we only need to call uv build:\n# In your terminal, from the root of the 'pyclean' project\nuv build\nThis creates a dist/ directory containing a source distribution (.tar.gz) and a compiled wheel (.whl). The wheel is the modern standard for distribution.\nOutside of a Nix shell, to use your package during development, you can install it in “editable” mode. This creates a link to your source code, so any changes you make are immediately reflected without needing to reinstall.\n# Install the package and its test dependencies\npip install -e .[test]\nBut we are working from a Nix shell. Instead, we will simply edit our default.nix to update the PYTHONPATH environment variable, so our package can easily be found. If you look at the default.nix file of the course you’ve been using, you’ll see the following at the bottom:\nshellHook = ''\n  export PYTHONPATH=$PWD/src:$PYTHONPATH\n'';\n(you may need to adapt the path depending on where you’re developing the package). With this, dropping into the Nix shell, starting the Python interpreter and then typing import pyclean will work without any issues.\n\n\n5.3.5 Step 5: Install from GitHub\nSharing via GitHub is the most common way to distribute packages that aren’t on the official Python Package Index (PyPI):\n\nCreate a new, empty repository on GitHub.\nPush your local project to the remote repository.\nNow, anyone can install your package directly from GitHub using pip, which is smart enough to find and process your pyproject.toml file: bash     pip install git+https://github.com/yourusername/pyclean.git\n\nFor Nix environments, add this to your default.nix:\npyclean = pkgs.python313Packages.buildPythonPackage rec {\n  pname = \"pyclean\";\n  version = \"0.1.0\";\n  src = pkgs.fetchgit {\n    url = \"https://github.com/b-rodrigues/pyclean\";\n    rev = \"174d4d482d400536bb0d987a3e25ae80cd81ef3c\";\n    sha256 = \"sha256-xTYydkuduPpZsCXE2fv5qZCnYYCRoNFpV7lQBM3LMSg=\";\n  };\n  pyproject = true;\n  propagatedBuildInputs = [ pkgs.python313Packages.pandas pkgs.python313Packages.setuptools ];\n  # Add more dependencies to propagatedBuildInputs as needed\n};\nYou need to add the rev, which corresponds to the commit that want, and the sha256. To find the right sha256, start with an empty one (sha256 = \"\";) and try to build the package. The error message will give you the right sha256. Also not that this isn’t the the most idiomatic way to build a Python package for Nix, but it’s good enough for our purposes.\nFinally, add pyclean to the buildInputs of the shell:\n  buildInputs = [ rpkgs pyconf pyclean tex system_packages github_pkgs ];\nThis process is naturally more involved than simply calling pip install, but it has the advantage of being entirely reproducible.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>A Short Intro to Packaging Your Code in R and Python</span>"
    ]
  },
  {
    "objectID": "06-packages.html#conclusion-the-packaging-mindset-in-the-age-of-ai",
    "href": "06-packages.html#conclusion-the-packaging-mindset-in-the-age-of-ai",
    "title": "5  A Short Intro to Packaging Your Code in R and Python",
    "section": "5.4 Conclusion: The Packaging Mindset in the Age of AI",
    "text": "5.4 Conclusion: The Packaging Mindset in the Age of AI\nYou have now successfully created, tested, documented, and shared a basic package in both R and Python. While there is much more to learn about advanced package development, you have already mastered the most important part: the packaging mindset.\nFrom now on, when you start a new analysis project, think of it as a small, internal package.\n\nPut your reusable logic into functions.\nPlace those functions in the R/ or mypackage/ source directory.\nDocument them.\nWrite a few simple tests to prove they work.\nManage dependencies formally in DESCRIPTION or pyproject.toml.\n\nAdopting this structure will make your work more robust, easier to share, and fundamentally more reproducible. It is the bridge between writing one-off scripts and building reliable, professional data science tools.\nThis packaging mindset becomes even more powerful when you introduce a modern collaborator: the LLM. The structured, component-based nature of a package is the perfect way to interact with AI assistants.\nA package provides a clear contract and a well-defined structure that LLMs thrive on. Instead of a vague prompt like, “Refactor my messy analysis script,” you can now make precise, targeted requests:\n\n“Here is my function clean_names. Please write three pytest unit tests for it, including one for the happy path, one for an empty DataFrame, and one for names that are already clean.”\n“Generate the roxygen2 documentation skeleton for this R function, including @param, @return, and @examples tags.”\n“I need a function in my pyclean/utils.py module that calculates the Z-score for a pandas Series. Please generate the function and its docstring.”\n\nThis synergy is a two-way street. Not only does the structure help you write better prompts, but LLMs excel at generating the very boilerplate that makes packaging robust. Tedious tasks like writing standard documentation headers, creating skeleton unit test files, or even generating a first draft of a function based on a clear description become near-instantaneous.\nThis elevates your role from a writer of code to an architect and a reviewer. Your job is to design the components (the functions), prompt the LLM to generate the implementation, and then—most critically—use the testing framework you just built to rigorously verify that the AI-generated code is correct, efficient, and robust. You are the final authority, and the package structure gives you the tools to enforce quality control.\nBy combining the discipline of packaging with the power of LLMs, you lower the barrier to adopting best practices like comprehensive testing and documentation. This combination doesn’t just make you faster; it makes you a more reliable and professional data scientist, capable of producing tools that are truly reproducible and built to last.\nWhile a full guide to package development is beyond the scope of this course, it is the natural next step in your journey as a data scientist who produces reliable tools. When you are ready to take that step, here are the definitive resources to guide you:\n\nFor R: The “R Packages” (2e) book by Hadley Wickham and Jennifer Bryan is the essential, comprehensive guide. It covers everything from initial setup with {usethis} to testing, documentation, and submission to CRAN. Read it online here.\nFor Python: The official Python Packaging User Guide is the place to start. For a more modern and streamlined approach that handles dependency management and publishing, many developers use tools like Poetry or Hatch.\n\nTreating your data analysis project like a small, internal software package, complete with functions and tests, is a powerful mindset that will elevate the quality and reliability of your work.\nHere are two new exercises to add to the end of the chapter.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>A Short Intro to Packaging Your Code in R and Python</span>"
    ]
  },
  {
    "objectID": "06-packages.html#hands-on-exercises",
    "href": "06-packages.html#hands-on-exercises",
    "title": "5  A Short Intro to Packaging Your Code in R and Python",
    "section": "5.5 Hands-on Exercises",
    "text": "5.5 Hands-on Exercises\n\n5.5.1 Exercise 1: Build Your First Python Package with Nix\nGoal: To build a complete, testable, and documented Python package and make it available in a reproducible Nix environment.\nYou will create a small Python package called statstools. This package will contain a single function that calculates descriptive statistics for a list of numbers.\nRequirements:\n\nProject Structure: Create the following directory structure for your package:\nstatstools/\n├── pyproject.toml\n├── default.nix\n├── src/\n│   └── statstools/\n│       ├── __init__.py\n│       └── calculations.py\n└── tests/\n    └── test_calculations.py\nFunctionality: In src/statstools/calculations.py, create a function descriptive_stats(numbers: list) -&gt; dict. This function should take a list of numbers and return a dictionary containing the mean and standard deviation. Use the numpy library for the calculations.\nDocumentation: Write a clear docstring for your descriptive_stats function explaining what it does, its parameters (Args), and what it returns (Returns).\nDependencies: In your pyproject.toml file:\n\nDefine the project name as statstools and give it a version of 0.1.0.\nAdd numpy as a runtime dependency.\nAdd pytest as an optional dependency for testing.\nAdd the [tool.pytest.ini_options] block to set the pythonpath correctly so pytest can find your src directory.\n\nUnit Tests: In tests/test_calculations.py, write at least two tests for your function using pytest:\n\nA “happy path” test with a simple list like [1, 2, 3, 4, 5].\nA test with negative numbers and floats.\n\nGitHub Repository: Push your completed package to a new public repository on GitHub.\nNix Expression: This is the most critical part. Create a default.nix file in the root of your project. This file should:\n\nFetch your package’s source code from your GitHub repository using pkgs.fetchgit. You will need to get the commit hash (rev) and the sha256 hash (remember, you can get the correct sha256 from the error message when you first try to build with an empty sha256 = \"\";).\nDefine your package using pkgs.python3Packages.buildPythonPackage. Ensure you set pyproject = true; and list its dependencies (numpy, pytest) in propagatedBuildInputs.\nCreate a reproducible shell using pkgs.mkShell that includes both your statstools package and the Python interpreter.\nHint: Look closely at the default.nix file from the main course repository for a complete example of how a Python package is fetched from GitHub and built.\n\n\nTo verify your work: * Run pytest from within the Nix shell to ensure all your tests pass. * Start a Python interpreter inside the Nix shell and successfully run from     statstools import descriptive_stats.\n\n\n5.5.2 Exercise 2: Build and Package an R Tool with Nix\nGoal: To apply R’s best practices for package development (usethis, devtools, roxygen2) and integrate the final product into a reproducible Nix environment.\nYou will create a small R package called datasummary. This package will provide a function to quickly summarize a data frame.\nRequirements:\n\nProject Setup: Use usethis::create_package(\"datasummary\") to generate the standard R package structure.\nFunctionality:\n\nUse usethis::use_r(\"summarize_df\") to create a new file for your function.\nThe function, summarize_df(df), should take a data frame as input.\nIt should return a new data frame with two columns: column_name and missing_values, showing the count of NAs in each column of the input data frame.\nThis function will require the {dplyr} package. Use usethis::use_package(\"dplyr\") to declare this dependency in your DESCRIPTION file.\n\nDocumentation:\n\nUse roxygen2 comments to document your summarize_df function.\nInclude @param, @return, and @export tags.\nProvide a working example in the @examples tag.\nSince your function will use functions from {dplyr}, add the appropriate @importFrom tags (e.g., @importFrom dplyr %&gt;%     summarise_all).\nRun devtools::document() to generate the documentation.\n\nUnit Tests:\n\nSet up testing with usethis::use_testthat().\nCreate a test file with usethis::use_test(\"summarize_df\").\nIn the test file, write at least one test using test_that() that creates a sample data frame with missing values and checks if your function returns the correct counts.\nRun devtools::test() to execute your tests.\n\nQuality Check: Run devtools::check() to ensure your package is free of errors, warnings, and notes.\nGitHub Repository: Push your completed R package to a new public repository on GitHub.\nNix Expression: Create a default.nix file in the root of your datasummary project. This file must:\n\nDefine a custom R environment.\nDefine your datasummary package by using buildRPackage. Fetch the source from your GitHub repository using pkgs.fetchgit (you will need the rev and sha256).\nMake sure to list its R dependencies (like {dplyr} and its own dependencies) in propagatedBuildInputs.\nCreate a final shell with pkgs.mkShell that drops you into an R session where your datasummary package is installed and available.\nHint: Refer to the main course repository’s default.nix file to see how R packages are defined with buildRPackage and included in the final R environment. Using {rix} is also an option.\n\n\nTo verify your work: * Drop into the Nix shell provided by your default.nix. * Start an R session. * Successfully run library(datasummary) and test your summarize_df() function on a data frame like iris.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>A Short Intro to Packaging Your Code in R and Python</span>"
    ]
  },
  {
    "objectID": "07-rixpress.html",
    "href": "07-rixpress.html",
    "title": "6  Building Reproducible Pipelines with Nix and {rixpress}",
    "section": "",
    "text": "6.1 Introduction: From Scripts and Notebooks to Pipelines\nWhat you’ll have learned by the end of the chapter: how to orchestrate a fully reproducible, polyglot analytical pipeline using Nix as a build automation tool, and why this is a fundamentally more robust approach than using computational notebooks or other common workflow tools.\nSo far, we have learned about the 3 main necessary pillars for building reproducible pipelines:\nThe last pillar is orchestration.\nHow do we take our collection of functions and data files and run them in the correct order to produce our final data product? This problem of managing computational workflows is not new, and a whole category of build automation tools has been created to solve it.\nThe original solution to this problem, dating back to the 1970s, is make. Created by Stuart Feldman at Bell Labs in 1976, make was born out of frustration. Feldman, working on his Fortran programs, was tired of the tedious and error-prone process of manually re-compiling only the necessary parts of his code after making a change. He designed make to read a Makefile that describes the dependency graph of a project. You tell it that report.pdf depends on plot.png. If you change the code that generates plot.png, make is smart enough to only re-run the steps needed to rebuild the plot and the final report. General-purpose tools like waf follow a similar philosophy.\nThe strength of these tools is their language-agnosticism, but their weakness is that they only track files and know nothing about the software environment needed to create those files. Another limitaton of these generic tools is that they are file-centric. This means that you are responsible for manually handling all input and output. Your first script must explicitly save its result as data.csv, and your second script must explicitly load data.csv. This adds boilerplate code and creates a new surface for errors.\nThis is where a specialized tool like R’s {targets} package shines. {targets} tracks dependencies between R objects directly, not just files. When you pass a data frame from one step to the next, {targets} automatically handles the serialization for you (serialization is the process of saving an object into a binary to disk) behind the scenes and loads it back when needed. This is a massive ergonomic improvement, allowing you to think in terms of data objects, not file paths.\nThe Python ecosystem, while rich in tools, lacks a single, dominant tool that offers the same lightweight, object-centric feel as {targets} for everyday analysis. Tools like Snakemake are powerful but often follow the make model of file-based I/O. Others like Luigi or Airflow are typically used for large-scale data engineering but can be overkill for a typical analytical project. This gap highlights the need for a solution that combines an ergonomic, object-passing interface with robust reproducibility.\nFurthermore, all these tools, from make to {targets} to Airflow, separate workflow management from environment management. You use one tool to run the pipeline and another (like conda, Docker, or {renv}) to set up the software. But what if we could use a single, declarative system to manage both?\nThis is why we will also be using Nix for build automation. Nix is not just a package manager; it is a full-fledged build system. When Nix builds a pipeline, it controls the entire dependency graph, from your input data files all the way down to the C compiler used to build R itself. It unifies the “what to run and when” problem with the “what tools to use” problem into a single, cohesive framework.\nHowever, writing build instructions directly in the Nix language can be complex. This is where {rixpress} comes in. It provides a user-friendly R interface, heavily inspired by {targets}, that lets us define our pipeline in familiar R code. {rixpress} then translates this into the necessary Nix expressions for us. We get the ergonomic, object-passing feel of {targets} with the unparalleled, bit-for-bit reproducibility of the Nix build system. It is the perfect tool to complete our reproducible workflow.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Building Reproducible Pipelines with Nix and `{rixpress}`</span>"
    ]
  },
  {
    "objectID": "07-rixpress.html#introduction-from-scripts-and-notebooks-to-pipelines",
    "href": "07-rixpress.html#introduction-from-scripts-and-notebooks-to-pipelines",
    "title": "6  Building Reproducible Pipelines with Nix and {rixpress}",
    "section": "",
    "text": "Define Reproducible Environments with Nix and {rix} to ensure everyone uses the exact same versions of R, Python or Julia, and all system-level dependencies.\nManage Reproducible History with Git to track every change to our code and collaborate effectively.\nWrite Reproducible Logic with Functional Programming to create clean, testable, and predictable functions.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Building Reproducible Pipelines with Nix and `{rixpress}`</span>"
    ]
  },
  {
    "objectID": "07-rixpress.html#our-first-polyglot-pipeline",
    "href": "07-rixpress.html#our-first-polyglot-pipeline",
    "title": "6  Building Reproducible Pipelines with Nix and {rixpress}",
    "section": "6.2 Our First Polyglot Pipeline",
    "text": "6.2 Our First Polyglot Pipeline\n\n\n\n\n\n\n\n\n\n\n\nLet’s start with a simple pipeline. Our goal will be to read the mtcars dataset, perform some initial filtering in Python with {polars}, pass the result to R for further manipulation with {dplyr}, and finally compile a Quarto document that presents the results.\nFirst, let’s create a new project directory. Inside, we’ll bootstrap our project. If you’re in a terminal, you can get a temporary shell with the necessary tools by running:\nnix-shell --expr \"$(curl -sl https://raw.githubusercontent.com/ropensci/rix/main/inst/extdata/default.nix)\"\nOnce inside this temporary shell, start R and run:\n\nrixpress::rxp_init()\n\nThis handy function creates two essential plain text files: gen-env.R and gen-pipeline.R.\n\n6.2.1 Step 0: Use Git\nThis might be the right time to start a Git repository. Either start by creating an empty project on GitHub, or start from your command line, locally:\ngit init\n\n\n6.2.2 Step 1: Defining the Environment\nOpen gen-env.R. This is where we use {rix} to define the tools our pipeline needs:\n\n# In gen-env.R\nlibrary(rix)\n\n# Define execution environment for our polyglot pipeline\nrix(\n  date = \"2025-10-14\",\n  r_pkgs = c(\"dplyr\", \"quarto\", \"reticulate\", \"jsonlite\", \"rixpress\"),\n  py_conf = list(\n    py_version = \"3.13\",\n    py_pkgs = c(\"polars\", \"pyarrow\", \"pandas\") # if you plan to build and\n  ),                                           # explore the pipeline using\n  ide = \"none\",                                # Python, also add 'ryxpress', 'rds2py' and 'biocframe'\n  project_path = \".\",\n  overwrite = TRUE\n)\n\nRun this script (source(\"gen-env.R\")) to generate the default.nix file that describes our complete environment. Now, exit the temporary shell, build your project environment with nix-build, and enter it with nix-shell.\n\n\n6.2.3 Step 2: Defining the Pipeline\nNow, open gen-pipeline.R. This plain text file is where we’ll define the actual pipeline. {rixpress} offers several ways to pass data between languages.\nA pipeline is a list of derivations. A derivation is defined using functions such as rxp_r(), rxp_py(), etc. Most of the time, we start by importing data. In this case, we will be importing a .csv file (which you can download here and save it in the data/ folder) using polars:\n# In gen-pipeline.R\nlibrary(rixpress)\n\nlist(\n  rxp_py_file(\n    name = mtcars_pl,\n    path = \"data/mtcars.csv\",\n    read_function = \"lambda x: polars.read_csv(x, separator='|')\"\n  ),\n  ...\n)\nWe use the rxp_py_file() function to define a derivation that reads in the .csv file using the read_csv() function from polars. When importing data using rxp_py_file() or (rxp_r_file()), the read_function argument must be a function of a single argument, the path to the data.\nNext, we want to filter the dataset:\n\n# In gen-pipeline.R\nlibrary(rixpress)\n\nlist(\n  rxp_py_file(\n    name = mtcars_pl,\n    path = 'data/mtcars.csv',\n    read_function = \"lambda x: polars.read_csv(x, separator='|')\"\n  ),\n  rxp_py(\n    name = mtcars_pl_am_pd,\n    expr = \"mtcars_pl.filter(polars.col('am') == 1).to_pandas()\"\n  ),\n  ...\n\nThe next derivation is defined using rxp_py() which runs Pyton code. As you can see, the expr argument is literal Python code, where polars is used to filter data and then convert the result to a pandas data frame.\nTo pass data to R, we have two methods available.\n\n6.2.3.1 Method 1: Using Language-Specific Converters\nThe rxp_r2py() and rxp_py2r() functions are convenient wrappers that use the {reticulate} package behind the scenes to convert objects:\n  rxp_py2r(\n    name = mtcars_am_r,\n    expr = mtcars_pl_am\n  ),\n  ...\nThis converts the mtcars_pl_am data frame (which is a pandas data frame) into an R data frame using the R package {reticulate}.\nWe can then continue with an R derivation:\n  rxp_r(\n    name = mtcars_head,\n    expr = head(mtcars_am_r)\n  ),\n  ...\nThis works well, but it tightly couples your pipeline to {reticulate}’s conversion capabilities, which in some cases could be overkill.\n\n\n6.2.3.2 Method 2: A lighter Approach with Universal Data Formats\nA lighter and language-agnostic approach is to use a universal data format like JSON. This makes your pipeline more modular, as any language that can read and write JSON could be added in the future. {rixpress} supports this via the serialize_function and unserialize_function arguments.\nLet’s rewrite our pipeline to use JSON. First, we need simple helper functions in our project.\nCreate a script called functions.py that will contain all the Python helper functions we might need. In it, add:\n# A function to save a polars DataFrame to a JSON file\ndef serialize_to_json(pl_df, path):\n    with open(path, 'w') as f:\n        f.write(pl_df.write_json())\nDo the same for R functions, in functions.R:\n# Just aliasing head for demonstration\nmy_head &lt;- head\nNow, we can update gen-pipeline.R to use these helpers:\n\nlibrary(rixpress)\n\nlist(\n  ....\n\n  rxp_py(\n    name = mtcars_pl_am_json,\n    expr = \"mtcars_pl_am_pd.filter(polars.col('am') == 1)\",\n    user_functions = \"functions.py\",\n    encoder = \"serialize_to_json\" # Use our Python helper\n  ),\n\n  rxp_r(\n    name = mtcars_head,\n    expr = my_head(mtcars_pl_am_json),\n    user_functions = \"functions.R\",\n    decoder = \"jsonlite::fromJSON\" # Use R's jsonlite\n  ),\n  ...\n)\n\nThis approach works well in simple cases like passing data frames between languages, but may not work for more complex objects for which {reticulate} may have specialized code for conversion.\n\n\n\n6.2.4 Step 3: Building and Inspecting the Pipeline\nThe complete pipeline will look like this:\n\nlibrary(rixpress)\n\nlist(\n  rxp_py_file(\n    name = mtcars_pl,\n    path = 'data/mtcars.csv',\n    read_function = \"lambda x: polars.read_csv(x, separator='|')\"\n  ),\n  rxp_py(\n    name = mtcars_pl_am_json,\n    expr = \"mtcars_pl.filter(polars.col('am') == 1)\",\n    user_functions = \"functions.py\",\n    encoder = \"serialize_to_json\" # Use our Python helper\n  ),\n\n  rxp_r(\n    name = mtcars_head,\n    expr = my_head(mtcars_pl_am_json),\n    user_functions = \"functions.R\",\n    decoder = \"jsonlite::fromJSON\" # Use R's jsonlite\n  )\n) |&gt;\n  rxp_populate()\n\nThe very last function, rxp_populate() takes a list of derivations as input and will translate the list of derivations into a pipeline.nix file and instruct Nix to build the entire pipeline.\nYou know have the choice to either build the pipeline and inspect artifacts using R (and {rixpress}) or Python, using ryxpress, the Python port of {rixpress}. If you use R, run source(\"gen-pipeline.R\") to generate the pipeline.nix file. This will not build the pipeline (unless you set the build argument to TRUE), so to build the pipeline use rxp_make(). If you use Python, and thus ryxpress, simply call:\n# Python\nfrom ryxpress import rxp_make, rxp_inspect, rxp_load\n\nrxp_make()\nOnce the build process is done, you can use rxp_inspect() to check which artifacts where built, and you can easily access the any of them:\n\n# R\n# Check out all artifacts\nrxp_inspect()\n\n# Load the mtcars_head data frame into your R session\nrxp_load(\"mtcars_head\")\n\n# You can now inspect it\nhead(mtcars_head)\n\nor from Python:\n# python\na = rxp_inspect()\n\nprint(a)\n\nrxp_load(\"mtcars_head\")",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Building Reproducible Pipelines with Nix and `{rixpress}`</span>"
    ]
  },
  {
    "objectID": "07-rixpress.html#caching",
    "href": "07-rixpress.html#caching",
    "title": "6  Building Reproducible Pipelines with Nix and {rixpress}",
    "section": "6.3 Caching",
    "text": "6.3 Caching\nFirst, visualize your pipeline’s dependency graph:\n\n# You'll need to firts generate the required files by running\n# The execution environment will need to include `ggplot2` and `ggdag`\n# Then you can visualize the graph\nrxp_ggdag()\n\nThis will show you a clear, unambiguous graph of your workflow. You can also use rxp_trace() for a text based view:\nNow, modify a step. Open gen-pipeline.R and change the my_head function in functions.R to use tail() for example. Save the file and re-run rixpress(). Nix will detect that the data loading and Python filtering steps are unchanged and instantly use the cached results from the /nix/store/. It will only re-build the final R step that was affected by the change.\nThis is the incredible power of a proper build automation tool. The cognitive load of tracking what to re-run is gone. You are free to experiment, confident that the tool will efficiently and correctly rebuild only what is necessary.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Building Reproducible Pipelines with Nix and `{rixpress}`</span>"
    ]
  },
  {
    "objectID": "07-rixpress.html#debugging-and-working-with-build-logs",
    "href": "07-rixpress.html#debugging-and-working-with-build-logs",
    "title": "6  Building Reproducible Pipelines with Nix and {rixpress}",
    "section": "6.4 Debugging and Working with Build Logs",
    "text": "6.4 Debugging and Working with Build Logs\nBut what happens to the old results? What if you want to compare the head() version of your data to the tail() version? This is where {rixpress}’s build logging becomes a superpower.\nEvery time you run rixpress(), a timestamped log of that specific build is saved in the _rixpress/ directory. This is like having a Git history for your pipeline’s outputs.\nYou can list all the past builds you’ve run:\n\nrxp_list_logs()\n#&gt;                                                          filename   modification_time\n#&gt; 1 build_log_20250602_143015_a1b2c3d4e5f6g7h8i9j0k1l2m3n4o5p6.rds 2025-06-02 14:30:15\n#&gt; 2 build_log_20250602_142500_z9y8x7w6v5u4t3s2r1q0p9o8n7m6l5k4.rds 2025-06-02 14:25:00\n\nLet’s say the first log (...a1b2c3d...) is our new tail() run, and the second (...z9y8x7w...) is our original head() run. You can now pull the artifact from the old run directly into your current session for comparison:\n\n# Load the result from the MOST RECENT build\nnew_result &lt;- rxp_read(\"mtcars_head\")\n\n# Load the result from the PREVIOUS build by matching part of its log name\nold_result &lt;- rxp_read(\"mtcars_head\", which_log = \"z9y8x\")\n\n# Now you can compare them!\nnew_result\nold_result\n\nThis is an incredibly powerful debugging and validation tool. You can go back in time to inspect the state of any output from any previous pipeline run, as long as it’s still in the Nix store. This provides a safety net and traceability that is simply absent in a notebook-based workflow.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Building Reproducible Pipelines with Nix and `{rixpress}`</span>"
    ]
  },
  {
    "objectID": "07-rixpress.html#running-someone-elses-pipeline-the-ultimate-test-of-reproducibility",
    "href": "07-rixpress.html#running-someone-elses-pipeline-the-ultimate-test-of-reproducibility",
    "title": "6  Building Reproducible Pipelines with Nix and {rixpress}",
    "section": "6.5 Running Someone Else’s Pipeline: The Ultimate Test of Reproducibility",
    "text": "6.5 Running Someone Else’s Pipeline: The Ultimate Test of Reproducibility\nImagine a collaborator wants to run your pipeline. If you had sent them a Jupyter notebook, they would face a series of questions: “Which version of Python did you use? What packages do I need? In what order do I run the cells? What is this variable that’s used but never defined?”\nWith our Nix-based workflow, the process is radically simpler and more robust. All they need to do is:\n\ngit clone your repository (which, unlike a notebook, has a clean, readable history).\nRun nix-build, then nix-shell in the project directory.\nStart an R session, and build the pipeline by running the gen-pipeline.R script, or by running rxp_make().\n\nThat’s it. Nix reads your default.nix and pipeline.nix files and builds the exact same environment and the exact same data product, bit-for-bit. It solves all the problems we identified with other approaches: it controls the language versions, the operating system libraries, and all dependencies in one unified, declarative system.\nYou now have the knowledge to build robust, efficient, polyglot, and truly reproducible analytical pipelines. By abandoning the chaos of notebooks for production work and embracing the structured, automatable world of plain text files and build automation, your work becomes more reliable, more scalable, and fundamentally more scientific.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Building Reproducible Pipelines with Nix and `{rixpress}`</span>"
    ]
  },
  {
    "objectID": "07-rixpress.html#hands-on-exercises",
    "href": "07-rixpress.html#hands-on-exercises",
    "title": "6  Building Reproducible Pipelines with Nix and {rixpress}",
    "section": "6.6 Hands-On Exercises",
    "text": "6.6 Hands-On Exercises\nCheck out the rixpress_demos GitHub repository and, by taking inspiration from them, write a pipeline that uses the packages you wrote for the exercises of Chapter 6 Packages.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Building Reproducible Pipelines with Nix and `{rixpress}`</span>"
    ]
  },
  {
    "objectID": "08-docker.html",
    "href": "08-docker.html",
    "title": "7  Containerization With Docker",
    "section": "",
    "text": "7.1 Introduction\nWhat you’ll have learned by the end of the chapter: build self-contained, truly reproducible analytical pipelines thanks to Docker.\nUp until now, we’ve been using Nix as a powerful tool for creating reproducible development environments directly on our machines. Nix gives us fine-grained control over every package and dependency in our project, ensuring bit-for-bit reproducibility. However, when it comes to distributing a data product, another technology, Docker, is incredibly popular.\nWhile Nix manages dependencies for an application that runs on a host operating system, Docker takes a different approach: it packages an application along with a lightweight operating system and all its dependencies into a single, portable unit called a container. This container can then run on any machine that has Docker installed, regardless of its underlying OS.\nThe idea is to not only deliver the source code for our data products, but also include it inside a complete package that contains not only R and the required libraries, but also the necessary components of the operating system itself (which will usually be a flavor of Linux, like Ubuntu). This approach solves the “it works on my machine” problem in a very direct way.\nFor rebuilding a data product, a single command can be used which will pull the Docker image from a registry, start a container, build the data product, and stop.\nIf you’ve never heard of Docker before, this chapter will provide the basic knowledge required to get started. Let’s start by watching this very short video that introduces the core concepts.\nIn a sense, Docker can be seen as a lightweight virtual machine running a Linux distribution (usually Ubuntu) that you can interact with using the command line. This also means that familiarity with Linux distributions will make using Docker easier. Thankfully, there is a very large community of Docker users who also use R. This community is organized as the Rocker Project and provides a very large collection of Dockerfiles to get started easily. As you saw in the video above, Dockerfiles are simple text files that define a Docker image, from which you can start a container.\nWhile Nix and Docker are often seen as competing tools for environment management, they can be used together effectively by leveraging their respective strengths. A powerful pattern is to use Nix inside a Docker container. In this setup, you start with a minimal base Docker image that has Nix installed. Then, you use Nix to declaratively build the precise, bit-for-bit reproducible development environment within the image. Docker’s role then shifts from environment provisioning to simply being a portable, universal runtime for this Nix-managed environment, making it excellent for deployment.\nThis approach contrasts with using Docker alone for reproducibility. While many attempt this, it’s not Docker’s core strength. Achieving a reproducible docker build often requires “abusing” Docker’s features—pinning base image hashes, freezing system package versions, and using specific package manager snapshots—because Docker was designed for creating portable runtime containers, not for guaranteeing reproducible builds. Its true reproducibility promise is that a specific, pre-built image will always launch an identical container, not that building the same Dockerfile twice will yield an identical image.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Containerization With Docker</span>"
    ]
  },
  {
    "objectID": "08-docker.html#docker-essentials",
    "href": "08-docker.html#docker-essentials",
    "title": "7  Containerization With Docker",
    "section": "7.2 Docker essentials",
    "text": "7.2 Docker essentials\n\n7.2.1 Installing Docker\nThe first step is to install Docker. You’ll find the instructions for Ubuntu here, for Windows here (read the system requirements section as well!) and for macOS here (make sure to choose the right version for the architecture of your Mac, if you have an M1 Mac use Mac with Apple silicon).\nAfter installation, it might be a good idea to restart your computer, if the installation wizard does not invite you to do so. To check whether Docker was installed successfully, run the following command in a terminal (or on the desktop app on Windows):\ndocker run --rm hello-world\nThis should print the following message:\nHello from Docker!\nThis message shows that your installation appears to be working correctly.\n\nTo generate this message, Docker took the following steps:\n 1. The Docker client contacted the Docker daemon.\n 2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub.\n    (amd64)\n 3. The Docker daemon created a new container from that image which runs the\n    executable that produces the output you are currently reading.\n 4. The Docker daemon streamed that output to the Docker client, which sent it\n    to your terminal.\n\nTo try something more ambitious, you can run an Ubuntu container with:\n $ docker run -it ubuntu bash\n\nShare images, automate workflows, and more with a free Docker ID:\n https://hub.docker.com/\n\nFor more examples and ideas, visit:\n https://docs.docker.com/get-started/\nIf you see this message, congratulations, you are ready to run Docker. If you see an error message about permissions, this means that something went wrong. If you’re running Linux, make sure that your user is in the Docker group by running:\ngroups $USER\nyou should see your username and a list of groups that your user belongs to. If a group called docker is not listed, then you should add yourself to the group by following these steps.\n\n\n7.2.2 The Rocker Project and image registries\nWhen running a command like:\ndocker run --rm hello-world\nwhat happens is that an image, in this case hello-world gets pulled from a so-called registry. A registry is a storage and distribution system for Docker images. Think of it as a GitHub for Docker images, where you can push and pull images, much like you would with code repositories. The default public registry that Docker uses is called Docker Hub, but companies can also host their own private registries to store proprietary images. When you execute a command like docker run, the Docker daemon first checks if the image is present on your local machine. If not, it connects to the configured registry, downloads the required image layers, and then assembles them to run the container.\nMany open source projects build and distribute Docker images through Docker Hub, for example the Rocker Project.\nThe Rocker Project is instrumental for R users that want to use Docker. The project provides a large list of images that are ready to run with a single command. As an illustration, open a terminal and paste the following line:\ndocker run --rm -e PASSWORD=yourpassword -p 8787:8787 rocker/rstudio\nOnce this stops running, go to http://localhost:8787/ and enter rstudio as the username and yourpassword as the password. You should login to a RStudio instance: this is the web interface of RStudio that allows you to work with R from a server. In this case, the server is the Docker container running the image. Yes, you’ve just pulled a Docker image containing Ubuntu with a fully working installation of RStudio web!\n(If you cannot connect to http://localhost:8787, try with the following command:\ndocker run --rm -ti -d -e PASSWORD=yourpassword -p 8787:8787 --network=\"host\" rocker/rstudio\n)\nLet’s open a new script and run the following lines:\n\ndata(mtcars)\n\nsummary(mtcars)\n\nYou can now stop the container (by pressing CTRL-C in the terminal). Let’s now rerun the container… (with the same command as before) you should realize that your script is gone! This is the first lesson: whatever you do inside a container will disappear once the container is stopped. This also means that if you install the R packages that you need while the container is running, you will need to reinstall them every time. Thankfully, the Rocker Project provides a list of images with many packages already available. For example to run R with the {tidyverse} collection of packages already pre-installed, run the following command:\ndocker run --rm -ti -e PASSWORD=yourpassword -p 8787:8787 rocker/tidyverse\nIf you compare it to the previous command, you see that we have replaced rstudio with tidyverse. This is because rocker/tidyverse references an image, hosted on Docker Hub, that provides the latest version of R, RStudio server and the packages from the {tidyverse}. You can find the image hosted on Docker Hub here. There are many different images, and we will be using the versioned images made specifically for reproducibility. For now, however, let’s stick with the tidyverse image, and let’s learn a bit more about some specifics.\n\n\n7.2.3 Basic Docker workflow\nYou already know about running containers using docker run. With the commands we ran before, your terminal will need to stay open, or else, the container will stop. Starting now, we will run Docker commands in the background. For this, we will use the -d flag (d as in detach), so let’s stop the container one last time with CTRL-C and rerun it using:\ndocker run --rm -d -e PASSWORD=yourpassword -p 8787:8787 rocker/tidyverse\n(notice -d just after run). You can run several containers in the background simultaneously. You can list running containers with docker ps:\ndocker ps\nCONTAINER ID   IMAGE              COMMAND   CREATED         STATUS         PORTS                                       NAMES\nc956fbeeebcb   rocker/tidyverse   \"/init\"   3 minutes ago   Up 3 minutes   0.0.0.0:8787-&gt;8787/tcp, :::8787-&gt;8787/tcp   elastic_morse\nThe running container has the ID c956fbeeebcb. Also, the very last column, shows the name of the running container. This is a label that you can change. For now, take note of ID, because we are going to stop the container:\ndocker stop c956fbeeebcb\nAfter Docker is done stopping the running container, you can check the running containers using docker ps again, but this time no containers should get listed. Let’s also discuss the other flags --rm, -e and -p. --rm removes the container once it’s stopped. Without this flag, we can restart the container and all the data and preferences we saved will be restored. However, this is dangerous because if the container gets removed, then everything will get lost, forever. We are going to learn how to deal with that later. -e allows you to provide environment variables to the container, so in this case the $PASSWORD variable. -p is for setting the port at which your app is going to get served. Let’s now rerun the container, but by giving it a name:\ndocker run -d --name my_r --rm -e PASSWORD=yourpassword -p 8787:8787 rocker/tidyverse\nNotice the --name flag followed by the name we want to use, my_r. We can now interact with this container using its name instead of its ID. For example, let’s open an interactive bash session. Run the following command:\ndocker exec -ti my_r bash\nYou are now inside a terminal session, inside the running container! This can be useful for debugging purposes. It’s also possible to start R in the terminal, simply replace bash by R in the command above.\nFinally, let’s solve the issue of our scripts disappearing. For this, create a folder somewhere on your computer (host). Then, rerun the container, but this time with this command:\ndocker run -d --name my_r --rm -e PASSWORD=yourpassword -p 8787:8787 -v /path/to/your/local/folder:/home/rstudio/scripts:rw rocker/tidyverse\nwhere /path/to/your/local/folder should be replaced to the folder you created. You can also use $PWD instead of /path/to/your/local/folder if you run the docker run ... call in the right folder. You should now be able to save the scripts inside the scripts/ folder from RStudio and they will appear in the folder you created.\n\n\n7.2.4 Making our own images\nTo create our own images, you can start from an image provided by an open source project like Rocker, or you can start from the base Ubuntu or Apline Linux images. These images are barebones compared to the ones from Rocker, but as a consequence they are very lightweight, which in some cases can be important. For the remainder of the course, we are going to start from a base Ubuntu image, and use Nix to add our software stack.\nThe snippet below is a minimal Dockerfile that shows exactly this:\nFROM ubuntu:latest\n\nRUN apt update -y\n\nRUN apt install curl -y\n\n# We don't have R nor {rix} in this image, so we can bootstrap it by downloading\n# the default.nix file that comes with {rix}. You can also download it beforehand\n# and then copy it to the Docker image\nRUN curl -O https://raw.githubusercontent.com/ropensci/rix/main/inst/extdata/default.nix\n\n# The next 4 lines install Nix inside Docker. See the Determinate Systems installer's documentation\nRUN curl --proto '=https' --tlsv1.2 -sSf -L https://install.determinate.systems/nix | sh -s -- install linux \\\n  --extra-conf \"sandbox = false\" \\\n  --init none \\\n  --no-confirm\n\n# Adds Nix to the path, as described by the Determinate Systems installer's documentation\nENV PATH=\"${PATH}:/nix/var/nix/profiles/default/bin\"\nENV user=root\n\n# Set up rstats-on-nix cache\n# Thanks to the rstats-on-nix cache, precompiled binary packages will\n# be downloaded instead of being compiled from source\nRUN mkdir -p /root/.config/nix && \\\n    echo \"substituters = https://cache.nixos.org https://rstats-on-nix.cachix.org\" &gt; /root/.config/nix/nix.conf && \\\n    echo \"trusted-public-keys = cache.nixos.org-1:6NCHdD59X431o0gWypbMrAURkbJ16ZPMQFGspcDShjY= rstats-on-nix.cachix.org-1:vdiiVgocg6WeJrODIqdprZRUrhi1JzhBnXv7aWI6+F0=\" &gt;&gt; /root/.config/nix/nix.conf\n\n# Copy a script to generate the environment of interest using {rix}\nCOPY gen-env.R .\n\n# This will overwrite the default.nix we downloaded previously with a new\n# expression generated from running `gen-env.R`\nRUN nix-shell --run \"Rscript gen-env.R\"\n\n# We now build the environment\nRUN nix-build\n\n# Finally, we run `nix-shell`. This will get executed when running\n# containers from this image. You can of course put anything in here\nCMD nix-shell\nThis can seem quite complicated, but if you take the time to read the comments, you’ll see that it’s actually quite simple.\nEvery Dockerfile starts with a FROM statement. This means that this Dockerfile will use the ubuntu:latest image as a starting point.\nWe start off from the ubuntu:latest image: you might read online that this is not a good practice, and that instead one should use a stable image, for example ubuntu:24.04 which will always use version 24.04 of Ubuntu. This is true IF you don’t use Nix. But since we are using Nix to set up the reproducible development environment, we can use ubuntu:latest: our development environment will always be exactly the same, thanks to Nix.\nThen, every command we wish to run starts with a RUN statement. We install and configure Nix, copy an R script to generate the environment (we could also copy an already generated default.nix instead) and then build the environment. Finally, we finish by running nix-shell when executing a container which is the command prepended with CMD.\nThis image actually does two things:\n\na first step which consists in setting up Nix inside Docker;\na second step which consists in setting up our project-specific Nix development environment.\n\nBecause the first step is generic, we will split up this in two stages.\nFirst, create a new Dockerfile in a separate directory, with a new Git repo so that you can commit and push it (later in the book we will set up continuous integration to build and publish this image automatically):\n# Stage 1 — Base with Nix and rstats-on-nix cache\nFROM ubuntu:latest AS nix-base\n\nRUN apt update -y && apt install -y curl\n\n# Install Nix via Determinate Systems installer\nRUN curl --proto '=https' --tlsv1.2 -sSf -L https://install.determinate.systems/nix | sh -s -- install linux \\\n  --extra-conf \"sandbox = false\" \\\n  --init none \\\n  --no-confirm\n\nENV PATH=\"/nix/var/nix/profiles/default/bin:${PATH}\"\nENV user=root\n\n# Configure Nix binary cache\nRUN mkdir -p /root/.config/nix && \\\n    echo \"substituters = https://cache.nixos.org https://rstats-on-nix.cachix.org\" &gt; /root/.config/nix/nix.conf && \\\n    echo \"trusted-public-keys = cache.nixos.org-1:6NCHdD59X431o0gWypbMrAURkbJ16ZPMQFGspcDShjY= rstats-on-nix.cachix.org-1:vdiiVgocg6WeJrODIqdprZRUrhi1JzhBnXv7aWI6+F0=\" &gt;&gt; /root/.config/nix/nix.conf\nCommit and push. Then, we need to build this image once, and tag it:\ndocker build -t nix-base:latest .\nThis image is now available on our machines under the tag nix-base:latest, and we can refer to it for any of our projects. For a new project, simply reuse it like so:\nFROM nix-base:latest\n\nCOPY gen-env.R .\n\nRUN curl -O https://raw.githubusercontent.com/ropensci/rix/main/inst/extdata/default.nix\nRUN nix-shell --run \"Rscript gen-env.R\"\nRUN nix-build\n\nCMD [\"nix-shell\"]\nThe issue with this approach is that now you have created a dependency between the two Dockerfiles which you need to manage. I would recommend the second approach only if you can push the first image with the Nix base on a registry (either public or a private one from your company). Later in this chapter we will publish the first image.\nIn the same folder than the second Dockerfile, add the required gen-env.R script:\nlibrary(rix)\n\nrix(\n  date = \"2025-08-04\",\n  r_pkgs = c(\"dplyr\", \"ggplot2\"),\n  py_conf = list(\n    py_version = \"3.13\",\n    py_pkgs = c(\"polars\", \"great-tables\")\n  ),\n  ide = \"none\",\n  project_path = \".\",\n  overwrite = TRUE\n)\nThis will setup an environment for our project. Let’s stop here, and build the image:\ndocker build -t my-project .\nand now run a container:\ndocker run -it --rm --name my-project-container my-project\nThis should drop you in an interactive Nix shell running inside Docker! As Docker is more popular than Nix, in particular in enterprise settings, this makes sharing development environments easier.\nRemember, anything you do in this container will be lost after you stop it. So if you want to use it to work interactively on files, you should mount a volume:\ndocker run --rm --name my-project-container -v /path/to/your/local/project-folder/workspace:/workspace:rw -w /workspace my-project\nThis will mount a folder called workspace inside a running Docker container that will map to a folder called workspace on your current project folder. The -w /workspace flag sets the working directory inside the container to /workspace, so any commands you run will execute from there. This acts as a kind of tunnel between the two; any file put there will be available and editable on the other side.\nWhile this is good to know, I don’t recommend using Docker to work interactively. Use Nix for this instead, and use Docker to then deploy whatever product you’ve been working on once you’re done.\nBefore moving on to actually build projects using Docker, let’s first publish the base Nix image on Docker Hub to easily re-use it across projects.\n\n\n7.2.5 Publishing images on Docker Hub\nIf you want to share Docker images through Docker Hub, you first need to create a free account. A free account gives you unlimited public repositories. If you want to make your images private, you need a paid account. For our purposes though, a free account is more than enough. In the next section, we will discuss how you can build new images upon other images without using Docker Hub.\nWe will be uploading the image nix-base to Docker Hub.\nNow is the right moment to talk about the docker images command. This will list all the images available on your computer. You should see something like this:\nREPOSITORY         TAG       IMAGE ID       CREATED       SIZE\nnix-base           latest    d3764d067534   2 days ago    1.61GB\ndev_env_r          latest    92fcf973ba42   2 days ago    1.42GB\nraps_ubuntu_r      latest    7dabadf3c7ee   4 days ago    1.04GB\nrocker/tidyverse   4.2.2     545e4538a28a   3 weeks ago   2.19GB\nrocker/r-ver       4.2.2     08942f81ec9c   3 weeks ago   824MB\nTake note of the image id of the nix-base image (second line), we will use it to push our image to Docker Hub. Also, don’t be alarmed by the size of the images, because this is a bit misleading. Different images that use the same base (so here Ubuntu), will reuse “layers” such that they don’t actually take up the size that is printed by docker images. So if images A and B both use the same version of Ubuntu as a base, but image A has RStudio installed and B also RStudio but Python as well, most of the space that A and B take up will be shared. The only difference will be that B will need a little bit more space for Python.\nYou can also list the running containers with docker container ls (or docker ps). If a container is running you should see something like this:\nCONTAINER ID   IMAGE              COMMAND   CREATED\n545e4538a28a   rocker/tidyverse   \"/init\"   3 minutes ago\n\nSTATUS         PORTS                                       NAMES\nUp 3 minutes   0.0.0.0:8787-&gt;8787/tcp, :::8787-&gt;8787/tcp   elastic_morse\nYou can stop the container by running docker stop CONTAINER ID. So, list the images again using docker images. Take note of the image id of the image you want to push to Docker Hub.\nNow, log in to Docker Hub using docker login (yes, from your terminal). You will be asked for your credentials, and if log in is successful, you see a message Log In Succeeded in your terminal (of course, you need first to have an account on Docker Hub).\nNow, you need to tag the image (this gives it a version number). So you would write something like:\ndocker tag IMAGE_ID your_username_on_docker_hub/your_image:version1\nso in my case, it would be:\ndocker tag 92fcf973ba42 brodriguesco/nix-base:latest\nNext, I need to push it using docker push:\ndocker push brodriguesco/nix-base:latest\nYou can go check your profile and your repositories, you should see your image there.\nThis image can now be used as a stable base for developing our pipelines. Here’s how I can now use this base image for our project:\nFROM brodriguesco/nix-base:latest\n\nRUN mkdir ...\nNow I’m re-using the image that defines the development environment, and I can do so for as many projects as necessary. I would recommend putting a link to the base image as a comment just before the first FROM.\nIf you want to test this, you could delete all images and containers from your system. This way, when you build the image using the above Dockerfile, it will have to pull from Docker Hub. To delete all containers, start by using docker system prune. You can then delete all images using docker rmi $(docker images -a -q). This should remove everything.\nIf you work for a company that has its own private registry, the process will be essentially the same, as it’s just that Docker would have been configured to pull and push to the private registry instead.\nIn the next section, I’ll explain to you how you can re-use base images like we just did, but without using Docker Hub, in case you cannot, or do not want, to rely on it.\n\n\n7.2.6 Sharing a compressed archive of your image\nIf you can’t upload the image on Docker Hub, you can still “save it” into a file and share that file instead (internally to your institution/company).\nRun docker save to save the image into a file:\ndocker save nix-base &gt; nix-base.tar\n\nThis will create a `.tar` file of the image. You can then compress this file\nwith an archiving tool if you want. If you're on Linux, you could do so in one\ngo (this will take some time):\n\n```bash\ndocker save nix-base | gzip &gt; nix-base.tgz\nIf you want to load this image, use docker load:\n# Uncompress it first\ngzip -d nix-base.tgz\n\n# Load it\ndocker load &lt; nix-base.tar\nyou should see an output like this:\n202fe64c3ce3: Loading layer [======================&gt;]  80.33MB/80.33MB\ne7484d5519b7: Loading layer [======================&gt;]  6.144kB/6.144kB\na0f5608ee4a8: Loading layer [======================&gt;]  645.4MB/645.4MB\n475d1d69813f: Loading layer [======================&gt;]  102.9kB/102.9kB\nd7963749937d: Loading layer [======================&gt;]  108.9MB/108.9MB\n224a0042a76f: Loading layer [======================&gt;]    600MB/600MB\na75e978c1654: Loading layer [======================&gt;]  605.7kB/605.7kB\n7efc10233531: Loading layer [======================&gt;]  1.474MB/1.474MB\nLoaded image: nix-base:latest\nor you can also use:\ndocker load -i nix-file.tar\nto load the archive.\nSince the image is available locally, it’ll get used instead of pulling it from Docker Hub. So in case you cannot use Docker Hub, you could build the base images, compress them, and share them on your corporate network. Then, people can simply download them and load them and build new images on top of them.\nSo in summary, here’s how you can share images with the world, your colleagues, or future you:\n\nOnly share the Dockerfiles. Users need to build the images.\nShare images on Docker Hub. It’s up to you if you want to share a base image with the required development environment, and then separate, smaller images for the pipelines, or if you want to share a single image which contains everything.\nShare images privately using a private registry, or by saving the image unto a file.\n\n\n\n7.2.7 What if you don’t use Nix?\nUsing Nix inside of Docker makes it very easy to setup an environment, but what if you can’t use Nix for some reason? In this case, you would need to use other tools to install the right R or Python packages to build your Docker image and it is likely that it’s going to be more difficulty. The main issue you will likely face is missing development libraries to successfully install R or Python packages. In this case, you will need to first install the right development library. For example, to install the and use the R {stringr} package, you will need to first install libicu-dev. Below is an example of how this may end up looking like:\nFROM rocker/r-ver:4.5.1\n\nRUN apt-get update && apt-get install -y \\\n    libglpk-dev \\\n    libxml2-dev \\\n    libcairo2-dev \\\n    libgit2-dev \\\n    default-libmysqlclient-dev \\\n    libpq-dev \\\n    libsasl2-dev \\\n    libsqlite3-dev \\\n    libssh2-1-dev \\\n    libxtst6 \\\n    libcurl4-openssl-dev \\\n    libharfbuzz-dev \\\n    libfribidi-dev \\\n    libfreetype6-dev \\\n    libpng-dev \\\n    libtiff5-dev \\\n    libjpeg-dev \\\n    unixodbc-dev \\\n    wget\nA way to avoid that is to configure.\nAnother issue that you will face is that building the image is not a reproducible process, only running containers is. To mitigate this issue you can use tagged images (like in the example above) or better yet, using a digest which you can find on Dockerhub:\nFROM rocker/r-ver@sha256:1dbe7a6718b7bd8630addc45a32731624fb7b7ffa08c0b5b91959b0dbf7ba88e\nThis will always pull exactly the same layers. However, this does not completely solve everything. At some point, that version of Ubuntu that you are using will be outdated, and it won’t be able to download anything from repositories anymore. At that point, if you still need that image, you either need to store and keep it, or you will need to start using a newer image, and potentially have to update your code as well. Using Nix, you can stay on ubuntu:latest.\nTo summarise, if you can’t use Nix inside of Docker, you will have to deal with the same issues you face when trying to setup environment on your computer.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Containerization With Docker</span>"
    ]
  },
  {
    "objectID": "08-docker.html#dockerizing-a-rixpress-pipeline",
    "href": "08-docker.html#dockerizing-a-rixpress-pipeline",
    "title": "7  Containerization With Docker",
    "section": "7.3 Dockerizing a {rixpress} Pipeline",
    "text": "7.3 Dockerizing a {rixpress} Pipeline\nIn the previous chapter, we learned how to build a fully reproducible, polyglot pipeline using Nix and {rixpress}. This workflow is perfect for development, ensuring that every run is bit-for-bit identical. However, what if you need to share your final data product with a collaborator or deploy it to a server where installing Nix is not an option?\nThis is the ideal use case for Docker. We can package our entire {rixpress} project (the Nix environment definition, the pipeline logic, and all source files) into a single Docker image. This image can then be run by anyone with Docker installed, regardless of their host operating system or whether they have Nix. The user doesn’t build the pipeline; they run the container, and the pre-built results are extracted. If they wish, they can “log-in” to an interactive session to acces the environment within a running Docker container and “play around” with the data and code.\nLet’s take the {rixpress} project we created in the last chapter and dockerize it. Assume your project directory has the following structure:\n.\n├── data/\n│   └── mtcars.csv\n├── gen-env.R\n├── gen-pipeline.R\n├── functions.R\n├── functions.py\n├── default.nix\n└── pipeline.nix\n\n7.3.1 Step 1: The Dockerfile\nCreate a new file named Dockerfile in your project’s root directory. We will use the nix-base image we built earlier as our foundation.\n# Use the base image with Nix pre-installed.\n# If you built the image locally, you can use its local tag:\nFROM nix-base:latest\n\n# If you pushed it to Docker Hub, you (or a collaborator) can use that.\n# This is the more portable and recommended approach for sharing.\n# FROM your-username/nix-base:latest\n\n# Optionally\n# Set a working directory inside the container\nWORKDIR /app\n\n# Copy all your project files into the image's working directory\nCOPY . .\n\n# Build the pipeline\n# This single command leverages the 'pipeline.nix' file.\n# Nix will first build the environment defined in 'default.nix',\n# then it will execute the pipeline defined in 'pipeline.nix'.\n# The results are stored immutably in the /nix/store inside the image.\n# Instead, you can also only copy the R files, and regenerate the\n# pipeline.nix file during the build process\nRUN nix-build pipeline.nix\n\n# The CMD will define what happens when the container is run.\n# We will create a small R script to export the final artifact\n# from the Nix store to a mounted volume.\nCOPY export-results.R .\nCMD [\"Rscript\", \"export-results.R\"]\nThis Dockerfile is elegant and concise because it delegates the heavy lifting of environment and pipeline management to Nix, which is its specialty.\n\n\n7.3.2 Step 2: The Export Script\nThe RUN nix-build command has already executed our entire pipeline during the image build process. This is ideal when your data can be bundled with the image. The user running the container doesn’t need to re-run the pipeline; they just need the output. (Note: If your data is too large or sensitive to be included in the image, an alternative approach is to remove the RUN nix-build step from the Dockerfile and instead execute the pipeline at runtime, using a mounted volume to provide the input data.)\nIn our case however, we can bundle the data in the image. The final artifacts, like our mtcars_head data frame, are now stored in the Nix store within the image. The user running the container doesn’t need to re-run the pipeline; they just need the output.\nCreate a small R script named export-results.R to extract these results:\n# export-results.R\n# This script runs inside the container after the pipeline has been built.\n\n# Ensure {rixpress} is available to find the results\n# The environment is activated via the Nix build process.\nlibrary(rixpress)\nlibrary(jsonlite)\n\n# Define the directory where Docker will mount the output volume.\n# This path MUST match the target path in the `docker run -v` command.\noutput_dir &lt;- \"/output\"\ndir.create(output_dir, showWarnings = FALSE)\n\n# Read the final artifact from the completed pipeline\nmessage(\"Reading target 'mtcars_head'...\")\nfinal_data &lt;- rxp_read(\"mtcars_head\")\n\n# Save the final data to the mounted directory in a universal format\n# You could also save the data in a `.csv` file\noutput_path &lt;- file.path(output_dir, \"mtcars_analysis_result.json\")\nwrite_json(final_data, output_path, pretty = TRUE)\n\nmessage(paste(\"Successfully exported result to\", output_path))\n\n\n7.3.3 Step 3: Build and Run\nWith the Dockerfile and export-results.R in place, you can now build your self-contained data product.\n\nBuild the Docker image: Open a terminal in your project directory and run: bash     docker build -t my-reproducible-pipeline .\nRun the container to get the results: Now, anyone can get the result of your analysis with a single command. We will create a local output folder and mount it into the container. (In case you couldn’t bundle the raw data into the image, then this is also how you would provide the data at run time. The pipeline would only be executed then).\n# Create a directory on your host machine to receive the output\nmkdir -p ./output\n\n# Run the container\ndocker run --rm --name my_pipeline_run \\\n  -v \"$(pwd)/output\":/output \\\n  my-reproducible-pipeline\n\nAfter the container runs and exits, check your local output directory. You will find the mtcars_analysis_result.json file, containing the exact, reproducible result of your pipeline. The docker run command automatically executed the CMD [\"Rscript\", \"export-results.R\"] we defined in our Dockerfile, which extracted the pre-built artifact.\nYou have successfully packaged a complex, polyglot pipeline into a simple, portable Docker image. This workflow combines the best of both worlds: Nix’s unparalleled power for creating reproducible builds and Docker’s universal standard for distributing and running applications.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Containerization With Docker</span>"
    ]
  },
  {
    "objectID": "08-docker.html#further-reading",
    "href": "08-docker.html#further-reading",
    "title": "7  Containerization With Docker",
    "section": "7.4 Further reading",
    "text": "7.4 Further reading\n\nhttps://www.statworx.com/content-hub/blog/wie-du-ein-r-skript-in-docker-ausfuehrst/ (in German, English translation: https://www.r-bloggers.com/2019/02/running-your-r-script-in-docker/)\nhttps://colinfay.me/docker-r-reproducibility/\nhttps://jsta.github.io/r-docker-tutorial/\nhttp://haines-lab.com/post/2022-01-23-automating-computational-reproducibility-with-r-using-renv-docker-and-github-actions/",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Containerization With Docker</span>"
    ]
  },
  {
    "objectID": "08-docker.html#hands-on-exercises",
    "href": "08-docker.html#hands-on-exercises",
    "title": "7  Containerization With Docker",
    "section": "7.5 Hands-on Exercises",
    "text": "7.5 Hands-on Exercises\n\n7.5.1 Level 1: Foundational Skills\nThese exercises focus on mastering the basic Docker commands and concepts.\nExercise 1: The Ephemeral Container and the Persistent Volume\nThe goal of this exercise is to solidify your understanding of how containers are ephemeral and how volumes provide persistence.\n\nRun the rocker/tidyverse image in detached mode (-d), giving it a name (e.g., my-rstudio). Publish the port 8787 and set a password.\nConnect to the RStudio instance in your browser. Create a new R script named test_script.R and save it in the default home directory (/home/rstudio). Inside the script, write a simple R command like message(\"Hello from inside     the container!\").\nStop and remove the container using docker stop and docker rm (or just use --rm from the start).\nRun a new container with the same command. Connect to RStudio again. Is your test_script.R still there? (It shouldn’t be).\nNow, create a folder on your local machine called r_projects.\nRun the rocker/tidyverse container again, but this time, mount your r_projects folder as a volume to /home/rstudio/projects inside the container. The flag should look something like -v     \"$(pwd)/r_projects\":/home/rstudio/projects.\nConnect to RStudio, navigate to the projects folder, and create your test_script.R there.\nStop and remove the container. Check your local r_projects folder. Is the script there? (It should be). This demonstrates how volumes link your host machine to the container’s filesystem.\n\nExercise 2: The Container Inspector\nThis exercise is designed to get you comfortable with interacting with a running container from your terminal.\n\nRun a basic ubuntu:latest container in detached mode (-d) and give it a name like my-ubuntu-box. Use the command sleep 3600 to keep it running for an hour.\n\nHint: The full command would be docker run -d --name my-ubuntu-box ubuntu:latest sleep 3600.\n\nUse docker ps to verify that your container is running.\nUse docker exec to get an interactive bash shell inside the my-ubuntu-box container.\nOnce inside, run the following Linux commands: ls -la, pwd, whoami, and cat /etc/os-release. What do you observe?\nStill inside the container, use apt-get update && apt-get install -y     fortunes to install a fun package. Run the fortune command.\nExit the container’s shell (type exit). Is the container still running? (It should be).\nStop the container using docker stop my-ubuntu-box.\n\n\n\n7.5.2 Level 2: Building and Distributing Images\nThese exercises focus on creating your own Dockerfile and sharing your work.\nExercise 3: Your First Custom R Image\nCreate a Dockerfile that builds a simple, non-Nix R image.\n\nCreate a new project folder. Inside, create a Dockerfile.\nIn the Dockerfile, start from rocker/r-ver:4.5.1 (or any other versioned tag).\nAdd a RUN command to install the R package {cowsay} from CRAN.\nCreate an R script named run.R in the same folder. The script should contain: r     library(cowsay)     say(\"Moo-ving to Docker!\", by = \"cow\")\nIn your Dockerfile, add a COPY command to copy run.R into the image (e.g., to /home/run.R).\nSet the CMD to execute your script using Rscript.\nBuild the image with the tag my-cowsay-app.\nRun a container from your new image. You should see the cow’s message printed to your terminal.\n\nExercise 4: Publish Your Nix Base\nTake the nix-base image you created in the chapter and practice the full distribution workflow.\n\nIf you haven’t already, build the nix-base image locally.\nCreate a free account on Docker Hub.\nLog in to Docker Hub from your terminal using docker login.\nTag your nix-base:latest image with your Docker Hub username, e.g., docker tag nix-base:latest your-username/nix-base:1.0.\nPush the image to Docker Hub using docker push.\nTo test that it works, remove your local copy of the image: docker rmi your-username/nix-base:1.0 and docker rmi nix-base:latest.\nCreate a simple Dockerfile that starts with FROM your-username/nix-base:1.0. When you build this Dockerfile, Docker should pull the image you just pushed from Docker Hub.\n\n\n\n7.5.3 Level 3: The Capstone Project\nThis exercise integrates all the concepts from the chapter into a complete, reproducible data product.\nExercise 5: Package Your Own {rixpress} Pipeline\nTake a {rixpress} pipeline (you can use the one from the previous chapter’s exercises or create a new one) and package it into a distributable Docker image.\n\nYour project should contain all the necessary files: gen-env.R, gen-pipeline.R, your default.nix and pipeline.nix, and any data/function scripts.\nCreate a Dockerfile that uses your published nix-base image from Exercise 4 as its FROM source.\nThe Dockerfile should:\n\nSet a working directory (e.g., /app).\nCOPY all your project files into the image.\nRUN the nix-build pipeline.nix command to execute the pipeline during the build process.\nInclude an export-results.R script (you’ll need to write this) that saves one or more of your final pipeline artifacts to an /output directory.\nSet the CMD to run your export-results.R script.\n\nBuild the final image with a descriptive tag (e.g., my-final-analysis:latest).\nRun the image, mounting a local output folder to the container’s /output folder.\nVerify that the final results (e.g., a plot, a CSV, or a JSON file) appear in your local output folder. Congratulations, you’ve created a fully portable and reproducible data product!\n\nExtra Challenge: Modify your solution from Exercise 5 for a “big data” scenario where the input data cannot be included in the image. * Your Dockerfile should not copy the data and should not run nix-build at build time. * Instead, the CMD should execute the entire pipeline at runtime. You will need to figure out how to pass the paths for input data and output results to your script. * The docker run command will now need to mount two volumes: one for the input data and one for the output results.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Containerization With Docker</span>"
    ]
  },
  {
    "objectID": "09-github_actions.html",
    "href": "09-github_actions.html",
    "title": "8  Intro to CI/CD with Github Actions",
    "section": "",
    "text": "8.1 Introduction\nWhat you’ll have learned by the end of the chapter: very basic knowledge of Github Actions, but enough to run your RAP in the cloud.\nWe are almost at the end; actually, we could have stopped at the end of the previous chapter. We have reached our goal; we are able to run pipeline in a 100% reproducible way. However, this requires some manual steps. And maybe that’s not a problem; if your image is done, and users only need to pull it and run the container, that’s not really a big problem. But you should keep in mind that manual steps don’t scale. Let’s imagine another context; let’s suppose that you are part of a company and that you are part of a team that needs to quickly ship products to clients. Maybe several people contribute to the product using an internal version control solution (like a Gitlab instance that is deployed on the premises of your company). Maybe you even need to work on several products in the same day; you (and your teammates) should only be focusing writing code (and Dockerfiles)… your time and resources cannot get clogged by building images (which depending on what you’re working on, can take quite some time). So ideally, we would want to automate this step. That is what we are going to learn in this chapter.\nThis chapter will introduce you to the basic ideas of CI/CD (Continuous Integration and Continuous Deployment/Delivery) and DevOps with Github Actions. Because we’re using Git to trigger all the events and automate the whole pipeline, this can also be referred to as GitOps. What’s Dev(Git)Ops? I think that the Atlassian page on DevOps makes a good job of explaining it. The bottom line is that DevOps makes it easy for developers to focus on coding, and makes it easy for them to ship data products. The core IT team provides the required infrastructure and tools to make this possible. GitOps is a variant of DevOps where the definition of the infrastructure is versioned, and can be changed by editing simple text files. Through events, such as pushing to the repository, new images can be built, or containers executed. Data products can then also be redeployed automatically. All the steps we’ve been doing manually, with one simple push! It’s also possible, in the context of package development, to execute unit tests when code gets pushed to repo, or get documentation and vignettes compiled. This also means that you could be developing on a very thin client with only a text editor and git installed. Pushing to Github would then execute everything needed to have a package ready for sharing.\nSo our goal here is, in short, to do exactly the same as what we have been doing on our computer (so build an image, run a container, and get back 3 plots), but on Github.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Intro to CI/CD with Github Actions</span>"
    ]
  },
  {
    "objectID": "09-github_actions.html#getting-your-repo-ready-for-github-actions",
    "href": "09-github_actions.html#getting-your-repo-ready-for-github-actions",
    "title": "8  Intro to CI/CD with Github Actions",
    "section": "8.2 Getting your repo ready for Github Actions",
    "text": "8.2 Getting your repo ready for Github Actions\nYou should see an “Actions” tab on top of any Github repo:\n\n\n\n\n\n\n\n\n\n\n\nThis will open a new view where you can select a lot of available, ready to use actions. Shop around for a bit, and choose the right one (depending on what you want to do). You should know that there is a very nice repository with many actions for R. Once you’re done choosing an action, a new view in which you can edit a file will open. This file will have the name of the chosen action, and have the .yml extension. This file will be automatically added to your repository, in the following path: .github/workflows.\nLet’s take a look at such a workflow file:\nname: Hello world\non: [push]\njobs:\n  say-hello:\n    runs-on: ubuntu-latest\n    steps:\n      - run: echo \"Hello from Github Actions!\"\n      - run: echo \"This command is running from an Ubuntu VM each time you push.\"\nLet’s study this workflow definition line by line:\nname: Hello world\nSimply gives a name to the workflow.\non: [push]\nWhen should this workflow be triggered? Here, whenever something gets pushed.\njobs:\nWhat is the actual things that should happen? This defines a list of actions.\n  say-hello:\nThis defines the say-hello job.\n    runs-on: ubuntu-latest\nThis job should run on an Ubuntu VM. You can also run jobs on Windows or macOS VMs, but this uses more compute minutes than a Linux VM (you have 2000 compute minutes for free per month).\n    steps:\nWhat are the different steps of the job?\n      - run: echo \"Hello from Github Actions!\"\nFirst, run the command echo \"Hello from Github Actions!\". This commands runs inside the VM. Then, run this next command:\n      - run: echo \"This command is running from an Ubuntu VM each time you push.\"\nLet’s push, and see what happens on github.com:\n\n  \n\nIf we take a look at the commit we just pushed, we see this yellow dot next to the commit name. This means that an action is running. We can then take a look at the output of the job, and see that our commands, defined with the run statements in the workflow file, succeeded and echoed what we asked them.\nSo, the next step is running our Docker image and getting back our plots. This next example can be found in this repository. This example doesn’t use Nix, {rix} nor {rixpress}, but the point here is to show how a Docker image can be executed on GitHub Actions, and artifacts can be recovered. The process is always the same, regardless is inside the Docker image.\nThis is what our workflow file looks like:\nname: Reproducible pipeline\n\non:\n  push:\n    branches: [ \"main\" ]\n  pull_request:\n    branches: [ \"main\" ]\n\njobs:\n\n  build:\n\n    runs-on: ubuntu-latest\n\n    steps:\n    - uses: actions/checkout@v5\n    - name: Build the Docker image\n      run: docker build -t my-image-name .\n    - name: Docker Run Action\n      run: docker run --rm --name my_pipeline_container -v /github/workspace/fig/:/home/graphs/:rw my-image-name\n    - uses: actions/upload-artifact@v4\n      with:\n        name: my-figures\n        path: /github/workspace/fig/\n\nFor now, let’s focus on the run statements, because these should be familiar:\nrun: docker build -t my-image-name .\nand:\nrun: docker run --rm --name my_pipeline_container -v /github/workspace/fig/:/home/graphs/:rw my-image-name\nThe only new thing here, is that the path has been changed to /github/workspace/. This is the home directory of your repository, so to speak. Now there’s the uses keyword that’s new:\nuses: actions/checkout@v5\nThis action checkouts your repository inside the VM, so the files in the repo are available inside the VM. Then, there’s this action here:\n- uses: actions/upload-artifact@v4\n  with:\n    name: my-figures\n    path: /github/workspace/fig/\nThis action takes what’s inside /github/workspace/fig/ (which will be the output of our pipeline) and makes the contents available as so-called “artifacts”. Artifacts are the outputs of your workflow. In our case, as stated, the output of the pipeline. So let’s run this by pushing a change, and let’s take a look at these artifacts!\n\n  \n\nAs you can see from the video above, a zip file is now available and can be downloaded. This zip contains our plots! It is thus possible to rerun our workflow in the cloud. This has the advantage that we can now focus on simply changing the code, and not have to bother with boring manual steps. For example, let’s change this target in the _targets.R file:\ntar_target(\n    commune_data,\n    clean_unemp(unemp_data,\n                place_name_of_interest = c(\"Luxembourg\", \"Dippach\",\n                                           \"Wiltz\", \"Esch/Alzette\",\n                                           \"Mersch\", \"Dudelange\"),\n                col_of_interest = active_population)\n)\n\nI’ve added “Dudelange” to the list of communes to plot. Let me push this change to the repo now, and let’s take a look at the artifacts. The video below summarises the process:\n\n  \n\nAs you can see in the video, the _targets.R script was changed, and the changes pushed to Github. This triggered the action we’ve defined before. The plots (artifacts) get refreshed, and we can download them. We see then that Dudelange was added in the communes.png plot!\nIt is also possible to “deploy” the plots directly to another branch, and do much, much more. I just wanted to give you a little taste of Github Actions (and more generally GitOps). The possibilities are virtually limitless, and I still can’t get over the fact that Github Actions is free for public repositories.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Intro to CI/CD with Github Actions</span>"
    ]
  },
  {
    "objectID": "09-github_actions.html#building-a-docker-image-and-pushing-it-to-a-registry",
    "href": "09-github_actions.html#building-a-docker-image-and-pushing-it-to-a-registry",
    "title": "8  Intro to CI/CD with Github Actions",
    "section": "8.3 Building a Docker image and pushing it to a registry",
    "text": "8.3 Building a Docker image and pushing it to a registry\nIt is also possible to build a Docker image and have it made available on an image registry. You can see how this works on this repository. This images can then be used as a base for other RAPs, as in this repository. Why do this? Well because of “separation of concerns”. You could have a repository which builds in image containing your development environment: this could be an image with a specific version of R and R packages built with Nix. And then have as many repositories as projects that run RAPs using that development environment image as a basis. Simply add the project-specific packages that you need for each project.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Intro to CI/CD with Github Actions</span>"
    ]
  },
  {
    "objectID": "09-github_actions.html#running-a-pipeline-straight-from-github-actions",
    "href": "09-github_actions.html#running-a-pipeline-straight-from-github-actions",
    "title": "8  Intro to CI/CD with Github Actions",
    "section": "8.4 Running a pipeline straight from Github Actions",
    "text": "8.4 Running a pipeline straight from Github Actions\nUsing Docker on Github Actions has the advantage that you can use the same image to develop locally on your computer, and then also on CI. However, you could also run the pipeline straight from a Github Actions runner, but it’ll take some effort to set up the environment on CI. Take a look at the example from this repository.\nThe yaml file used in this action (which you can find here) was generated by running targets::tar_github_actions() and was then modified further, mostly to add the required development libraries to compile the needed R packages (under the Install Linux system dependencies step).\nThis action takes advantage of the included Github Actions cache to backup the targets from the pipeline, so they can also get skipped with subsequent runs:\n\n\n\n\n\n\n\n\n\n\n\nThis can also be achieved with Docker by mounting volumes, but requires more manual setup.\nAnother difference with Docker is that the outputs are not saved as an artifact, but instead get pushed to the targets-runs branch:\n\n\n\n\n\n\n\n\n\n\n\nThe previous examples assumed you didn’t use Nix, but if you did, you can also run pipelines from inside Nix and with {rixpress}. Look at this yaml file from the rixpress_demos repository that shows how to run a {rixpress} pipeline on GitHub Actions link.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Intro to CI/CD with Github Actions</span>"
    ]
  },
  {
    "objectID": "09-github_actions.html#running-unit-tests-on-github-actions",
    "href": "09-github_actions.html#running-unit-tests-on-github-actions",
    "title": "8  Intro to CI/CD with Github Actions",
    "section": "8.5 Running unit tests on Github Actions",
    "text": "8.5 Running unit tests on Github Actions\nSetting up your project as a package (or at least, the parts of your project that can be reused for others) as a package also has the advantage that it becomes very easy to run unit tests on CI. See for example the {myPackage} package that we developed together, in particular this file. This action runs on each push and pull request on Windows, Ubuntu and macOS:\non:\n  push:\n    branches: [ \"main\" ]\n  pull_request:\n    branches: [ \"main\" ]\n\njobs:\n  rcmdcheck:\n    runs-on: ${{ matrix.os }}\n    strategy:\n      matrix:\n        os: [ubuntu-latest, windows-latest, macos-latest]\nSeveral steps are executed, all using pre-defined actions from the r-lib project:\n    steps:\n    - uses: actions/checkout@v4\n    - uses: r-lib/actions/setup-r@v2\n    - uses: r-lib/actions/setup-r-dependencies@v2\n      with:\n        extra-packages: any::rcmdcheck\n        needs: check\n    - uses: r-lib/actions/check-r-package@v2\nAn action such as r-lib/actions/setup-r@v2 will install R on any of the supported operating systems without requiring any configuration from you. If you didn’t use such an action, you would need to define three separate actions: one that would be executed on Windows, on Ubuntu and on macOS. Each of these operating-specific actions would install R in their operating-specific way.\nCheck out the workflow results to see how the package could be improved here.\nHere again, using Nix simplifies this process immensely. Look at this workflow file from {rix}’s repository here. Setting up the environment is much easier, as is running the actual test suite.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Intro to CI/CD with Github Actions</span>"
    ]
  },
  {
    "objectID": "09-github_actions.html#further-reading",
    "href": "09-github_actions.html#further-reading",
    "title": "8  Intro to CI/CD with Github Actions",
    "section": "8.6 Further reading",
    "text": "8.6 Further reading\n\nhttp://haines-lab.com/post/2022-01-23-automating-computational-reproducibility-with-r-using-renv-docker-and-github-actions/\nhttps://orchid00.github.io/actions_sandbox/\nhttps://www.petefreitag.com/item/903.cfm\nhttps://dev.to/mihinduranasinghe/using-docker-containers-in-jobs-github-actions-3eof",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Intro to CI/CD with Github Actions</span>"
    ]
  },
  {
    "objectID": "10-what-else.html",
    "href": "10-what-else.html",
    "title": "9  What else should you learn?",
    "section": "",
    "text": "9.1 Touch typing\nHere’s a list of things I think would be nice for you to invest some time in, in no particular order.\nOne of the things I NEVER see discussed when talking “upskilling” is improving your typing speed. According to a survey (which I’m sure is not statistically, nor scientifically sound, but still…) by onlinetyping.org (which you can find here, most back office workers (who spend all day typing) have a typing speed of 20 to 30 wpm (words per minute). According to this article by the Atlantic people write about 41638 words in email per year. You as programmers (yes, even if you’re focused on data, you’re a programmer) very surely type twice or thrice this amount of words per year. But let’s stay with 41638 words per year. That would translate to almost 28 days of non stop typing at a typing speed of 25 words per minute. Doubling to 50 wpm is actually quite easy, and reaching 70 is really doable. This could improve productivity, or better yet, make you go home earlier instead of working until 19h00 every day because you type like a snail.\nYou need to learn touch typing, meaning, typing without looking at your keyboard.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>What else should you learn?</span>"
    ]
  },
  {
    "objectID": "10-what-else.html#vim",
    "href": "10-what-else.html#vim",
    "title": "9  What else should you learn?",
    "section": "9.2 Vim",
    "text": "9.2 Vim\nYes, I think you should learn vim, or at the very least, your text editor of choice, by heart. You should know every keyboard shortcut and every possibility that your text editor offers. You should never touch the mouse when writing text. This is not just because of productivity, but also for your health. Grabbing the mouse to click one or twice, and then go back to typing, then go back to moving the mouse, etc, will destroy your shoulder. By keeping your hands on the keyboard at all times and minimizing mouse usage, you may be able to grow old healthy. Vim helps with that because it is a modal text editor (and most editors actually ship a Vim-mode). Watch this video to get a quick introduction on Vim, and how to enable Vim mode in Vscode.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>What else should you learn?</span>"
    ]
  },
  {
    "objectID": "10-what-else.html#statistical-modeling",
    "href": "10-what-else.html#statistical-modeling",
    "title": "9  What else should you learn?",
    "section": "9.3 Statistical modeling",
    "text": "9.3 Statistical modeling\nStatistical modeling is crucial, and if you didn’t major in stats, you very likely lack this knowledge. Here’s a reading (and watching) list:\n\nRegression and other stories (has a free PDF)\nStatistical Rethinking 2022 (on youtube)\nMostly harmless econometrics",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>What else should you learn?</span>"
    ]
  },
  {
    "objectID": "11-conclusion.html",
    "href": "11-conclusion.html",
    "title": "10  Conclusion",
    "section": "",
    "text": "10.1 Why bother\nWe’re at the end of this course, which I hope you enjoyed. There is now yet another question we need to ask ourselves: is this worth it? Why should we bother with making our pipelines reproducible? I believe that there are two, fundamental, essential reasons.\nThe first one, is that time is finite, and working manually does not scale. Reproducible pipelines do take time to set up, but they allow us to win this time back once we start re-running them. Wasting time and resources running things manually (with the potential for introducing errors) is simply not acceptable. This freed up time can then be used to provide further value to your employer, yourself, and ideally your community as well.\nThe second reason, is that setting up RAPs is in itself an enjoyable activity, which requires the full depth and breadth of your skills. If you’re working in science, there is the added benefit that by setting up a RAP you’re doing actual science: providing a reproducible analysis where an hypothesis gets tested (an not writing papers).",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  }
]