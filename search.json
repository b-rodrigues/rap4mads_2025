[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Building Reproducible Analytical Pipelines",
    "section": "",
    "text": "Introduction\nThis is the 2025 edition of the course. If you‚Äôre looking for the 2024 edition, you can click here\nWhat‚Äôs new:\nThis course is based on my book titled Building Reproducible Analytical Pipelines with R. This course focuses only on certain aspects that are discussed in greater detail in the book.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Building Reproducible Analytical Pipelines",
    "section": "Schedule",
    "text": "Schedule\n\n2025/09/04 - 4 hours,\n2025/09/11 - 4 hours,\n2025/09/14 - 4 hours,\n2025/09/02 - 4 hours,\n2025/09/05 - 2 hours,\n2025/09/09 - 5 hours,\n2025/09/16 - 4 hours,\n2025/09/19 - 3 hours,",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#reproducible-analytical-pipelines",
    "href": "index.html#reproducible-analytical-pipelines",
    "title": "Building Reproducible Analytical Pipelines",
    "section": "Reproducible analytical pipelines?",
    "text": "Reproducible analytical pipelines?\nThis course is my take on setting up code that results in some data product. This code has to be reproducible, documented and production ready. Not my original idea, but introduced by the UK‚Äôs Analysis Function.\nThe basic idea of a reproducible analytical pipeline (RAP) is to have code that always produces the same result when run, whatever this result might be. This is obviously crucial in research and science, but this is also the case in businesses that deal with data science/data-driven decision making etc.\nA well documented RAP avoids a lot of headache and is usually re-usable for other projects as well.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#data-products",
    "href": "index.html#data-products",
    "title": "Building Reproducible Analytical Pipelines",
    "section": "Data products?",
    "text": "Data products?\nIn this course each of you will develop a data product. A data product is anything that requires data as an input. This can be a very simple report in PDF or Word format or a complex web app. This website is actually also a data product, which I made using the R programming language. In this course we will not focus too much on how to create automated reports or web apps (but I‚Äôll give an introduction to these, don‚Äôt worry) but our focus will be on how to set up a pipeline that results in these data products in a reproducible way.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#machine-learning",
    "href": "index.html#machine-learning",
    "title": "Building Reproducible Analytical Pipelines",
    "section": "Machine learning?",
    "text": "Machine learning?\nNo, being a master in machine learning is not enough to become a data scientist. Actually, the older I get, the more I think that machine learning is almost optional. What is not optional is knowing how:\n\nto write, test, and properly document code;\nto acquire (reading in data can be tricky!) and clean data;\nto work inside the Linux terminal/command line interface;\nto use Git, Docker for Dev(Git)Ops;\nthe Internet works (what‚Äôs a firewall? what‚Äôs a reverse proxy? what‚Äôs a domain name? etc, etc‚Ä¶);\n\nBut what about machine learning? Well, depending what you‚Äôll end up doing, you might indeed focus a lot on machine learning and/or statistical modeling. That being said, in practice, it is very often much more efficient to let some automl algorithm figure out the best hyperparameters of a XGBoost model and simply use that, at least as a starting point (but good luck improving upon automl‚Ä¶). What matters, is that the data you‚Äôre feeding to your model is clean, that your analysis is sensible, and most importantly, that it could be understood by someone taking over (imagine you get sick) and rerun with minimal effort in the future. The model here should simply be a piece that could be replaced by another model without much impact. The model is rarely central‚Ä¶ but of course there are exceptions to this, especially in research, but every other point I‚Äôve made still stands. It‚Äôs just that not only do you have to care about your model a lot, you also have to care about everything else.\nSo in this course we‚Äôre going to learn a bit of all of this. We‚Äôre going to learn how to write reusable code, learn some basics of the Linux command line, Git and Docker.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#what-actually-is-reproducibility",
    "href": "index.html#what-actually-is-reproducibility",
    "title": "Building Reproducible Analytical Pipelines",
    "section": "What actually is reproducibility?",
    "text": "What actually is reproducibility?\nA reproducible project means that this project can be rerun by anyone at 0 (or very minimal) cost. But there are different levels of reproducibility, and I will discuss this in the next section. Let‚Äôs first discuss some requirements that a project must have to be considered a RAP.\n\nThe requirements of a RAP\nFor something to be truly reproducible, it has to respect the following bullet points:\n\nSource code must obviously be available and thoroughly tested and documented (which is why we will be using Git and Github);\nAll the dependencies must be easy to find and install (we are going to deal with this using dependency management tools);\nTo be written with an open source programming language (nocode tools like Excel are by default non-reproducible because they can‚Äôt be used non-interactively, and which is why we are going to use the R programming language);\nThe project needs to be run on an open source operating system (thankfully, we can deal with this without having to install and learn to use a new operating system, thanks to Docker);\nData and the paper/report need obviously to be accessible as well, if not publicly as is the case for research, then within your company.\n\nAlso, reproducibility is on a continuum, and depending on the constraints you face your project can be ‚Äúnot very reproducible‚Äù to ‚Äútotally reproducible‚Äù. Let‚Äôs consider the following list of anything that can influence how reproducible your project truly is:\n\nVersion of the programming language used;\nVersions of the packages/libraries of said programming language used;\nOperating System, and its version;\nVersions of the underlying system libraries (which often go hand in hand with OS version, but not necessarily).\nAnd even the hardware architecture that you run all that software stack on.\n\nSo by ‚Äúreproducibility is on a continuum‚Äù, what I mean is that you could set up your project in a way that none, one, two, three, four or all of the preceding items are taken into consideration when making your project reproducible.\nThis is not a novel, or new idea. Peng (2011) already discussed this concept but named it the reproducibility spectrum.\n\n\n\nThe reproducibility spectrum from Peng‚Äôs 2011 paper.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#large-language-models",
    "href": "index.html#large-language-models",
    "title": "Building Reproducible Analytical Pipelines",
    "section": "Large Language Models",
    "text": "Large Language Models\nLLMs have rapidly become an essential powertool in the data scientist‚Äôs toolbox. But as with any powertool, beginners risk cutting their fingers if they‚Äôre not careful. So it is important to learn how to use them. This course will give you some pointers on how to integrate LLMs into your workflow.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#why-r-why-not-insert-your-favourite-programming-language",
    "href": "index.html#why-r-why-not-insert-your-favourite-programming-language",
    "title": "Building Reproducible Analytical Pipelines",
    "section": "Why R? Why not [insert your favourite programming language]",
    "text": "Why R? Why not [insert your favourite programming language]\nR is a domain-specific language whose domain is statistics, data analysis/science and machine learning, and as such has many built-in facilities to make handling data very efficient.\nIf you learn R you have access to almost 25‚Äô000 packages (as of June 2025, including both CRAN and Bioconductor packages) to:\n\nclean data (see: {dplyr}, {tidyr}, {data.table}‚Ä¶);\nwork with medium and big data (see: {arrow}, {sparklyr}‚Ä¶);\nvisualize data (see: {ggplot2}, {plotly}, {echarts4r}‚Ä¶);\ndo literate programming (using Rmarkdown or Quarto, you can write books, documents even create a website);\ndo functional programming (see: {purrr}‚Ä¶);\ncall other languages from R (see: {reticulate} to call Python from R);\ndo machine learning and AI (see: {tidymodels}, {tensorflow}, {keras}‚Ä¶)\ncreate webapps (see: {shiny}‚Ä¶)\ndomain specific statistics/machine learning (see CRAN Task Views for an exhaustive list);\nand more\n\nIt‚Äôs not just about what the packages provide: installing R and its packages and dependencies is rarely frustrating, which is not the case with Python (Python 2 vs Python 3, pip vs conda, pyenv vs venv vs uv, ‚Ä¶, dependency hell is a real place full of snakes)\n\n\n\n\n\n\n\n\n\n\n\nThat doesn‚Äôt mean that R does not have any issues. Quite the contrary, R sometimes behaves in seemingly truly bizarre ways (as an example, try running nchar(\"1000000000\") and then nchar(1000000000) and try to make sense of it). To know more about such bizarre behaviour, I recommend you read The R Inferno (linked at the end of this chapter). So, yes, R is far from perfect, but it sucks less than the alternatives (again, in my absolutely objective opinion).\n\n  üü° Loading\n    webR...\n  \n    \n    \n      \n    \n  \n  \n  \n\n\nThat being said, the reality of data science is that the future is becoming more and more polyglot. Data products are evermore complex, and necessity are built using many languages; so ideally we would like to find a way to use whatever tool is best fit for the job at hand. Sometimes it can be R, sometimes Python, sometimes shell scripts, or any other language. This is where Nix will help us.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#nix",
    "href": "index.html#nix",
    "title": "Building Reproducible Analytical Pipelines",
    "section": "Nix",
    "text": "Nix\nNix is a package manager for Linux distributions, macOS and it even works on Windows if you enable WSL2. What‚Äôs a package manager? If you‚Äôre not a Linux user, you may not be aware. Let me explain it this way: in R, if you want to install a package to provide some functionality not included with a vanilla installation of R, you‚Äôd run this:\ninstall.packages(\"dplyr\")\nIt turns out that Linux distributions, like Ubuntu for example, work in a similar way, but for software that you‚Äôd usually install using an installer (at least on Windows). For example you could install Firefox on Ubuntu using:\nsudo apt-get install firefox\n(there‚Äôs also graphical interfaces that make this process ‚Äúmore user-friendly‚Äù). In Linux jargon, packages are simply what we call software (or I guess it‚Äôs all ‚Äúapps‚Äù these days). These packages get downloaded from so-called repositories (think of CRAN, the repository of R packages, or Pypi, in the case of Python) but for any type of software that you might need to make your computer work: web browsers, office suites, multimedia software and so on.\nSo Nix is just another package manager that you can use to install software.\nBut what interests us is not using Nix to install Firefox, but instead to install R, Python and the R and Python packages that we require for our analysis. But why use Nix instead of the usual ways to install software on our operating systems?\nThe first thing that you should know is that Nix‚Äôs repository, nixpkgs, is huge. Humongously huge. As I‚Äôm writing these lines, there‚Äôs more than 120‚Äô000 pieces of software available, and the entirety of CRAN and Bioconductor is also available through nixpkgs. So instead of installing R as you usually do and then use install.packages() to install packages, you could use Nix to handle everything. But still, why use Nix at all?\nNix has an interesting feature: using Nix, it is possible to install software in (relatively) isolated environments. So using Nix, you can install as many versions of R and R packages that you need. Suppose that you start working on a new project. As you start the project, with Nix, you would install a project-specific version of R and R packages that you would only use for that particular project. If you switch projects, you‚Äôd switch versions of R and R packages.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#pre-requisites",
    "href": "index.html#pre-requisites",
    "title": "Building Reproducible Analytical Pipelines",
    "section": "Pre-requisites",
    "text": "Pre-requisites\nI will assume basic programming knowledge, and not much more. Ideally you‚Äôll be following this course from a Linux machine, but if you‚Äôre macOS, that‚Äôs fine as well. On Windows, you will have to set up WSL2 to follow along.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#grading",
    "href": "index.html#grading",
    "title": "Building Reproducible Analytical Pipelines",
    "section": "Grading",
    "text": "Grading\nThe way grading works in this course is as follows: during lecture hours you will follow along. At home, you‚Äôll be working on setting up your own pipeline. For this, choose a dataset that ideally would need some cleaning and/or tweaking to be usable. We are going first to learn how to package this dataset alongside some functions to make it clean. If time allows, I‚Äôll leave some time during lecture hours for you to work on it and ask me and your colleagues for help. At the end of the semester, I will need to download your code and get it running. The less effort this takes me, the better your score. Here is a tentative breakdown:\n\nCode is on github.com and the repository is documented with a Readme.md file: 5 points;\nData and functions to run pipeline are documented and tested: 5 points;\nEvery software dependency is easily installed: 5 points;\nPipeline can be executed in one command: 5 points;\nBonus points: pipeline is dockerized, or uses Nix, and/or uses Github Actions to run? 5 points\n\nThe way to fail this class is to write an undocumented script that only runs on your machine and expect me to debug it to get it to run.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#jargon",
    "href": "index.html#jargon",
    "title": "Building Reproducible Analytical Pipelines",
    "section": "Jargon",
    "text": "Jargon\nThere‚Äôs some jargon that is helpful to know when working with R. Here‚Äôs a non-exhaustive list to get you started:\n\nCRAN: the Comprehensive R Archive Network. This is a curated online repository of packages and R installers. When you type install.packages(\"package_name\") in an R console, the package gets downloaded from there;\nLibrary: the collection of R packages installed on your machine;\nR console: the program where the R interpreter runs;\nPosit/RStudio: Posit (named RStudio in the past) are the makers of the RStudio IDE and of the tidyverse collection of packages;\ntidyverse: a collection of packages created by Posit that offer a common language and syntax to perform any task required for data science ‚Äî from reading in data, to cleaning data, up to machine learning and visualisation;\nbase R: refers to a vanilla installation (and vanilla capabilities) of R. Often used to contrast a tidyverse specific approach to a problem (for example, using base R‚Äôs lapply() in constrast to the tidyverse purrr::map()).\npackage::function(): Functions can be accessed in several ways in R, either by loading an entire package at the start of a script with library(dplyr) or by using dplyr::select().\nFunction factory (sometimes adverb): a function that returns a function.\nVariable: the variable of a function (as in x in f(x)) or the variable from statistical modeling (synonym of feature)\n&lt;- vs =: in practice, you can use &lt;- and = interchangeably. I prefer &lt;-, but feel free to use = if you wish.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#further-reading",
    "href": "index.html#further-reading",
    "title": "Building Reproducible Analytical Pipelines",
    "section": "Further reading",
    "text": "Further reading\n\nAn Introduction to R (from the R team themselves)\nWhat is CRAN?\nThe R Inferno\nBuilding Reproducible Analytical Pipelines with R\nReproducible Analytical Pipelines (RAP)",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Building Reproducible Analytical Pipelines",
    "section": "License",
    "text": "License\nThis course is licensed under the WTFPL.\n\n\n\n\n\nPeng, Roger D. 2011. ‚ÄúReproducible Research in Computational Science.‚Äù Science 334 (6060): 1226‚Äì27.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "02-intro-nix.html",
    "href": "02-intro-nix.html",
    "title": "1¬† Reproducibility with Nix",
    "section": "",
    "text": "1.1 Learning Outcomes\nBy the end of this chapter, you will:",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Reproducibility with Nix</span>"
    ]
  },
  {
    "objectID": "02-intro-nix.html#learning-outcomes",
    "href": "02-intro-nix.html#learning-outcomes",
    "title": "1¬† Reproducibility with Nix",
    "section": "",
    "text": "Understand the need for environment reproducibility in modern workflows\nInstall Nix\nUse {rix} to generate default.nix files\nBuild cross-language environments for data work or software development",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Reproducibility with Nix</span>"
    ]
  },
  {
    "objectID": "02-intro-nix.html#why-reproducibility-why-nix-2h",
    "href": "02-intro-nix.html#why-reproducibility-why-nix-2h",
    "title": "1¬† Reproducibility with Nix",
    "section": "1.2 Why Reproducibility? Why Nix? (2h)",
    "text": "1.2 Why Reproducibility? Why Nix? (2h)\n\n1.2.1 Motivation: Reproducibility in Scientific and Data Workflows\nTo ensure that a project is reproducible you need to deal with at least four things:\n\nMake sure that the required/correct version of R (or any other language) is installed;\nMake sure that the required versions of packages are installed;\nMake sure that system dependencies are installed (for example, you‚Äôd need a working Java installation to install the rJava R package on Linux);\nMake sure that you can install all of this for the hardware you have on hand.\n\nBut in practice, one or most of these bullet points are missing from projects. The goal of this course is to learn how to fullfill all the requirements to build reproducible projects.\n\n\n1.2.2 Problems with Ad-Hoc Tools\nTools like Python‚Äôs venv or R‚Äôs renv only deal with some pieces of the reproducibility puzzle. Often, they assume an underlying OS, do not capture native system dependencies (like libxml2, pandoc, or curl), and require users to ‚Äúrebuild‚Äù their environments from partial metadata. Docker helps but introduces overhead, security challenges, and complexity.\nTraditional approaches fail to capture the entire dependency graph of a project in a deterministic way. This leads to ‚Äúit works on my machine‚Äù syndromes, onboarding delays, and subtle bugs.\n\n\n1.2.3 Nix, a declarative package manager\nNix is a tool for reproducible builds and development environments, often introduced as a package manager. It captures complete dependency trees, from your programming language interpreter to every system-level library you rely on. With Nix, environments are not recreated from documentation, but rebuilt precisely from code.\nNix can be installed on Linux distributions, macOS and it even works on Windows if you enable WSL2. In this course, we will use Nix mostly as a package manager (but towards the end also as a build automation tool).\nWhat‚Äôs a package manager? If you‚Äôre not a Linux user, you may not know. Let me explain it this way: in R, if you want to install a package to provide some functionality not included with a vanilla installation of R, you‚Äôd run this:\ninstall.packages(\"dplyr\")\nIt turns out that Linux distributions, like Ubuntu for example, work in a similar way, but for software that you‚Äôd usually install using an installer (at least on Windows). For example you could install Firefox on Ubuntu using:\nsudo apt-get install firefox\n(there‚Äôs also graphical interfaces that make this process ‚Äúmore user-friendly‚Äù). In Linux jargon, packages are simply what we call software (or I guess it‚Äôs all ‚Äúapps‚Äù these days). These packages get downloaded from so-called repositories (think of CRAN, the repository of R packages) but for any type of software that you might need to make your computer work: web browsers, office suites, multimedia software and so on.\nSo Nix is just another package manager that you can use to install software.\nBut what interests us is not using Nix to install Firefox, but instead to install R and the R packages that we require for our analysis (or any other programming language that we need). But why use Nix instead of the usual ways to install software on our operating systems?\nThe first thing that you should know is that Nix‚Äôs repository, nixpkgs, is huge. Humongously huge. As I‚Äôm writing these lines, there‚Äôs more than 120‚Äô000 pieces of software available, and the entirety of CRAN and Bioconductor is also available through nixpkgs. So instead of installing R as you usually do and then use install.packages() to install packages, you could use Nix to handle everything. But still, why use Nix at all?\nNix has an interesting feature: using Nix, it is possible to install software in (relatively) isolated environments. So using Nix, you can install as many versions of R and R packages that you need. Suppose that you start working on a new project. As you start the project, with Nix, you would install a project-specific version of R and R packages that you would only use for that particular project. If you switch projects, you‚Äôd switch versions of R and R packages.\nHowever Nix has quite a steep learning curve, so this is why for the purposes of this course we are going to use an R package called {rix} to set up reproducible environments.\n\n\n1.2.4 The rix package\nThe idea of {rix} is for you to declare the environment you need using the provided rix() function. rix() is the package‚Äôs main function and generates a file called default.nix which is then used by the Nix package manager to build that environment. Ideally, you would set up such an environment for each of your projects. You can then use this environment to either work interactively, or run R or Python scripts. It is possible to have as many environments as projects, and software that is common to environments will simply be re-used and not get re-installed to save space. Environments are isolated for each other, but can still interact with your system‚Äôs files, unlike with Docker where a volume must be mounted. Environments can also interact with the software installed on your computer through the usual means, which can sometimes lead to issues. For example, if you already have R installed, and a user library of R packages, more caution is required to properly use environments managed by Nix.\nYou don‚Äôt need to have R installed or be an R user to use {rix}. If you have Nix installed on your system, it is possible to ‚Äúdrop‚Äù into a temporary environment with R and {rix} available and generate the required Nix expression from there.\nBut first, let‚Äôs install Nix and try to use temporary shells.\n\n\n1.2.5 Installing Nix\nIf you are on Windows, you need the Windows Subsystem for Linux 2 (WSL2) to run Nix. If you are on a recent version of Windows 10 or 11, you can simply run this as an administrator in PowerShell:\nwsl --install\nYou can find further installation notes at this official MS documentation.\nI recommend to activate systemd in Ubuntu WSL2, mainly because this supports other users than root running Nix. To set this up, please do as outlined this official Ubuntu blog entry:\n\n# in WSL2 Ubuntu shell\n\nsudo -i\nnano /etc/wsl.conf\nThis will open the /etc/wsl.conf in a nano, a command line text editor. Add the following line:\n[boot]\nsystemd=true\nSave the file with CTRL-O and then quit nano with CTRL-X. Then, type the following line in powershell:\nwsl --shutdown\nand then relaunch WSL (Ubuntu) from the start menu. For those of you running Windows, we will be working exclusively from WSL2 now. If that is not an option, then I highly recommend you set up a virtual machine with Ubuntu using VirtualBox for example, or dual-boot Ubuntu.\nInstalling (and uninstalling) Nix is quite simple, thanks to the installer from Determinate Systems, a company that provides services and tools built on Nix, and works the same way on Linux (native or WSL2) and macOS.\nDo not use your operating system‚Äôs package manager to install Nix. Instead, simply open a terminal and run the following line (on Windows, if you cannot or have decided not to activate systemd, then you have to append --init none to the command. You can find more details about this on The Determinate Nix Installer page):\n\ncurl --proto '=https' --tlsv1.2 -sSf \\\n    -L https://install.determinate.systems/nix | \\\n     sh -s -- install\n\nThen, install the cachix client and configure the rstats-on-nix cache: this will install binary versions of many R packages which will speed up the building process of environments:\nnix-env -iA cachix -f https://cachix.org/api/v1/install\nthen use the cache:\ncachix use rstats-on-nix\nYou only need to do this once per machine you want to use {rix} on. Many thanks to Cachix for sponsoring the rstats-on-nix cache!\n\n\n1.2.6 Temporary shells\nYou now have Nix installed; before continuing, it let‚Äôs see if everything works (close all your terminals and reopen them) by droping into a temporary shell with a tool you likely have not installed on your machine.\nOpen a terminal and run:\nwhich sl\nyou will likely see something like this:\nwhich: no sl in ....\nnow run this:\nnix-shell -p sl\nand then again:\nwhich sl\nthis time you should see something like:\n/nix/store/cndqpx74312xkrrgp842ifinkd4cg89g-sl-5.05/bin/sl\nThis is the path to the sl binary installed through Nix. The path starts with /nix/store: the Nix store is where all the software installed through Nix is stored. Now type sl and see what happens!\nYou can find the list of available packages here.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Reproducibility with Nix</span>"
    ]
  },
  {
    "objectID": "02-intro-nix.html#session-1.2-dev-environments-with-nix-2h",
    "href": "02-intro-nix.html#session-1.2-dev-environments-with-nix-2h",
    "title": "1¬† Reproducibility with Nix",
    "section": "1.3 Session 1.2 ‚Äì Dev Environments with Nix (2h)",
    "text": "1.3 Session 1.2 ‚Äì Dev Environments with Nix (2h)\n\n1.3.1 Some Nix concepts\nWhile temporary shells are useful for quick testing, this is not how Nix is typically used in practice. Nix is a declarative package manager: users specify what they want to build, and Nix takes care of the rest.\nTo do so, users write files called default.nix that contain the a so-called Nix expression. This expression will contain the definition of a (or several) derivations.\nIn Nix terminology, a derivation is a specification for running an executable on precisely defined input files to repeatably produce output files at uniquely determined file system paths. (source)\nIn simpler terms, a derivation is a recipe with precisely defined inputs, steps, and a fixed output. This means that given identical inputs and build steps, the exact same output will always be produced. To achieve this level of reproducibility, several important measures must be taken:\n\nAll inputs to a derivation must be explicitly declared.\nInputs include not just data files, but also software dependencies, configuration flags, and environment variables, essentially anything necessary for the build process.\nThe build process takes place in a hermetic sandbox to ensure the exact same output is always produced.\n\nThe next sections of this document explain these three points in more detail.\n\n\n1.3.2 Derivations\nHere is an example of a simple Nix expression:\nlet\n pkgs = import (fetchTarball \"https://github.com/rstats-on-nix/nixpkgs/archive/2025-04-11.tar.gz\") {};\n\nin\n\npkgs.stdenv.mkDerivation {\n  name = \"filtered_mtcars\";\n  buildInputs = [ pkgs.gawk ];\n  dontUnpack = true;\n  src = ./mtcars.csv;\n  installPhase = ''\n    mkdir -p $out\n    awk -F',' 'NR==1 || $9==\"1\" { print }' $src &gt; $out/filtered.csv\n  '';\n}\nI won‚Äôt go into details here, but what‚Äôs important is that this code uses awk, a common Unix data processing tool, to filter the mtcars.csv file to keep only rows where the 9th column (the am column) equals 1. As you can see, a significant amount of boilerplate code is required to perform this simple operation. However, this approach is completely reproducible: the dependencies are declared and pinned to a specific dated branch of our rstats-on-nix/nixpkgs fork (more on this later), and the only thing that could make this pipeline fail (though it‚Äôs a bit of a stretch to call this a pipeline) is if the mtcars.csv file is not provided to it. This expression can be instantiated into a derivation, and the derivation is then built into the actual output that interests us, namely the filtered mtcars data.\nThe derivation above uses the Nix builtin function mkDerivation: as its name implies, this function makes a derivation. But there is also mkShell, which is the function that builds a shell instead. Nix expressions that built a shell is the kind of expressions {rix} generates for you.\n\n\n1.3.3 Using {rix} to generate development environments\nIf you have successfully installed Nix, but don‚Äôt have yet R installed on your system, you could install R as you would usually do on your operating system, and then install the {rix} package, and from there, generate project-specific expressions and build them. But you could also install R using Nix. Running the following line in a terminal will drop you in an interactive R session that you can use to start generating expressions:\nnix-shell -p R rPackages.rix\nThis will drop you in a temporary shell with R and {rix} available. Navigate to an empty directory to help a project, call it rix-session-1:\nmkdir rix-session-1\nand start R and load {rix}:\nR\nlibrary(rix)\nyou can now generate an expression by running the following code:\nrix(\n  date = \"2025-06-02\",\n  r_pkgs = c(\"dplyr\", \"ggplot2\"),\n  py_conf = list(\n    py_version = \"3.13\",\n    py_pkgs = c(\"polars\", \"great-tables\")\n  ),\n  ide = \"positron\",\n  project_path = \".\",\n  overwrite = TRUE\n)\nThis will write a file called default.nix in your project‚Äôs directory. This default.nix contains a Nix expression which will build a shell that comes with R, {dplyr} and {ggplot2} as they were on the the 2nd of June 2025 on CRAN. This will also add Python 3.13 and the ploars and great-tables Python packages as they were at the time in nixpkgs (more on this later). Finally, this also add the Positron IDE, which is a fork of VS Code for data science. This is just an example, and you can use another IDE if you wish. See this vignette for learning how to setup your IDE with Nix.\n\n\n1.3.4 Using nix-shell to Launch Environments\nOnce your file is in place, simply run:\nnix-shell\nThis gives you an isolated shell session with all declared packages available. You can test code, explore APIs, or install further tools within this session.\nTo remove the packages that were installed, call nix-store --gc. This will call the garbage collector. If you want to avoid that an environment gets garbage-collected, use nix-build instead of nix-shell. This will create a symlink called result in your project‚Äôs root directory and nix-store --gc won‚Äôt garbage-collect this environment until you manually remove result.\n\n\n1.3.5 Pinning with nixpkgs\nTo ensure long-term reproducibility, pin the version of Nixpkgs used. Replace &lt;nixpkgs&gt; with a fixed import:\nlet\n  pkgs = import (fetchTarball \"https://github.com/rstats-on-nix/nixpkgs/archive/2025-06-02.tar.gz\") {};\nin\npkgs.mkShell {\n  buildInputs = [ pkgs.r pkgs.rPackages.dplyr ];\n}\nThis avoids unexpected updates and lets others reproduce your environment exactly.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Reproducibility with Nix</span>"
    ]
  },
  {
    "objectID": "02-intro-nix.html#hands-on-exercises",
    "href": "02-intro-nix.html#hands-on-exercises",
    "title": "1¬† Reproducibility with Nix",
    "section": "1.4 Hands-On Exercises",
    "text": "1.4 Hands-On Exercises\n\nStart a temporary shell with R and {rix} again using nix-shell -p R rPackages.rix. Start an R session (by typing R) and then load the {rix} package (using library(rix)). Run the available_dates() function: using the latest available date, generate a new default.nix.\nInside of an activated shell, type which R and echo $PATH. Explore what is being added to your environment. What is the significance of paths like /nix/store/...?\nBreak it on purpose: generate a new environment with a wrong R package name, for example dplyrnaught. Try to build the environment. What happens?\nGo to https://search.nixos.org/packages and look for packages that you usually use for your projects to see if they are available.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Reproducibility with Nix</span>"
    ]
  },
  {
    "objectID": "03-git.html",
    "href": "03-git.html",
    "title": "2¬† Git",
    "section": "",
    "text": "2.1 Introduction\nWhat you‚Äôll learn by the end of this chapter: - How to manage your own data science projects using Git‚Äôs core command-line tools. - How to collaborate effectively with a team using professional workflows like Pull Requests and Trunk-Based Development. - How to safely review, manage, and integrate code generated by AI assistants like GitHub Copilot.\nGit is a software for version control. Version control is absolutely essential in software engineering, or when setting up a RAP. If you don‚Äôt install a version control system such as Git, don‚Äôt even start trying to set up a RAP. But what does a version control system like Git actually do? The basic workflow of Git is as follows: you start by setting up a repository for a project. On your computer, this is nothing more than a folder with your scripts in it. However, if you‚Äôre using Git to keep track of what‚Äôs inside that folder, there will be a hidden .git folder with a bunch of files in it. You can forget about that folder, this is for Git‚Äôs own internal needs. What matters, is that when you make changes to your files, you can first commit these changes, and then push them back to a repository. Collaborators can copy this repository and synchronize their files saved on their computers with your changes. Your collaborators can then also work on the files, then commit and push the changes to the repository as well.\nYou can then pull back these changes onto your computer, add more code, commit, push, etc‚Ä¶ Git makes it easy to collaborate on projects either with other people, or with future you. It is possible to roll back to previous versions of your code base, you can create new branches of your project to test new features (without affecting the main branch of your code), collaborators can submit patches that you can review and merge, and and and‚Ä¶\nIn my experience, learning Git is one of the most difficult things there is for students. And this is because Git solves a complex problem, and there is no easy way to solve a complex problem. But I would however say that Git is not unnescessarily complex. So buckle up, because this chapter is not going to be easy.\nGit is incredibly powerful, and absolutely essential in our line of work, it is simply not possible to not know at least some basics of Git. And this is what we‚Äôre going to do, learn the basics, it‚Äôll keep us plenty busy already.\nBut for now, let‚Äôs pause for a brief moment and watch this video that explains in 2 minutes the general idea of Git.\nLet‚Äôs get started.\nYou might have heard of github.com: this is a website that allows programmers to set up repositories on which they can host their code. The way to interact with github.com is via Git; but there are many other website like github.com, such as gitlab.com and bitbucket.com.\nFor this course, you should create an account on github.com. This should be easy enough. Then you should install Git on your computer.\nAnother advantage of using GitHub is that, as students, you will have access to Copilot for free. We will be using Copilot as our LLM for pair programming throughout the rest of this course. Get GitHub education here.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Git</span>"
    ]
  },
  {
    "objectID": "03-git.html#installing-git",
    "href": "03-git.html#installing-git",
    "title": "2¬† Git",
    "section": "2.2 Installing Git",
    "text": "2.2 Installing Git\nInstalling Git is not hard; it installs like any piece of software on your computer. If you‚Äôre running a Linux distribution, chances are you already have Git installed. To check if it‚Äôs already installed on a Linux system, open a terminal and type which git. If a path gets returned, like usr/bin/git, congratulations, it‚Äôs installed, if the command returns nothing you‚Äôll have to install it. On Ubuntu, type sudo apt-get install git and just wait a bit. If you‚Äôre using macOS or Windows, you will need to install it manually. For Windows, download the installer from here, and for macOS from here; you‚Äôll see that there are several ways of installing it on macOS, if you‚Äôve never heard of homebrew or macports then install the binary package from here.\nIt would also be possible to install it with Nix, but because Git is also useful outside of development shells, it is better to have it installed at the level of your operating system.\nNext, configure git:\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"your.email@example.com\"",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Git</span>"
    ]
  },
  {
    "objectID": "03-git.html#setting-up-a-repo",
    "href": "03-git.html#setting-up-a-repo",
    "title": "2¬† Git",
    "section": "2.3 Setting up a repo",
    "text": "2.3 Setting up a repo\nOk so now that Git is installed, we can actually start using it. First, let‚Äôs start by creating a new repository on github.com. As I‚Äôve mentioned in the introductory paragraph, Git will allow you to interact with github.com, and you‚Äôll see in what ways soon enough. For now, login to your github.com account, and create a new repository by clicking on the ‚Äòplus‚Äô sign in the top right corner of your profile page and then choose ‚ÄòNew repository‚Äô:\n\n\n\n\n\n\n\n\n\n\n\nIn the next screen, choose a nice name for your repository and ignore the other options, they‚Äôre not important for now. Then click on ‚ÄòCreate repository‚Äô:\n\n\n\n\n\n\n\n\n\n\n\nOk, we‚Äôre almost done with the easy part. The next screen tells us we can start interacting with the repository. For this, we‚Äôre first going to click on ‚ÄòREADME‚Äô:\n\n\n\n\n\n\n\n\n\n\n\nThis will add a README file that we can also edit from github.com directly:\n\n\n\n\n\n\n\n\n\n\n\nAdd some lines to the file, and then click on ‚ÄòCommit new file‚Äô. You‚Äôll end up on the main page of your freshly created repository. We are now done with setting up the repository on github.com. We can now clone the repository onto our machines. For this, click on ‚ÄòCode‚Äô, then ‚ÄòSSH‚Äô and then on the copy icon:\n\n\n\n\n\n\n\n\n\n\n\nNow we‚Äôre going to work exclusively from the command line. While graphical interfaces for Git exist, learning the command line is essential because:\n\nMost servers run Linux and only provide command line access\nThe command line gives you access to all Git features\nUnderstanding the command line makes you more versatile as a developer\nMany advanced Git operations can only be done from the command line",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Git</span>"
    ]
  },
  {
    "objectID": "03-git.html#cloning-the-repository-onto-your-computer",
    "href": "03-git.html#cloning-the-repository-onto-your-computer",
    "title": "2¬† Git",
    "section": "2.4 Cloning the repository onto your computer",
    "text": "2.4 Cloning the repository onto your computer\nOpen your terminal (Linux/macOS) or WSL2 if on Windows. First, let‚Äôs navigate to where we want to store our repository. For example, let‚Äôs create a directory for our projects:\nmkdir ~/Documents/projects\ncd ~/Documents/projects\nNow let‚Äôs clone the repository. Use the SSH URL you copied from GitHub:\ngit clone git@github.com:yourusername/your-repo-name.git\nReplace yourusername and your-repo-name with your actual GitHub username and repository name.\nAfter cloning, navigate into the repository:\ncd your-repo-name\nls -la\nYou should see the files from your repository, including the README file you created, plus a hidden .git directory that contains Git‚Äôs internal files.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Git</span>"
    ]
  },
  {
    "objectID": "03-git.html#setting-up-ssh-authentication",
    "href": "03-git.html#setting-up-ssh-authentication",
    "title": "2¬† Git",
    "section": "2.5 Setting up SSH authentication",
    "text": "2.5 Setting up SSH authentication\nBefore we can push code from our computer to GitHub, we need a way to prove that we are who we say we are. While you can use a username and password (HTTPS), a more secure and professional method is to use SSH (Secure Shell) keys.\nThink of it this way: * HTTPS (Password): Like using a password to unlock a door. You have to type it in frequently. * SSH (Key): Like having a special key that unlocks the door automatically. You set it up once, and it grants you access without needing to re-enter a password.\nWe will create a pair of digital keys: a public key that we will give to GitHub, and a private key that will stay on our computer. When we try to connect, GitHub will use our public key to check if we have the matching private key, proving our identity.\nLet‚Äôs generate our SSH key pair. We‚Äôll use the modern and highly secure Ed25519 algorithm. Open your terminal (or WSL2 on Windows) and run the following command, replacing the email with the one you used for GitHub:\nssh-keygen -t ed25519 -C \"your_email@example.com\"\nYou will be prompted with a few questions. Here is what you‚Äôll see and how to answer:\n# Press Enter to accept the default file location\n&gt; Enter a file in which to save the key (/home/your_username/.ssh/id_ed25519): [Press Enter]\n\n# You can optionally set a passphrase.\n&gt; Enter passphrase (empty for no passphrase): [Press Enter]\n&gt; Enter same passphrase again: [Press Enter]\nWhat about the passphrase? A passphrase adds an extra layer of security. If someone were to steal your computer, they still couldn‚Äôt use your SSH key without knowing the passphrase. However, you would have to type it every time you interact with GitHub. For this course, it is fine to leave it empty for convenience by simply pressing Enter.\nAfter running the command, two files have been created in a hidden directory in your home folder called .ssh: 1. id_ed25519: This is your private key. NEVER share this file with anyone or upload it anywhere. It must remain secret on your computer. 2. id_ed25519.pub: This is your public key. The .pub stands for ‚Äúpublic‚Äù. This is the key you can safely share and will upload to GitHub in the next step.\n\nNote for Older Systems: If the ssh-keygen command gives an error about ed25519 being an ‚Äúinvalid option‚Äù, your system might be too old to support it. In that rare case, you can use the older RSA algorithm instead: ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"\n\nNow that we have our key pair, our next task is to give the public key to GitHub. Let‚Äôs display the public key:\ncat ~/.ssh/id_rsa.pub\nCopy the entire output (starting with ssh-rsa and ending with your email).\nGo to GitHub.com, click on your profile picture, then Settings ‚Üí SSH and GPG keys ‚Üí New SSH key. Paste your public key and give it a descriptive title.\nLet‚Äôs test the connection:\nssh -T git@github.com\nYou should see a message confirming successful authentication.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Git</span>"
    ]
  },
  {
    "objectID": "03-git.html#your-first-commit",
    "href": "03-git.html#your-first-commit",
    "title": "2¬† Git",
    "section": "2.6 Your first commit",
    "text": "2.6 Your first commit\nLet‚Äôs create a simple script and add some code to it (in what follows, all the code is going to get written into files using the command line, but you can also use your text editor to do it):\necho 'print(\"Hello, Git!\")' &gt; hello.py\nOr create a more complex example:\ncat &gt; analysis.R &lt;&lt; 'EOF'\n# Load data\ndata(mtcars)\n\n# Create a simple plot\nplot(mtcars$mpg, mtcars$hp,\n     xlab = \"Miles per Gallon\",\n     ylab = \"Horsepower\",\n     main = \"MPG vs Horsepower\")\nEOF\nNow let‚Äôs check the status of our repository:\ngit status\nYou‚Äôll see that Git has detected new untracked files. Let‚Äôs add them to the staging area:\ngit add .\nThe . adds all files in the current directory. You can also add specific files:\ngit add analysis.R\nLet‚Äôs check the differences before committing:\ngit diff --staged\nThis shows what changes are staged for commit. Now let‚Äôs commit with a descriptive message:\ngit commit -m \"Add initial analysis script with basic plot\"\nLet‚Äôs check our commit history:\ngit log --oneline\nFinally, push our changes to GitHub:\ngit push origin main",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Git</span>"
    ]
  },
  {
    "objectID": "03-git.html#understanding-git-workflow-commands",
    "href": "03-git.html#understanding-git-workflow-commands",
    "title": "2¬† Git",
    "section": "2.7 Understanding Git workflow commands",
    "text": "2.7 Understanding Git workflow commands\nHere are the essential Git commands you‚Äôll use daily:\nChecking status and differences:\ngit status              # Show working directory status\ngit diff                # Show unstaged changes\ngit diff --staged       # Show staged changes\ngit diff HEAD~1         # Compare with previous commit\nAdding and committing:\ngit add filename        # Stage specific file\ngit add .               # Stage all changes\ngit commit -m \"message\" # Commit with message\ngit commit -am \"msg\"    # Add and commit tracked files\nWorking with remote repositories:\ngit push origin main    # Push to main branch\ngit pull origin main    # Pull latest changes\ngit fetch               # Download changes without merging\nViewing history:\ngit log                 # Show detailed commit history\ngit log --oneline       # Show abbreviated history\ngit log --graph         # Show branching history\ngit show commit-hash    # Show specific commit details",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Git</span>"
    ]
  },
  {
    "objectID": "03-git.html#working-with-commit-history",
    "href": "03-git.html#working-with-commit-history",
    "title": "2¬† Git",
    "section": "2.8 Working with commit history",
    "text": "2.8 Working with commit history\nLet‚Äôs explore how to work with previous versions. First, let‚Äôs make another change:\necho '# This is a new line' &gt;&gt; analysis.R\ngit add analysis.R\ngit commit -m \"Add comment to analysis script\"\nView the commit history:\ngit log --oneline\nTo view a previous version without changing anything:\ngit checkout &lt;commit-hash&gt;\ncat analysis.R  # View the file at that point in time\nYou‚Äôll be in ‚Äúdetached HEAD‚Äù state. To return to the latest version:\ngit checkout main\nTo permanently revert a commit (creates a new commit that undoes changes):\ngit revert &lt;commit-hash&gt;",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Git</span>"
    ]
  },
  {
    "objectID": "03-git.html#collaborating",
    "href": "03-git.html#collaborating",
    "title": "2¬† Git",
    "section": "2.9 Collaborating",
    "text": "2.9 Collaborating\nLet‚Äôs set up collaboration. Have a colleague invite you to their repository, or invite someone to yours. On GitHub, go to Settings ‚Üí Manage access ‚Üí Invite a collaborator.\nOnce you‚Äôre both collaborators, try this workflow:\n\nBoth of you clone the repository\nOne person makes changes and pushes:\n\necho 'library(ggplot2)' &gt; new_analysis.R\ngit add new_analysis.R\ngit commit -m \"Add ggplot2 analysis\"\ngit push origin main\n\nThe other person attempts to push their own changes:\n\necho 'data(iris)' &gt; iris_analysis.R\ngit add iris_analysis.R\ngit commit -m \"Add iris analysis\"\ngit push origin main  # This will fail!\n\n2.9.1 Handling conflicts\nYou‚Äôll get an error like ! [rejected] main -&gt; main (non-fast-forward). This sounds scary, but it‚Äôs Git‚Äôs safe way of telling you: ‚ÄúThe remote repository on GitHub has changes that you don‚Äôt have on your computer. I‚Äôm stopping you from pushing because you would overwrite those changes.‚Äù\nTo solve this, you must first pull the changes from the remote repository and combine them with your local work. Git gives you two primary ways to do this: merging and rebasing.\n\n2.9.1.1 Strategy 1: Merging (The Default)\nIf you just run git pull, Git will perform a merge. It looks at the remote changes and your local changes and creates a new, special ‚Äúmerge commit‚Äù to tie the two histories together.\nImagine the history looks like this: * Your colleague pushed commit D. * You worked locally and created commit C.\n      C (Your local work)\n     /\nA---B ---D (Remote work on GitHub)\nA git pull (which is git fetch + git merge) will result in this:\n      C-------E (Merge commit)\n     /       /\nA---B-------D\nThe history is now non-linear. While this accurately records that two lines of work were merged, it can clutter up the project history with many ‚ÄúMerge branch ‚Äòmain‚Äô‚Ä¶‚Äù commits, making it harder to read.\n\n\n2.9.1.2 Strategy 2: Rebasing (The Cleaner Way)\nThe second strategy is to rebase. Rebasing does something clever. It says: ‚ÄúLet me temporarily put your local changes aside. I‚Äôll download the latest remote changes first. Then, I‚Äôll take your changes and re-apply them one-by-one on top of the new remote history.‚Äù\nUsing the same scenario: * Start: C (Your local work)          /     A---B ---D (Remote work on GitHub)\n\nRunning git pull --rebase does this:\n\nIt ‚Äúunplugs‚Äù your commit C.\nIt fast-forwards your main branch to include D.\nIt then ‚Äúre-plays‚Äù your commit C on top of D, creating a new commit C'.\n\nThe final result is a clean, single, linear history:\nA---B---D---C' (Your work is now on top)\n\nYour project‚Äôs history now looks like you did your work after your colleague, even if you did it at the same time. This makes the log much easier to read and understand.\nFor its clean, linear history, rebasing is the preferred method in many professional workflows, and it‚Äôs the one we will use.\nNow, let‚Äôs do it. To pull the remote changes and place your local commits on top, run:\ngit pull --rebase origin main\nIf there are no conflicts, Git will automatically complete the rebase. Your local work will now be neatly stacked on top of the remote changes, and your git push will succeed.\nIf there are conflicts, Git will pause the rebase process and tell you which files have conflicts. This happens when you and a collaborator changed the same lines in the same file.\ngit status  # Shows \"You are currently rebasing.\" and lists conflicted files\nYour job is to be the surgeon. Open the conflicted files (e.g., analysis.R). You will see Git‚Äôs conflict markers:\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\n# This is my version of the code\ndata(iris)\n=======\n# This is their version from the server\ndata(mtcars)\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; a1b2c3d... Add mtcars analysis\nManually edit the file to resolve the conflict. You must delete the &lt;&lt;&lt;&lt;&lt;&lt;&lt;, =======, and &gt;&gt;&gt;&gt;&gt;&gt;&gt; markers and decide what the final, correct version of the code should be. For example:\n# I decided to keep both datasets for now\ndata(iris)\ndata(mtcars)\nOnce you have fixed the file and saved it, you need to tell Git you‚Äôre done:\n# Mark the conflict as resolved\ngit add conflicted-file.R\n\n# Continue the rebase process\ngit rebase --continue\nGit will continue applying your commits one by one. If you have another conflict, repeat the process. Once the rebase is complete, you can finally push your work.\nFinally, push your changes:\ngit push origin main\nThis time, it should succeed.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Git</span>"
    ]
  },
  {
    "objectID": "03-git.html#working-with-branches",
    "href": "03-git.html#working-with-branches",
    "title": "2¬† Git",
    "section": "2.10 Working with branches",
    "text": "2.10 Working with branches\nBranches allow you to work on features without affecting the main codebase:\n# Create and switch to a new branch\ngit checkout -b feature-new-plots\n\n# Or use the newer syntax\ngit switch -c feature-new-plots\nList all branches:\ngit branch\nWork on your feature:\necho 'boxplot(mtcars$mpg ~ mtcars$cyl)' &gt;&gt; analysis.R\ngit add analysis.R\ngit commit -m \"Add boxplot analysis\"\nPush the branch to GitHub:\ngit push origin feature-new-plots\nSwitch back to main and merge your feature:\ngit checkout main\ngit merge feature-new-plots\nIf you‚Äôre done with the branch, delete it:\ngit branch -d feature-new-plots             # Delete locally\ngit push origin --delete feature-new-plots  # Delete on GitHub",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Git</span>"
    ]
  },
  {
    "objectID": "03-git.html#advanced-workflow-with-branches",
    "href": "03-git.html#advanced-workflow-with-branches",
    "title": "2¬† Git",
    "section": "2.11 Advanced workflow with branches",
    "text": "2.11 Advanced workflow with branches\nFor more complex workflows, you might want to keep branches separate and use pull requests on GitHub instead of direct merging:\n# Create feature branch\ngit checkout -b feature-advanced-stats\necho 'summary(lm(mpg ~ hp + wt, data = mtcars))' &gt;&gt; analysis.R\ngit add analysis.R\ngit commit -m \"Add linear regression analysis\"\ngit push origin feature-advanced-stats\nThen go to GitHub and create a Pull Request from the web interface. This allows for code review before merging.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Git</span>"
    ]
  },
  {
    "objectID": "03-git.html#essential-daily-workflow",
    "href": "03-git.html#essential-daily-workflow",
    "title": "2¬† Git",
    "section": "2.12 Essential daily workflow",
    "text": "2.12 Essential daily workflow\nHere‚Äôs the typical daily workflow:\n\nStart your day: Pull latest changes\n\ngit pull origin main\n\nCreate a feature branch:\n\ngit checkout -b feature-description\n\nWork and commit frequently:\n\n# Make changes\ngit add .\ngit commit -m \"Descriptive commit message\"\n\nPush your branch:\n\ngit push origin feature-description\n\nWhen feature is complete: Merge or create pull request\n\ngit checkout main\ngit pull origin main  # Get latest changes\ngit merge feature-description\ngit push origin main\n\n2.12.1 A Better Way to Collaborate: Trunk-Based Development\nThe ‚ÄúEssential Daily Workflow‚Äù you just learned is a great start, but it leaves one important question unanswered: how long should a feature branch live? Days? Weeks? Months?\nA common mistake for new teams is to let branches live for a very long time. A data scientist might create a branch called feature-big-analysis, work on it for three weeks, and then try to merge it back into main. The result is often what‚Äôs called ‚Äúmerge hell‚Äù: main has changed so much in three weeks that merging the branch back in creates dozens of conflicts and is a painful, stressful process.\nTo avoid this, many professional teams use a workflow called Trunk-Based Development (TBD). The philosophy is simple but powerful:\n\nAll developers integrate their work back into the main branch (the ‚Äútrunk‚Äù) as frequently as possible‚Äîat least once a day.\n\nThis means that feature branches are incredibly short-lived. Instead of a single, massive feature branch that takes weeks, you create many tiny branches that each take a few hours or a day at most.\nThe goal is to keep the main branch constantly updated with the latest code from everyone on the team. This has huge benefits: * Fewer Merge Conflicts: Because you are merging small changes frequently, the chance of conflicting with a teammate‚Äôs work is dramatically lower. * Easier Code Reviews: Reviewing a small change that adds one function is much easier and faster than reviewing a 1,000-line change that refactors an entire analysis. * Continuous Integration: Everyone is working from the most up-to-date version of the project, which reduces integration problems and keeps the project moving forward.\n\n2.12.1.1 How to Work with Short-Lived Branches\nBut how can you merge something back into main if the feature isn‚Äôt finished? The main branch must always be stable and runnable. You can‚Äôt merge broken code.\nHere are two key strategies to make TBD work in practice.\n\n2.12.1.1.1 Strategy 1: Feature Flags (or Toggles)\nA feature flag is just a simple variable (like a TRUE/FALSE switch) that lets you turn a new, unfinished part of the code on or off. This allows you to merge the code into main while keeping it ‚Äúoff‚Äù until it‚Äôs ready.\nImagine you are adding a new, complex plot to analysis.R, but it will take a few days to get right.\n# At the top of your analysis.R script\n# --- Configuration ---\nuse_new_scatterplot &lt;- FALSE # Set to FALSE while in development\n\n# ... lots of existing, working code ...\n\n# --- New Feature Code ---\nif (use_new_scatterplot) {\n  # All your new, unfinished, possibly-buggy plotting code goes here.\n  # It won't run as long as the flag is FALSE.\n  library(scatterplot3d)\n  scatterplot3d(mtcars$mpg, mtcars$hp, mtcars$wt)\n}\nWith this if block, you can safely merge your changes into main. The new code is there, but it won‚Äôt execute and won‚Äôt break the existing analysis. Other developers can pull your changes and won‚Äôt even notice. Once you‚Äôve finished the feature in subsequent small commits, the final change is just to flip the switch: use_new_scatterplot &lt;- TRUE.\n\n\n2.12.1.1.2 Strategy 2: Stacked Pull Requests\nSometimes, a feature is too big for one small change, but it can be broken down into a logical sequence of steps. For example, to add a new analysis, you might need to: 1. Add a new data cleaning function. 2. Use that function to process the data. 3. Generate a new plot from the processed data.\nInstead of putting all this in one giant Pull Request (PR), you can ‚Äústack‚Äù them. A stacked PR is a PR that is based on another PR branch, not on main.\nHere‚Äôs the workflow: 1. Create the first branch from main for the first step. bash     git switch -c add-cleaning-function     # ...do the work, commit, and push... Create a Pull Request on GitHub for this branch (add-cleaning-function -&gt; main).\n\nCreate the second branch from the first branch. This is the key step. bash     git switch -c process-the-data     # ...do the work that DEPENDS on the cleaning function...\nCreate a new PR for this branch. On GitHub, when you create the PR, manually change the base branch from main to add-cleaning-function. Now this PR only shows the changes for step 2.\n\nYour team can now review and approve add-cleaning-function first. Once it‚Äôs merged into main, you go to your process-the-data PR on GitHub and change its base back to main. It will now be ready to merge after a quick update.\nThis approach breaks down large features into small, logical, reviewable chunks, keeping your development velocity high while adhering to the TBD philosophy.\nBy embracing short-lived branches, feature flags, and stacked PRs, you can make collaboration smoother, less stressful, and far more productive.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Git</span>"
    ]
  },
  {
    "objectID": "03-git.html#contributing-to-someone-elses-repository",
    "href": "03-git.html#contributing-to-someone-elses-repository",
    "title": "2¬† Git",
    "section": "2.13 Contributing to someone else‚Äôs repository",
    "text": "2.13 Contributing to someone else‚Äôs repository\nTo contribute to repositories you don‚Äôt have write access to:\n\nFork the repository on GitHub (click the Fork button)\nClone your fork:\n\ngit clone git@github.com:yourusername/original-repo-name.git\ncd original-repo-name\n\nAdd the original repository as upstream:\n\ngit remote add upstream git@github.com:originalowner/original-repo-name.git\n\nCreate a feature branch:\n\ngit checkout -b fix-issue-123\n\nMake changes and commit:\n\n# Make your changes\ngit add .\ngit commit -m \"Fix issue #123: describe what you fixed\"\n\nPush to your fork:\n\ngit push origin fix-issue-123\n\nCreate a Pull Request on GitHub from your fork to the original repository\n\nThis workflow is fundamental for contributing to open source projects and collaborating in professional environments.\nThe command line approach to Git gives you complete control and understanding of the version control process, making you a more effective developer and collaborator.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Git</span>"
    ]
  },
  {
    "objectID": "03-git.html#working-with-llms-and-git-managing-ai-generated-changes",
    "href": "03-git.html#working-with-llms-and-git-managing-ai-generated-changes",
    "title": "2¬† Git",
    "section": "2.14 Working with LLMs and Git: Managing AI-Generated Changes",
    "text": "2.14 Working with LLMs and Git: Managing AI-Generated Changes\nWhen working with Large Language Models (LLMs) like GitHub Copilot, ChatGPT, or Claude to generate or modify code, it‚Äôs crucial to review changes carefully before committing them. Git provides excellent tools for examining and selectively accepting or rejecting AI-generated modifications.\n\n2.14.1 The LLM workflow with Git\nHere‚Äôs a recommended workflow when using LLMs to modify your code:\n\nAlways commit your working code first:\n\ngit add .\ngit commit -m \"Working state before LLM modifications\"\n\nApply LLM suggestions to your files (copy-paste, or use tools that directly modify files)\nReview changes chunk by chunk using Git‚Äôs tools\nSelectively accept or reject changes\nCommit accepted changes with descriptive messages\n\n\n\n2.14.2 Examining LLM changes\nAfter an LLM has modified your files, use Git to see exactly what changed:\n# See all modified files\ngit status\n\n# See all changes at once\ngit diff\n\n# See changes in a specific file\ngit diff analysis.R\n\n# See changes with more context (10 lines before/after)\ngit diff -U10 analysis.R\nFor a more visual review, you can use Git‚Äôs word-level diff:\n# Show word-by-word changes instead of line-by-line\ngit diff --word-diff analysis.R\n\n# Show character-level changes\ngit diff --word-diff=color --word-diff-regex=.\n\n\n2.14.3 Interactive staging: Accepting changes chunk by chunk\nGit‚Äôs interactive staging feature (git add -p) is perfect for reviewing LLM changes. It lets you review each ‚Äúhunk‚Äù (chunk of changes) individually:\ngit add -p\nThis will show you each chunk of changes and prompt you with options: - y - stage this hunk - n - do not stage this hunk - q - quit; do not stage this hunk or any remaining ones - a - stage this hunk and all later hunks in the file - d - do not stage this hunk or any later hunks in the file - s - split the current hunk into smaller hunks - e - manually edit the current hunk - ? - print help\n\n\n2.14.4 Example: Reviewing LLM changes to an R script\nLet‚Äôs say an LLM modified your analysis.R file. Here‚Äôs how to review it:\n# First, see what files were modified\ngit status\n\n# Review the changes\ngit diff analysis.R\nYou might see output like:\n@@ -1,8 +1,12 @@\n # Load required libraries\n-library(ggplot2)\n+library(ggplot2)\n+library(dplyr)\n+library(tidyr)\n \n # Load data\n data(mtcars)\n+mtcars &lt;- mtcars %&gt;% \n+  mutate(efficiency = ifelse(mpg &gt; 20, \"High\", \"Low\"))\n \n-# Create a simple plot\n-plot(mtcars$mpg, mtcars$hp)\n+# Create an improved plot with ggplot2\n+ggplot(mtcars, aes(x = mpg, y = hp, color = efficiency)) +\n+  geom_point(size = 3) +\n+  theme_minimal()\nNow use interactive staging to review each change:\ngit add -p analysis.R\nGit will show you each hunk and ask what to do. For example:\n@@ -1,2 +1,4 @@\n # Load required libraries\n library(ggplot2)\n+library(dplyr)\n+library(tidyr)\nStage this hunk [y,n,q,a,d,s,e,?]?\nYou might decide: - y if you want the additional libraries - n if you think they‚Äôre unnecessary - s to split this into smaller chunks if you want only one library\n\n\n2.14.5 Advanced chunk management\nSometimes hunks are too large. Use s to split them:\n# When prompted with a large hunk\nStage this hunk [y,n,q,a,d,s,e,?]? s\nIf Git can‚Äôt split automatically, use e to manually edit:\nStage this hunk [y,n,q,a,d,s,e,?]? e\nThis opens your editor where you can: - Remove lines you don‚Äôt want (delete the entire line) - Keep lines by leaving them as-is - Lines starting with + are additions - Lines starting with - are deletions - Lines starting with  (space) are context\n\n\n2.14.6 Creating meaningful commits after LLM review\nAfter selectively staging changes, commit with descriptive messages:\n# Commit the staged changes\ngit commit -m \"Add dplyr and efficiency categorization\n\n- Added dplyr for data manipulation\n- Created efficiency category based on mpg &gt; 20\n- LLM suggested changes reviewed and approved\"\n\n# If there are remaining unstaged changes you want to reject\ngit checkout -- analysis.R  # Revert unstaged changes\n\n\n2.14.7 Working with multiple files modified by LLM\nWhen an LLM modifies multiple files, review them systematically:\n# See all changed files\ngit status\n\n# Review each file individually\ngit diff analysis.R\ngit diff data_processing.R\ngit diff visualization.R\n\n# Use interactive staging for each file\ngit add -p analysis.R\ngit add -p data_processing.R\n# ... etc\nOr stage all changes interactively at once:\ngit add -p\n\n\n2.14.8 Handling LLM-generated new files\nWhen an LLM creates entirely new files:\n# See new files\ngit status\n\n# Review new file content\ncat new_functions.R\n\n# Add if you approve\ngit add new_functions.R\n\n# Or ignore if you don't want it\necho \"new_functions.R\" &gt;&gt; .gitignore\n\n\n2.14.9 Using Git to compare LLM suggestions\nCreate a branch to safely experiment with LLM suggestions:\n# Create a branch for LLM experiments\ngit checkout -b llm-suggestions\n\n# Apply LLM changes\n# ... make modifications ...\n\n# Commit the LLM suggestions\ngit add .\ngit commit -m \"LLM suggestions for code improvement\"\n\n# Compare with original\ngit diff main..llm-suggestions\n\n# If you like some but not all changes, cherry-pick specific commits\ngit checkout main\ngit cherry-pick --no-commit &lt;commit-hash&gt;\ngit add -p  # Selectively stage parts of the cherry-picked changes\ngit commit -m \"Selected improvements from LLM suggestions\"\n\n\n2.14.10 Best practices for LLM + Git workflow\n\nAlways commit working code before applying LLM suggestions\nNever blindly accept all LLM changes - review each modification\nUse descriptive commit messages that mention LLM involvement\nTest code after accepting LLM suggestions before final commit\nKeep LLM-generated changes in separate commits for easier tracking\nUse branches for experimental LLM suggestions\nDocument why you accepted or rejected specific suggestions\n\n\n\n2.14.11 Example complete workflow\n# 1. Save current working state\ngit add .\ngit commit -m \"Working analysis script before LLM optimization\"\n\n# 2. Apply LLM suggestions (manually copy-paste or use tools)\n# ... LLM modifies your files ...\n\n# 3. Review all changes\ngit status\ngit diff\n\n# 4. Interactively stage only the changes you want\ngit add -p\n\n# 5. Commit approved changes\ngit commit -m \"LLM improvements: added data validation and error handling\n\nReviewed and approved:\n- Input validation for data loading\n- Error handling for missing values\n- Improved variable naming\n\nRejected:\n- Overly complex optimization that hurt readability\"\n\n# 6. Discard remaining unwanted changes\ngit checkout .\n\n# 7. Test the code\nRscript analysis.R  # or python script.py\n\n# 8. Push if everything works\ngit push origin main\nThis workflow ensures you maintain full control over your codebase while benefiting from LLM assistance, with complete traceability of what changes were made and why.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Git</span>"
    ]
  },
  {
    "objectID": "04-functional-programming.html",
    "href": "04-functional-programming.html",
    "title": "3¬† Functional Programming: The Cornerstone of Reproducible Analysis",
    "section": "",
    "text": "3.1 Introduction: From Scripts to Functions\nWhat you‚Äôll learn by the end of this chapter: * Why functional programming is crucial for reproducible, testable, and collaborative data science. * How to write self-contained, ‚Äúpure‚Äù functions in both R and Python. * How to use functional concepts like map, filter, and reduce to replace error-prone loops. * How writing functions makes your code easier to review, debug, and even generate with LLMs.\nSo far, we‚Äôve established two pillars of reproducible data science: 1. Reproducible Environments (with Nix): Ensuring everyone has the exact same tools (R, Python, system libraries) to run the code. 2. Reproducible History (with Git): Ensuring everyone has the exact same version of the code and can collaborate effectively.\nNow we turn to the third and arguably most important pillar: writing reproducible code itself. A common way to start a data analysis is by writing a script: a sequence of commands that are executed from top to bottom.\nThis works, but it has a hidden, dangerous property: state. The script relies on variables like heavy_cars existing in the environment, making the code hard to reason about, debug, and test. If scripting with state is a crack in the foundation of reproducibility, then using computational notebooks is a gaping hole.\nNotebooks like Jupyter introduce an even more insidious form of state: the cell execution order. You can execute cells out of order, meaning the visual layout of your code has no relation to how it actually ran. This is a recipe for non-reproducible results and a primary cause of the ‚Äúit worked yesterday, why is it broken today?‚Äù problem.\nThe solution to this chaos is to embrace a paradigm that minimizes state: Functional Programming (FP). Instead of a linear script, we structure our code as a collection of self-contained, predictable functions. To support this, we will work exclusively in plain text files (.R, .py), which enforce a predictable, top-to-bottom execution, and use literate programming (using Quarto). The power of FP comes from the concept of purity, borrowed from mathematics. A mathematical function has a beautiful property: for a given input, it always returns the same output. sqrt(4) is always 2. Its result doesn‚Äôt depend on what you calculated before or on a random internet connection. Our Nix environments handle the ‚Äúright library‚Äù problem; purity handles the ‚Äúright logic‚Äù problem. Our goal is to write our analysis code with this same level of rock-solid predictability.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Functional Programming: The Cornerstone of Reproducible Analysis</span>"
    ]
  },
  {
    "objectID": "04-functional-programming.html#introduction-from-scripts-to-functions",
    "href": "04-functional-programming.html#introduction-from-scripts-to-functions",
    "title": "3¬† Functional Programming: The Cornerstone of Reproducible Analysis",
    "section": "",
    "text": "# R script example\nlibrary(dplyr)\ndata(mtcars)\nheavy_cars &lt;- filter(mtcars, wt &gt; 4)\nmean_mpg_heavy &lt;- mean(heavy_cars$mpg)\nprint(mean_mpg_heavy)\n# Python script example\nimport pandas as pd\nmtcars = pd.read_csv(\"mtcars.csv\") # Assume the file exists\nheavy_cars = mtcars[mtcars['wt'] &gt; 4]\nmean_mpg_heavy = heavy_cars['mpg'].mean()\nprint(mean_mpg_heavy)\n\n\n\n\n3.1.1 Why Does This Matter for Data Science?\nAdopting a functional style brings massive benefits that directly connect to our previous chapters:\n\nUnit Testing is Now Possible: You can‚Äôt easily test a 200-line script. But you can easily test a small function that does one thing. Does calculate_mean_mpg(data) return the correct value for a sample dataset? This makes your code more reliable.\nCode Review is Easier (Git Workflow): As we saw in the Git chapter, reviewing a small, self-contained change is much easier than reviewing a giant, sprawling one. A Pull Request that just adds or modifies a single function is simple for your collaborators to understand and approve.\nWorking with LLMs is More Effective: It‚Äôs difficult to ask an LLM to ‚Äúfix my 500-line analysis script.‚Äù It‚Äôs incredibly effective to ask, ‚ÄúWrite a Python function that takes a pandas DataFrame and a column name, and returns the mean of that column, handling missing values. Also, write three pytest unit tests for it.‚Äù Functions provide the clear boundaries and contracts that LLMs excel at working with.\nReadability and Maintainability: Well-named functions are self-documenting. starwars %&gt;% group_by(species) %&gt;% summarize(mean_height     = mean(height)) is instantly understandable. The equivalent for loop is a puzzle you have to solve.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Functional Programming: The Cornerstone of Reproducible Analysis</span>"
    ]
  },
  {
    "objectID": "04-functional-programming.html#purity-and-side-effects",
    "href": "04-functional-programming.html#purity-and-side-effects",
    "title": "3¬† Functional Programming: The Cornerstone of Reproducible Analysis",
    "section": "3.2 Purity and Side Effects",
    "text": "3.2 Purity and Side Effects\nA pure function has two rules: 1. It only depends on its inputs. It doesn‚Äôt use any ‚Äúglobal‚Äù variables defined outside the function. 2. It doesn‚Äôt change anything outside of its own scope. It doesn‚Äôt modify a global variable or write a file to disk. This is called having ‚Äúno side effects.‚Äù\nConsider this ‚Äúimpure‚Äù function in Python:\n# IMPURE: Relies on a global variable\ndiscount_rate = 0.10\n\ndef calculate_discounted_price(price):\n  return price * (1 - discount_rate) # What if discount_rate changes?\n\nprint(calculate_discounted_price(100))\n# &gt; 90.0\ndiscount_rate = 0.20 # Someone changes the state\nprint(calculate_discounted_price(100))\n# &gt; 80.0  -- Same input, different output!\nThe pure version passes all its dependencies as arguments:\n# PURE: All inputs are explicit arguments\ndef calculate_discounted_price_pure(price, rate):\n  return price * (1 - rate)\n\nprint(calculate_discounted_price_pure(100, 0.10))\n# &gt; 90.0\nprint(calculate_discounted_price_pure(100, 0.20))\n# &gt; 80.0\nNow the function is predictable and self-contained.\n\n3.2.1 Handling ‚ÄúImpure‚Äù Operations like Randomness\nSome operations, like generating random numbers, are inherently impure. Each time you run rnorm(10) or numpy.random.rand(10), you get a different result.\nThe functional approach is not to avoid this, but to control it by making the source of impurity (the random seed) an explicit input.\nIn R, the {withr} package helps create a temporary, controlled context:\nlibrary(withr)\n\n# This function is now pure! For a given seed, the output is always the same.\npure_rnorm &lt;- function(n, seed) {\n  with_seed(seed, {\n    rnorm(n)\n  })\n}\n\npure_rnorm(n = 5, seed = 123)\npure_rnorm(n = 5, seed = 123)\nIn Python, numpy provides a more modern, object-oriented way to handle this, which is naturally functional:\nimport numpy as np\n\n# Create a random number generator instance with a seed\nrng = np.random.default_rng(seed=123)\n\n# Now, calls on this 'rng' object are deterministic within its context\nprint(rng.standard_normal(5))\n\n# If we re-create the same generator, we get the same numbers\nrng2 = np.random.default_rng(seed=123)\nprint(rng2.standard_normal(5))\nThe key is the same: the ‚Äústate‚Äù (the seed) is explicitly managed, not hidden globally.\nHowever, this introduces a concept from another programming paradigm: Object-Oriented Programming (OOP). The rng variable is not just a value; it‚Äôs an object that bundles together data (its internal seed state) and methods that operate on that data (.standard_normal()). This is called encapsulation. This is a double-edged sword for reproducibility. On one hand, it‚Äôs a huge improvement over hidden global state. On the other, the rng object itself is now a stateful entity. If we called rng.standard_normal(5) a second time, it would produce different numbers because its internal state would have been mutated by the first call.\nIn a purely functional world, we would avoid creating such stateful objects. However, in the pragmatic world of Python data science, this is often unavoidable. Core libraries like pandas, scikit-learn, and matplotlib are fundamentally object-oriented. You create DataFrame objects, model objects, and plot objects, all of which encapsulate state. Our guiding principle, therefore, must be one of careful management: use functions for the flow and logic of your analysis, and treat objects from libraries as values that are passed between these functions. Avoid building your own complex classes with hidden state for your data pipeline. A pipeline composed of functions (df2 = clean_data(df1); df3 = analyze_data(df2)) is almost always more transparent and reproducible than an object-oriented one (pipeline.load(); pipeline.clean(); pipeline.analyze()).\n\n\n3.2.2 Can We Make This Truly Pure?\nThis naturally raises this next question: can we force this numpy example to be truly pure? A pure function cannot have side effects, which means it cannot mutate the rng object‚Äôs internal state. To achieve this, our function must take the generator‚Äôs current state as an explicit input and return a tuple containing both the desired random numbers and the new, updated state of the generator.\nLet‚Äôs write a wrapper function that does exactly this:\nimport numpy as np\n\ndef pure_standard_normal(generator_state, n_samples):\n    \"\"\"\n    A pure function to generate standard normal random numbers.\n\n    Args:\n        generator_state: The current state of a numpy BitGenerator.\n        n_samples: The number of samples to generate.\n\n    Returns:\n        A tuple containing (random_numbers, new_generator_state).\n    \"\"\"\n    # 1. Create a temporary generator instance from the input state\n    temp_rng = np.random.Generator(np.random.PCG64(generator_state))\n\n    # 2. Generate the numbers (this mutates the *temporary* generator)\n    numbers = temp_rng.standard_normal(n_samples)\n\n    # 3. Extract the new state from the temporary generator\n    new_state = temp_rng.bit_generator.state\n\n    # 4. Return both the result and the new state\n    return (numbers, new_state)\n\n# --- How to use this pure function ---\n\n# 1. Get an initial state from a seed\ninitial_state = np.random.PCG64(123).state\n\n# 2. First call: provide the state, get back numbers and a *new* state\nfirst_numbers, state_after_first_call = pure_standard_normal(initial_state, 5)\nprint(\"First call results:\", first_numbers)\n\n# 3. Second call: MUST use the new state from the previous call\nsecond_numbers, state_after_second_call = pure_standard_normal(state_after_first_call, 5)\nprint(\"Second call results:\", second_numbers)\n\n# Proof of purity: If we re-use the initial state, we get the exact same \"first\" result\nproof_numbers, _ = pure_standard_normal(initial_state, 5)\nprint(\"Proof call results:\", proof_numbers)\nAs you can see, this is now 100% pure and predictable. The function pure_standard_normal will always produce the same output tuple for the same input tuple.\n\n3.2.2.1 Is This Feasible in Practice?\nWhile this is a powerful demonstration of functional principles, it is often not practical for day-to-day data science in Python. Manually passing the state variable from one function to the next throughout an entire analysis script (state_1, state_2, state_3‚Ä¶) would be extremely verbose and cumbersome.\nThe key takeaway is understanding the trade-off. The object-oriented approach (rng = np.random.default_rng(seed=123)) is a pragmatic compromise. It encapsulates the state in a predictable way, which is a vast improvement over hidden global state, even if it‚Äôs not technically ‚Äúpure‚Äù. If you have to use Python: treat stateful library objects like rng as values that are created once with a fixed seed and passed into your pure analysis functions. This gives you 99% of the benefit of reproducibility with a fraction of the complexity.\nThis difference in the ‚Äúfeel‚Äù of functional composition between R‚Äôs pipe and Python‚Äôs method chaining is no accident; it reflects the deep-seated design philosophies of each language. This context is crucial for understanding why certain patterns feel more ‚Äúnatural‚Äù in each environment. R‚Äôs lineage traces back to the S language, which was itself heavily influenced by Scheme, a dialect of Lisp and a bastion of functional programming. Consequently, treating data operations as a series of function transformations is baked into R‚Äôs DNA. The entire Tidyverse ecosystem, with its ubiquitous pipe, is a modern implementation of this functional heritage.\nPython, in contrast, was designed with a different set of priorities, famously summarized in its Zen: ‚ÄúThere should be one‚Äîand preferably only one‚Äîobvious way to do it.‚Äù Its creator, Guido van Rossum, historically argued that explicit for loops and list comprehensions were more readable and ‚ÄúPythonic‚Äù than functional constructs like map and lambda. He was so committed to this principle of one clear path that he even proposed removing these functions from the language entirely at one point.\nR is fundamentally a functional language that has acquired object-oriented features, while Python is a quintessential object-oriented language with powerful functional capabilities. Recognizing this history helps explain why a chain of functions feels native in R, while method chaining on objects is the default in pandas and polars. My goal in this course is for you to master the functional paradigm so you can apply it effectively in either language, leveraging the native strengths of each.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Functional Programming: The Cornerstone of Reproducible Analysis</span>"
    ]
  },
  {
    "objectID": "04-functional-programming.html#writing-your-own-functions",
    "href": "04-functional-programming.html#writing-your-own-functions",
    "title": "3¬† Functional Programming: The Cornerstone of Reproducible Analysis",
    "section": "3.3 Writing Your Own Functions",
    "text": "3.3 Writing Your Own Functions\nLet‚Äôs learn the syntax. The goal is always to encapsulate a single, logical piece of work.\n\n3.3.0.1 In R\nR functions are first-class citizens. You can assign them to variables and pass them to other functions.\n# A simple function\ncalculate_ci &lt;- function(x, level = 0.95) {\n  # Calculate the mean and standard error\n  se &lt;- sd(x, na.rm = TRUE) / sqrt(length(x))\n  mean_val &lt;- mean(x, na.rm = TRUE)\n  \n  # Calculate the confidence interval bounds\n  alpha &lt;- 1 - level\n  lower &lt;- mean_val - qnorm(1 - alpha/2) * se\n  upper &lt;- mean_val + qnorm(1 - alpha/2) * se\n\n  # Return a named vector\n  # the `return()` statement is not needed at the end\n  # but can be useful for early returning a result\n  c(mean = mean_val, lower = lower, upper = upper)\n}\n\n# Use it\ndata &lt;- c(1.2, 1.5, 1.8, 1.3, 1.6, 1.7)\ncalculate_ci(data)\nFor data analysis, you‚Äôll often want to write functions that work with data frames and column names. The {dplyr} package uses a special technique called ‚Äútidy evaluation‚Äù for this.\nlibrary(dplyr)\n\n# A function that summarizes a column in a dataset\nsummarize_variable &lt;- function(dataset, var_to_summarize) {\n  dataset %&gt;%\n    summarise(\n      n = n(),\n      mean = mean({{ var_to_summarize }}, na.rm = TRUE),\n      sd = sd({{ var_to_summarize }}, na.rm = TRUE)\n    )\n}\n\n# The {{ }} (curly-curly) syntax tells dplyr to use the column name\n# passed into the function.\nstarwars %&gt;%\n  group_by(species) %&gt;%\n  summarize_variable(height)\nThis is incredibly powerful for creating reusable analysis snippets. To learn more, read about programming with {dplyr} here.\n\n\n3.3.0.2 In Python\nPython‚Äôs syntax is similar, using the def keyword. Type hints are a best practice for clarity.\nimport pandas as pd\nimport numpy as np\n\n# A function to summarize a column in a DataFrame\ndef summarize_variable_py(dataset: pd.DataFrame, var_to_summarize: str) -&gt; pd.DataFrame:\n  \"\"\"Calculates summary statistics for a given column.\"\"\"\n  summary = dataset.groupby('species').agg(\n      n=(var_to_summarize, 'size'),\n      mean=(var_to_summarize, 'mean'),\n      sd=(var_to_summarize, 'std')\n  ).reset_index()\n  return summary\n\n# Load data (assuming starwars.csv exists)\n# starwars_py = pd.read_csv(\"starwars.csv\")\n# summarize_variable_py(starwars_py, 'height')",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Functional Programming: The Cornerstone of Reproducible Analysis</span>"
    ]
  },
  {
    "objectID": "04-functional-programming.html#the-functional-toolkit-map-filter-and-reduce",
    "href": "04-functional-programming.html#the-functional-toolkit-map-filter-and-reduce",
    "title": "3¬† Functional Programming: The Cornerstone of Reproducible Analysis",
    "section": "3.4 The Functional Toolkit: Map, Filter, and Reduce",
    "text": "3.4 The Functional Toolkit: Map, Filter, and Reduce\nOnce you start thinking in functions, you‚Äôll notice common patterns emerge. Most for loops can be replaced by one of three core functional concepts: mapping, filtering, or reducing. These operations are handled by ‚Äúhigher-order functions‚Äù‚Äîfunctions that take other functions as arguments. Mastering them is key to writing elegant, declarative code.\n\n3.4.1 1. Mapping: Applying a Function to Each Element\nThe pattern: You have a list of things, and you want to perform the same action on each element, producing a new list of the same length.\nThis is the most common replacement for a for loop. Instead of manually iterating and storing results, you just state your intent: ‚Äúmap this function over this list.‚Äù\n\n3.4.1.1 In R with purrr::map()\nThe {purrr} package is the gold standard for functional programming in R. The map() family is its workhorse.\n\nmap(): Always returns a list.\nmap_dbl(): Returns a vector of doubles (numeric).\nmap_chr(): Returns a vector of characters (strings).\nmap_lgl(): Returns a vector of logicals (booleans).\nmap_dfr(): Returns a data frame by row-binding the results.\n\nExample: Calculate the mean of every column in a data frame.\nlibrary(purrr)\n\n# The classic for-loop way (verbose and clunky)\n# Allocate an empty vector with the right size\nmeans_loop &lt;- vector(\"double\", ncol(mtcars))\n\nfor (i in seq_along(mtcars)) {\n  means_loop[[i]] &lt;- mean(mtcars[[i]], na.rm = TRUE)\n}\n\nprint(means_loop)\n\n# The functional way with map_dbl()\nmeans_functional &lt;- map_dbl(mtcars, mean, na.rm = TRUE)\n\nprint(means_functional)\nThe map() version is not just shorter; it‚Äôs safer. You can‚Äôt make an off-by-one error, and you don‚Äôt have to pre-allocate means_loop. The code clearly states its purpose.\n\n\n3.4.1.2 In Python with List Comprehensions and map()\nPython‚Äôs most idiomatic tool for mapping is the list comprehension, which we saw earlier. It‚Äôs concise and highly readable.\nnumbers = [1, 2, 3, 4, 5]\nsquares = [n**2 for n in numbers]\n# &gt; [1, 4, 9, 16, 25]\nPython also has a built-in map() function, which returns a ‚Äúmap object‚Äù (an iterator). You usually wrap it in list() to see the results. It‚Äôs most useful when you already have a function defined.\ndef to_upper_case(s: str) -&gt; str:\n    return s.upper()\n\nwords = [\"hello\", \"world\"]\nupper_words = list(map(to_upper_case, words))\n# &gt; ['HELLO', 'WORLD']\n\n\n\n3.4.2 2. Filtering: Keeping Elements That Match a Condition\nThe pattern: You have a list of things, and you want to keep only the elements that satisfy a certain condition. The condition is defined by a function that returns TRUE or FALSE.\n\n3.4.2.1 In R with purrr::keep() or purrr::discard()\nkeep() retains elements where the function returns TRUE. discard() does the opposite.\nExample: From a list of data frames, keep only the ones with more than 100 rows.\n# setup: create a list of data frames\ndf1 &lt;- data.frame(x = 1:50)\ndf2 &lt;- data.frame(x = 1:200)\ndf3 &lt;- data.frame(x = 1:75)\nlist_of_dfs &lt;- list(a = df1, b = df2, c = df3)\n\n# The functional way to filter the list\nlarge_dfs &lt;- keep(list_of_dfs, ~ nrow(.x) &gt; 100)\nprint(names(large_dfs))\n\n\n3.4.2.2 In Python with List Comprehensions\nList comprehensions have a built-in if clause that makes filtering incredibly natural.\nnumbers = [1, 10, 5, 20, 15, 30]\n\n# Keep only numbers greater than 10\nlarge_numbers = [n for n in numbers if n &gt; 10]\n# &gt; [20, 15, 30]\nPython also has a built-in filter() function, which, like map(), returns an iterator.\ndef is_even(n: int) -&gt; bool:\n    return n % 2 == 0\n\nnumbers = [1, 2, 3, 4, 5, 6]\neven_numbers = list(filter(is_even, numbers))\n# &gt; [2, 4, 6]\n\n\n\n3.4.3 3. Reducing: Combining All Elements into a Single Value\nThe pattern: You have a list of things, and you want to iteratively combine them into a single summary value. You start with an initial value and repeatedly apply a function that takes the ‚Äúcurrent total‚Äù and the ‚Äúnext element.‚Äù\nThis is the most complex of the three but is powerful for things like summing, finding intersections, or joining a list of data frames.\n\n3.4.3.1 In R with purrr::reduce()\nExample: Find the total sum of a vector of numbers.\n# reduce() will take the first two elements (1, 2), apply `+` to get 3.\n# Then it takes the result (3) and the next element (3), applies `+` to get 6.\n# And so on.\ntotal_sum &lt;- reduce(c(1, 2, 3, 4, 5), `+`)\n\n# This is equivalent to 1 + 2 + 3 + 4 + 5\nprint(total_sum)\nA more practical data science example: find all the column names that are common to a list of data frames.\n# Get the column names of each df in the list\nlist_of_colnames &lt;- map(list_of_dfs, names)\nprint(list_of_colnames)\n\n# Use reduce with the `intersect` function to find common elements\ncommon_cols &lt;- reduce(list_of_colnames, intersect)\nprint(common_cols)\n\n\n3.4.3.2 In Python with functools.reduce\nThe reduce function was moved out of the built-ins and into the functools module in Python 3 because it‚Äôs often less readable than an explicit for loop for simple operations like summing. However, it‚Äôs still the right tool for more complex iterative combinations.\nfrom functools import reduce\nimport operator\n\nnumbers = [1, 2, 3, 4, 5]\n\n# Use reduce with the addition operator to sum the list\ntotal_sum_py = reduce(operator.add, numbers)\n# &gt; 15\n\n# You can also use a lambda function\ntotal_product = reduce(lambda x, y: x * y, numbers)\n# &gt; 120",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Functional Programming: The Cornerstone of Reproducible Analysis</span>"
    ]
  },
  {
    "objectID": "04-functional-programming.html#the-power-of-composition",
    "href": "04-functional-programming.html#the-power-of-composition",
    "title": "3¬† Functional Programming: The Cornerstone of Reproducible Analysis",
    "section": "3.5 The Power of Composition",
    "text": "3.5 The Power of Composition\nThe final, beautiful consequence of a functional style is composition. You can chain functions together to build complex workflows from simple, reusable parts. This is exactly what the pipe operators (|&gt; in R, %&gt;% from {magrittr}) and method chaining (the . in pandas) are designed for.\nThis R code is a sequence of function compositions:\nstarwars %&gt;%\n  filter(!is.na(mass)) %&gt;%\n  select(species, sex, mass) %&gt;%\n  group_by(sex, species) %&gt;%\n  summarise(mean_mass = mean(mass), .groups = \"drop\")\nThis is equivalent to summarise(group_by(select(filter(starwars, ...)))). The pipe makes it readable.\nThe same idea applies in Python with pandas:\n# (starwars_py\n#  .dropna(subset=['mass'])\n#  .filter(items=['species', 'sex', 'mass'])\n#  .groupby(['sex', 'species'])\n#  ['mass'].mean()\n#  .reset_index()\n# )\nEach step is a function that takes a data frame and returns a new, transformed data frame. By combining map, filter, and reduce with this compositional style, you can express complex data manipulation pipelines without writing a single for loop. This makes your code more declarative, less prone to bugs, and easier to reason about‚Äîa perfect fit for a reproducible workflow.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Functional Programming: The Cornerstone of Reproducible Analysis</span>"
    ]
  },
  {
    "objectID": "05-unit-testing.html",
    "href": "05-unit-testing.html",
    "title": "4¬† Unit Testing: The Safety Net for Your Code",
    "section": "",
    "text": "4.1 Introduction: Proving Your Code Works\nWhat you‚Äôll learn by the end of this chapter: * What unit tests are and why they are essential for reliable data analysis. * How to write and run unit tests for your functions in both R (with {testthat}) and Python (with pytest). * How to use testing to improve the design and robustness of your code. * How to leverage LLMs to accelerate test writing and embrace your role as a code reviewer.\nI hope you are starting to see the pieces of our reproducible workflow coming together. We now have:\nThis brings us to the final, crucial question: How do we prove that our functions actually do what we claim they do?\nThe answer is unit testing. A unit test is a piece of code whose sole job is to check that another piece of code, a ‚Äúunit‚Äù, works correctly. In our functional world, the ‚Äúunit‚Äù is almost always a single function. This is why we spent so much time on FP in the previous chapter. Small, pure functions are not just easy to reason about; they are incredibly easy to test.\nWriting tests is your contract with your collaborators and your future self. It‚Äôs a formal promise that your function, calculate_mean_mpg(), given a specific input, will always produce a specific, correct output. It‚Äôs the safety net that catches bugs before they make it into your final analysis and the tool that gives you the confidence to refactor and improve your code without breaking it.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Unit Testing: The Safety Net for Your Code</span>"
    ]
  },
  {
    "objectID": "05-unit-testing.html#introduction-proving-your-code-works",
    "href": "05-unit-testing.html#introduction-proving-your-code-works",
    "title": "4¬† Unit Testing: The Safety Net for Your Code",
    "section": "",
    "text": "Reproducible Environments (Nix): The correct tools for everyone.\nReproducible History (Git): The correct version of the code for everyone.\nReproducible Logic (Functional Programming): A philosophy for writing clean, predictable, and self-contained code.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Unit Testing: The Safety Net for Your Code</span>"
    ]
  },
  {
    "objectID": "05-unit-testing.html#the-philosophy-of-a-good-unit-test",
    "href": "05-unit-testing.html#the-philosophy-of-a-good-unit-test",
    "title": "4¬† Unit Testing: The Safety Net for Your Code",
    "section": "4.2 The Philosophy of a Good Unit Test",
    "text": "4.2 The Philosophy of a Good Unit Test\nSo, what should we test? Writing good tests is a skill, but it revolves around answering a few key questions about your function. For any function you write, you should have tests that cover:\n\nThe ‚ÄúHappy Path‚Äù: Does the function return the expected, correct value for a typical, valid input?\nBad Inputs: Does the function fail gracefully or throw an informative error when given garbage input (e.g., a string instead of a number, a data frame with the wrong columns)?\nEdge Cases: How does the function handle tricky but valid inputs? For example, what happens if it receives an empty data frame, a vector with NA values, or a vector where all the numbers are the same?\n\nWriting tests forces you to think through these scenarios, and in doing so, almost always leads you to write more robust and well-designed functions.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Unit Testing: The Safety Net for Your Code</span>"
    ]
  },
  {
    "objectID": "05-unit-testing.html#unit-testing-in-practice",
    "href": "05-unit-testing.html#unit-testing-in-practice",
    "title": "4¬† Unit Testing: The Safety Net for Your Code",
    "section": "4.3 Unit Testing in Practice",
    "text": "4.3 Unit Testing in Practice\nLet‚Äôs imagine we‚Äôve written a simple helper function to normalize a numeric vector (i.e., scale it to have a mean of 0 and a standard deviation of 1). We‚Äôll save this in a file named utils.R or utils.py.\nR version (utils.R):\nnormalize_vector &lt;- function(x) {\n  (x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE)\n}\nPython version (utils.py):\nimport numpy as np\n\ndef normalize_vector(x):\n  return (x - np.nanmean(x)) / np.nanstd(x)\nNow, let‚Äôs write tests for it.\n\n4.3.1 Testing in R with {testthat}\nIn R, the standard for unit testing is the {testthat} package. The convention is to create a tests/testthat/ directory in your project, and for a script utils.R, you would create a test file named test-utils.R.\nInside test-utils.R, we use the test_that() function to group related expectations.\n# In file: tests/testthat/test-utils.R\n\n# First, we need to load the function we want to test\nsource(\"../../utils.R\")\n\nlibrary(testthat)\n\ntest_that(\"Normalization works on a simple vector (the happy path)\", {\n  # 1. Setup: Create input and expected output\n  input_vector &lt;- c(10, 20, 30)\n  expected_output &lt;- c(-1, 0, 1)\n  \n  # 2. Action: Run the function\n  actual_output &lt;- normalize_vector(input_vector)\n  \n  # 3. Expectation: Check if the actual output matches the expected output\n  expect_equal(actual_output, expected_output)\n})\n\ntest_that(\"Normalization handles NA values correctly\", {\n  input_with_na &lt;- c(10, 20, 30, NA)\n  expected_output &lt;- c(-1, 0, 1, NA)\n  \n  actual_output &lt;- normalize_vector(input_with_na)\n  \n  # We need to use expect_equal because it knows how to compare NAs\n  expect_equal(actual_output, expected_output)\n})\nThe expect_equal() function checks for near-exact equality. {testthat} has many other expect_*() functions, like expect_error() to check that a function fails correctly, or expect_warning() to check for warnings.\n\n\n4.3.2 Testing in Python with pytest\nIn Python, the de facto standard is pytest. It‚Äôs incredibly simple and powerful. The convention is to create a tests/ directory, and your test files should be named test_*.py. Inside, you just write functions whose names start with test_ and use Python‚Äôs standard assert keyword.\n# In file: tests/test_utils.py\n\nimport numpy as np\nfrom utils import normalize_vector # Import our function\n\ndef test_normalize_vector_happy_path():\n    # 1. Setup\n    input_vector = np.array([10, 20, 30])\n    expected_output = np.array([-1.0, 0.0, 1.0])\n    \n    # 2. Action\n    actual_output = normalize_vector(input_vector)\n    \n    # 3. Expectation\n    # For floating point numbers, it's better to check for \"close enough\"\n    assert np.allclose(actual_output, expected_output)\n\ndef test_normalize_vector_with_nas():\n    input_with_na = np.array([10, 20, 30, np.nan])\n    expected_output = np.array([-1.0, 0.0, 1.0, np.nan])\n    \n    actual_output = normalize_vector(input_with_na)\n    \n    # `np.allclose` doesn't handle NaNs, but `np.testing.assert_allclose` does!\n    np.testing.assert_allclose(actual_output, expected_output)\nTo run your tests, you simply navigate to your project‚Äôs root directory in the terminal and run the command pytest. It will automatically discover and run all your tests for you.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Unit Testing: The Safety Net for Your Code</span>"
    ]
  },
  {
    "objectID": "05-unit-testing.html#testing-as-a-design-tool",
    "href": "05-unit-testing.html#testing-as-a-design-tool",
    "title": "4¬† Unit Testing: The Safety Net for Your Code",
    "section": "4.4 Testing as a Design Tool",
    "text": "4.4 Testing as a Design Tool\nHere is where testing becomes a superpower. What happens if we try to normalize a vector where all the elements are the same? The standard deviation will be 0, leading to a division by zero!\nLet‚Äôs write a test for this edge case first.\npytest version:\n# tests/test_utils.py\ndef test_normalize_vector_with_zero_std():\n    input_vector = np.array([5, 5, 5, 5])\n    actual_output = normalize_vector(input_vector)\n    # The current function will return `[nan, nan, nan, nan]`\n    # Let's assert that we expect a vector of zeros instead.\n    assert np.allclose(actual_output, np.array([0, 0, 0, 0]))\nIf we run pytest now, this test will fail. This is great! Our test has just revealed a flaw in our function‚Äôs design. This process is a core part of Test-Driven Development (TDD): write a failing test, then write the code to make it pass.\nLet‚Äôs improve our function:\nImproved Python version (utils.py):\nimport numpy as np\n\ndef normalize_vector(x):\n  std_dev = np.nanstd(x)\n  if std_dev == 0:\n    # If std is 0, all elements are the mean. Return a vector of zeros.\n    return np.zeros_like(x, dtype=float)\n  return (x - np.nanmean(x)) / std_dev\nNow, if we run pytest again, our new test will pass. We used testing not just to verify our code, but to actively make it more robust and thoughtful.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Unit Testing: The Safety Net for Your Code</span>"
    ]
  },
  {
    "objectID": "05-unit-testing.html#the-modern-data-scientists-role-reviewer-and-ai-collaborator",
    "href": "05-unit-testing.html#the-modern-data-scientists-role-reviewer-and-ai-collaborator",
    "title": "4¬† Unit Testing: The Safety Net for Your Code",
    "section": "4.5 The Modern Data Scientist‚Äôs Role: Reviewer and AI Collaborator",
    "text": "4.5 The Modern Data Scientist‚Äôs Role: Reviewer and AI Collaborator\nIn the past, writing tests was often seen as a chore. Today, LLMs make this process very easy.\n\n4.5.1 Using LLMs to Write Tests\nLLMs are fantastic at writing unit tests. They are good at handling boilerplate code and thinking of edge cases. You can provide your function to an LLM and give it a prompt like this:\n\nPrompt: ‚ÄúHere is my Python function normalize_vector. Please write three pytest unit tests for it. Include a test for the happy path with a simple array, a test for an array containing np.nan, and a test for the edge case where all elements in the array are identical.‚Äù\n\nThe LLM will likely generate high-quality test code that is very similar to what we wrote above. This is a massive productivity boost. However, this introduces a new, critical role for the data scientist: you are the reviewer.\nAn LLM does not write your tests; it generates a draft. It is your professional responsibility to: 1. Read and understand every line of the test code. 2. Verify that the expected_output is actually correct. 3. Confirm that the tests cover the cases you care about. 4. Commit that code under your name, taking full ownership of it.\n‚ÄúA COMPUTER CAN NEVER BE HELD ACCOUNTABLE THEREFORE A COMPUTER MUST NEVER MAKE A MANAGEMENT DECISION‚Äù ‚Äì IBM Training Manual, 1979.\nIf I ask you why you did something, and your answer is something to the effect of ‚ÄúI dunno, the LLM generated it‚Äù, be glad we‚Äôre not in the USA where I could just fire you, because that‚Äôs what I‚Äôd do.\n\n\n4.5.2 Testing and Code Review\nThis role as a reviewer is central to modern collaborative data science. When a teammate (or your future self) submits a Pull Request on GitHub, the tests are your first line of defense. A PR that changes logic but doesn‚Äôt update the tests is a major red flag. A PR that adds a new feature without adding any tests should be rejected until tests are included.\nEven as a junior member of a team, one of the most valuable contributions you can make during a code review is to ask: ‚ÄúThis looks great, but what happens if the input is NA? Could we add a test for that case?‚Äù This moves the quality of the entire project forward.\nBy embracing testing, you are not just writing better code; you are becoming a better collaborator and a more responsible data scientist.\n\n\n4.5.3 A Note on Packaging and Project Structure\nThroughout this chapter, we‚Äôve focused on testing individual functions within a simple project structure (utils.R and tests/test-utils.R). This is the fundamental skill. It‚Äôs important to recognize, however, that this entire process becomes even more streamlined and robust when your code is organized into a formal package.\nPackaging your code provides a standardized structure for your functions, documentation, and tests. It solves many logistical problems automatically: testing frameworks know exactly where to find your source code without needing manual source() or from utils import ... statements, and tools can easily run all tests with a single command. It also makes your code installable, versionable, and distributable, which is the ultimate form of reproducibility.\nWhile a full guide to package development is beyond the scope of this course, it is the natural next step in your journey as a data scientist who produces reliable tools. When you are ready to take that step, here are the definitive resources to guide you:\n\nFor R: The ‚ÄúR Packages‚Äù (2e) book by Hadley Wickham and Jennifer Bryan is the essential, comprehensive guide. It covers everything from initial setup with {usethis} to testing, documentation, and submission to CRAN. Read it online here.\nFor Python: The official Python Packaging User Guide is the place to start. For a more modern and streamlined approach that handles dependency management and publishing, many developers use tools like Poetry or Hatch.\n\nTreating your data analysis project like a small, internal software package, complete with functions and tests, is a powerful mindset that will elevate the quality and reliability of your work.\n\n\n4.5.4 Hands-On Exercises\nFor these exercises, create a project directory with a tests/ subdirectory. Place your function code in a script in the root directory (e.g., my_functions.R or my_functions.py) and your test code inside the tests/ directory (e.g., tests/test_my_functions.R or tests/test_my_functions.py).\n\n4.5.4.1 Exercise 1: Testing the ‚ÄúHappy Path‚Äù\nThe median of a list of numbers is a common calculation. However, the logic is slightly different depending on whether the list has an odd or even number of elements. Your task is to test both of these ‚Äúhappy paths.‚Äù\nHere is the function in R and Python.\nR (my_functions.R):\ncalculate_median &lt;- function(x) {\n  sorted_x &lt;- sort(x)\n  n &lt;- length(sorted_x)\n  mid &lt;- floor(n / 2)\n  \n  if (n %% 2 == 1) {\n    # Odd number of elements\n    return(sorted_x[mid + 1])\n  } else {\n    # Even number of elements\n    return(mean(c(sorted_x[mid], sorted_x[mid + 1])))\n  }\n}\nPython (my_functions.py):\nimport numpy as np\n\ndef calculate_median(x):\n  sorted_x = np.sort(np.array(x))\n  n = len(sorted_x)\n  mid = n // 2\n  \n  if n % 2 == 1:\n    # Odd number of elements\n    return sorted_x[mid]\n  else:\n    # Even number of elements\n    return (sorted_x[mid - 1] + sorted_x[mid]) / 2.0\nYour Task: 1. Create a test file (test-my_functions.R or tests/test_my_functions.py). 2. Write a test that checks if calculate_median gives the correct result for a vector with an odd number of elements (e.g., c(10, 20, 40)). 3. Write a second test that checks if calculate_median gives the correct result for a vector with an even number of elements (e.g., [1, 2, 8,     10]).\n\n\n4.5.4.2 Exercise 2: Testing Edge Cases and Expected Errors\nThe geometric mean is another way to calculate an average, but it has strict requirements: it only works with non-negative numbers. This makes it a great candidate for testing edge cases and expected failures.\nR (my_functions.R):\ncalculate_geometric_mean &lt;- function(x) {\n  if (any(x &lt; 0)) {\n    stop(\"Geometric mean is not defined for negative numbers.\")\n  }\n  return(prod(x)^(1 / length(x)))\n}\nPython (my_functions.py):\nimport numpy as np\n\ndef calculate_geometric_mean(x):\n  if np.any(np.array(x) &lt; 0):\n    raise ValueError(\"Geometric mean is not defined for negative numbers.\")\n  return np.prod(x)**(1 / len(x))\nYour Task: Write three tests for this function: 1. A ‚Äúhappy path‚Äù test with a simple vector of positive numbers (e.g., c(1, 2,     4) should result in 2). 2. An edge case test for a vector that includes 0. The expected result should be 0. 3. An error test that confirms the function fails correctly when given a vector with a negative number. * In R, use testthat::expect_error(). * In Python, use pytest.raises(). Example: with         pytest.raises(ValueError): your_function_call()\n\n\n4.5.4.3 Exercise 3: Test-Driven Development (in miniature)\nTesting can help you design better functions. Here is a simple function that is slightly flawed. Your task is to use testing to find the flaw and fix it.\nR (my_functions.R):\n# Initial flawed version\nfind_longest_string &lt;- function(string_vector) {\n  # This will break on an empty vector!\n  string_vector[which.max(nchar(string_vector))]\n}\nPython (my_functions.py):\n# Initial flawed version\ndef find_longest_string(string_list):\n  # This will break on an empty list!\n  return max(string_list, key=len)\nYour Task: 1. Part A: Write a simple test to prove the function works for a standard case (e.g., c(\"a\", \"b\", \"abc\") should return \"abc\"). 2. Part B: Write a new test for an empty input (c() or []). Run your tests. This test should fail with an error. 3. Part C: Modify the original find_longest_string function in your source file to handle the empty input gracefully (e.g., it could return NULL in R, or None in Python). 4. Run your tests again. Now all tests should pass. You have just completed a mini-cycle of Test-Driven Development!\n\n\n4.5.4.4 Exercise 4: The AI Collaborator\nOne of the most powerful uses of LLMs is to accelerate the creation of tests. Your job is to act as the senior reviewer for the code an LLM generates.\nHere is a simple data cleaning function in Python.\nPython (my_functions.py):\nimport pandas as pd\n\ndef clean_sales_data(df: pd.DataFrame) -&gt; pd.DataFrame:\n  \"\"\"\n  Cleans a raw sales DataFrame.\n  - Renames 'ts' column to 'timestamp'.\n  - Converts 'timestamp' column to datetime objects.\n  - Ensures 'sale_value' is a numeric type.\n  \"\"\"\n  if 'ts' not in df.columns:\n    raise KeyError(\"Input DataFrame must contain a 'ts' column.\")\n  \n  df = df.rename(columns={'ts': 'timestamp'})\n  df['timestamp'] = pd.to_datetime(df['timestamp'])\n  df['sale_value'] = pd.to_numeric(df['sale_value'])\n  return df\nYour Task: 1. Prompt your LLM: Copy the function above and give your LLM a prompt like this: &gt; ‚ÄúYou are a helpful assistant writing tests for a Python data science project. Here is a function. Please write a pytest test file for it. Include a test for the happy path where everything works correctly. Also, include a test that verifies the function raises a KeyError if the ‚Äòts‚Äô column is missing.‚Äù\n\nAct as the Reviewer:\n\nCreate a new test file (tests/test_data_cleaning.py) and paste the LLM‚Äôs response.\nRead every line of the generated test code. Is the logic correct? Is the expected_output data frame what you would actually expect?\nRun the tests using pytest. Do they pass? If not, debug and fix them. It is your responsibility to ensure the final committed code is correct.\nAdd a comment at the top of the test file describing one thing the LLM did well and one thing you had to change or fix (e.g., # LLM correctly     set up the test for the KeyError, but I had to correct the expected data     type in the happy path test.).",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Unit Testing: The Safety Net for Your Code</span>"
    ]
  }
]